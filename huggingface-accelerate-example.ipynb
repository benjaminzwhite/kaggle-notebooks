{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Basically the Accelerate code on Chapter 3 of HuggingFace NLP course (\"A Full Training\" section) doesn't work\n\nAt least not out of the box : i looked up and found this page instead:\n\n[https://github.com/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_nlp_example.ipynb](https://github.com/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_nlp_example.ipynb)","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\n\nfrom accelerate import Accelerator, DistributedType\nfrom datasets import load_dataset, load_metric\nfrom transformers import (\n    AdamW,\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    get_linear_schedule_with_warmup,\n    set_seed,\n)\n\nfrom tqdm.auto import tqdm\n\nimport datasets\nimport transformers","metadata":{"execution":{"iopub.status.busy":"2024-05-04T23:38:42.989882Z","iopub.execute_input":"2024-05-04T23:38:42.990633Z","iopub.status.idle":"2024-05-04T23:38:42.996425Z","shell.execute_reply.started":"2024-05-04T23:38:42.990598Z","shell.execute_reply":"2024-05-04T23:38:42.995244Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\nraw_datasets = load_dataset(\"glue\", \"mrpc\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T23:31:38.645182Z","iopub.execute_input":"2024-05-04T23:31:38.646108Z","iopub.status.idle":"2024-05-04T23:31:46.053043Z","shell.execute_reply.started":"2024-05-04T23:31:38.646072Z","shell.execute_reply":"2024-05-04T23:31:46.051998Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51f44971a42443b98f0a6442290268c1"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 649k/649k [00:00<00:00, 2.73MB/s]\nDownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75.7k/75.7k [00:00<00:00, 703kB/s]\nDownloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 308k/308k [00:00<00:00, 2.78MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c13d7ec934b453894fa64da323f8343"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"baa44fa9455042fca26536d8bd47e585"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1725 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06e5e055b53d47abb2cd869a5368089f"}},"metadata":{}}]},{"cell_type":"code","source":"\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\n\ncheckpoint = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T23:31:55.640328Z","iopub.execute_input":"2024-05-04T23:31:55.641406Z","iopub.status.idle":"2024-05-04T23:32:12.429799Z","shell.execute_reply.started":"2024-05-04T23:31:55.641370Z","shell.execute_reply":"2024-05-04T23:32:12.428693Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-05-04 23:32:03.001944: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-04 23:32:03.002073: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-04 23:32:03.117429: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6198975db3547f4aee889054daac013"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adf4958bec8a47b08c7c6472006531d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4727843eb66740189088342e5d8fd50d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5c3546735f6445a9b306520a62f827b"}},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_function(examples):\n    outputs = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, padding=\"max_length\", max_length=128)\n    return outputs\n\n\ntokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=[\"idx\", \"sentence1\", \"sentence2\"])\n\ntokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n\n#data_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T23:43:21.519214Z","iopub.execute_input":"2024-05-04T23:43:21.519610Z","iopub.status.idle":"2024-05-04T23:43:22.854129Z","shell.execute_reply.started":"2024-05-04T23:43:21.519579Z","shell.execute_reply":"2024-05-04T23:43:22.853227Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3668 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28616622b42b4fcdbad321c7a6f2649b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/408 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e7a1348da304220afc019aade0c0649"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1725 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd5e359218784387ac9cf34445cb9163"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets.set_format(\"torch\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T23:43:24.453071Z","iopub.execute_input":"2024-05-04T23:43:24.453511Z","iopub.status.idle":"2024-05-04T23:43:24.460624Z","shell.execute_reply.started":"2024-05-04T23:43:24.453482Z","shell.execute_reply":"2024-05-04T23:43:24.459310Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T23:43:31.444159Z","iopub.execute_input":"2024-05-04T23:43:31.444558Z","iopub.status.idle":"2024-05-04T23:43:32.168422Z","shell.execute_reply.started":"2024-05-04T23:43:31.444528Z","shell.execute_reply":"2024-05-04T23:43:32.167288Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"\nmetric = load_metric(\"glue\", \"mrpc\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T23:38:52.948838Z","iopub.execute_input":"2024-05-04T23:38:52.949266Z","iopub.status.idle":"2024-05-04T23:38:53.294654Z","shell.execute_reply.started":"2024-05-04T23:38:52.949235Z","shell.execute_reply":"2024-05-04T23:38:53.293767Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3759140866.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n  metric = load_metric(\"glue\", \"mrpc\")\n/opt/conda/lib/python3.10/site-packages/datasets/load.py:756: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/glue/glue.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.84k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebe682d401854aa4ad28aee854f62f95"}},"metadata":{}}]},{"cell_type":"code","source":"def create_dataloaders(train_batch_size=8, eval_batch_size=32):\n    train_dataloader = DataLoader(\n        tokenized_datasets[\"train\"], shuffle=True, batch_size=train_batch_size\n    )\n    eval_dataloader = DataLoader(\n        tokenized_datasets[\"validation\"], shuffle=False, batch_size=eval_batch_size\n    )\n    return train_dataloader, eval_dataloader","metadata":{"execution":{"iopub.status.busy":"2024-05-04T23:43:38.821987Z","iopub.execute_input":"2024-05-04T23:43:38.822393Z","iopub.status.idle":"2024-05-04T23:43:38.829045Z","shell.execute_reply.started":"2024-05-04T23:43:38.822364Z","shell.execute_reply":"2024-05-04T23:43:38.827820Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"\ntrain_dataloader, eval_dataloader = create_dataloaders()","metadata":{"execution":{"iopub.status.busy":"2024-05-04T23:43:40.874989Z","iopub.execute_input":"2024-05-04T23:43:40.875390Z","iopub.status.idle":"2024-05-04T23:43:40.881991Z","shell.execute_reply.started":"2024-05-04T23:43:40.875359Z","shell.execute_reply":"2024-05-04T23:43:40.880866Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"\nhyperparameters = {\n    \"learning_rate\": 2e-5,\n    \"num_epochs\": 3,\n    \"train_batch_size\": 8, # Actual batch size will this x 8\n    \"eval_batch_size\": 32, # Actual batch size will this x 8\n    \"seed\": 42,\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-04T23:40:33.526778Z","iopub.execute_input":"2024-05-04T23:40:33.527754Z","iopub.status.idle":"2024-05-04T23:40:33.532388Z","shell.execute_reply.started":"2024-05-04T23:40:33.527721Z","shell.execute_reply":"2024-05-04T23:40:33.531414Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def training_function(model):\n    # Initialize accelerator\n    accelerator = Accelerator()\n\n    \n    # To have only one message (and not 8) per logs of Transformers or Datasets, we set the logging verbosity\n    # to INFO for the main process only.\n    if accelerator.is_main_process:\n        datasets.utils.logging.set_verbosity_warning()\n        transformers.utils.logging.set_verbosity_info()\n    else:\n        datasets.utils.logging.set_verbosity_error()\n        transformers.utils.logging.set_verbosity_error()\n\n    train_dataloader, eval_dataloader = create_dataloaders(\n        train_batch_size=hyperparameters[\"train_batch_size\"], eval_batch_size=hyperparameters[\"eval_batch_size\"]\n    )\n    # The seed need to be set before we instantiate the model, as it will determine the random head.\n    set_seed(hyperparameters[\"seed\"])\n\n    # Instantiate optimizer\n    optimizer = AdamW(params=model.parameters(), lr=hyperparameters[\"learning_rate\"])\n\n    # Prepare everything\n    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the\n    # prepare method.\n    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader\n    )\n\n    num_epochs = hyperparameters[\"num_epochs\"]\n    # Instantiate learning rate scheduler after preparing the training dataloader as the prepare method\n    # may change its length.\n    lr_scheduler = get_linear_schedule_with_warmup(\n        optimizer=optimizer,\n        num_warmup_steps=100,\n        num_training_steps=len(train_dataloader) * num_epochs,\n    )\n\n    # Instantiate a progress bar to keep track of training. Note that we only enable it on the main\n    # process to avoid having 8 progress bars.\n    progress_bar = tqdm(range(num_epochs * len(train_dataloader)), disable=not accelerator.is_main_process)\n    # Now we train the model\n    for epoch in range(num_epochs):\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            outputs = model(**batch)\n            loss = outputs.loss\n            accelerator.backward(loss)\n            \n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n            progress_bar.update(1)\n\n        model.eval()\n        all_predictions = []\n        all_labels = []\n\n        for step, batch in enumerate(eval_dataloader):\n            with torch.no_grad():\n                outputs = model(**batch)\n            predictions = outputs.logits.argmax(dim=-1)\n\n            # We gather predictions and labels from the 8 TPUs to have them all.\n            all_predictions.append(accelerator.gather(predictions))\n            all_labels.append(accelerator.gather(batch[\"labels\"]))\n\n        # Concatenate all predictions and labels.\n        # The last thing we need to do is to truncate the predictions and labels we concatenated\n        # together as the prepared evaluation dataloader has a little bit more elements to make\n        # batches of the same size on each process.\n        all_predictions = torch.cat(all_predictions)[:len(tokenized_datasets[\"validation\"])]\n        all_labels = torch.cat(all_labels)[:len(tokenized_datasets[\"validation\"])]\n\n        eval_metric = metric.compute(predictions=all_predictions, references=all_labels)\n\n        # Use accelerator.print to print only on the main process.\n        accelerator.print(f\"epoch {epoch}:\", eval_metric)\n     ","metadata":{"execution":{"iopub.status.busy":"2024-05-04T23:43:46.349889Z","iopub.execute_input":"2024-05-04T23:43:46.350280Z","iopub.status.idle":"2024-05-04T23:43:46.366770Z","shell.execute_reply.started":"2024-05-04T23:43:46.350251Z","shell.execute_reply":"2024-05-04T23:43:46.365386Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"\nfrom accelerate import notebook_launcher\n\n\n\n\n\nnotebook_launcher(training_function, (model,), num_processes=2) # I ADDED num_processes = 2 here","metadata":{"execution":{"iopub.status.busy":"2024-05-04T23:43:49.536043Z","iopub.execute_input":"2024-05-04T23:43:49.536460Z","iopub.status.idle":"2024-05-04T23:46:43.576722Z","shell.execute_reply.started":"2024-05-04T23:43:49.536430Z","shell.execute_reply":"2024-05-04T23:46:43.574868Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Launching training on 2 GPUs.\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/690 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b37c24d026447f88b33525e24a561ae"}},"metadata":{}},{"name":"stdout","text":"epoch 0: {'accuracy': 0.8014705882352942, 'f1': 0.8656716417910448}\nepoch 1: {'accuracy': 0.8480392156862745, 'f1': 0.8973509933774835}\nepoch 2: {'accuracy': 0.8651960784313726, 'f1': 0.9046793760831888}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}