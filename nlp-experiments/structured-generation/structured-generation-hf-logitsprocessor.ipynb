{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Structured Generation Experiments with HuggingFace - transformers LogitsProcessor\n\nAfter reading this article:\n\n[https://towardsdatascience.com/structured-generative-ai-e772123428e4](https://towardsdatascience.com/structured-generative-ai-e772123428e4)\n\nand discovering `from transformers.generation.logits_process import LogitsProcessorList, LogitsProcessor`\n\n---\n\n- The idea of this article is forcing output to conform to SQL syntax.\n- The mental image is to think of the task as \"translating to a structured language\"\n- So the list of legitimate tokens at every generation step is limited\n\n**We want to \"insert\" this knowledge (i.e. of legitimate tokens) into the generative process**\n\n---\n\n### How to do it\n\nAt each step your output is a list of logit values **for all possible tokens in your vocabulary**.\n\nTo limit token generation, the idea is then to assign `-inf` value to all the tokens that you do not want to occur at that step.\n\n### LogitsProcessor\n\nTo use this from HuggingFace you need to implement a class with a `__call__` method, which will be called after the logits are calculated, but before the sampling step.\n\nThe method:\n- receives all token logits and generated input IDs\n- returns modified logits for all tokens (i.e. some will now be `-inf` based on some rule(s) for example)\n\n**We'll use BART for this notebook and we're trying to generate SQL**","metadata":{}},{"cell_type":"code","source":"import torch\n\nfrom transformers import BartForConditionalGeneration, BartTokenizerFast, PreTrainedTokenizer\nfrom transformers.generation.logits_process import LogitsProcessorList, LogitsProcessor\n\n\nname = 'facebook/bart-large'\ntokenizer = BartTokenizerFast.from_pretrained(name, add_prefix_space=True)\npretrained_model = BartForConditionalGeneration.from_pretrained(name)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:34:08.640405Z","iopub.execute_input":"2024-05-20T20:34:08.641163Z","iopub.status.idle":"2024-05-20T20:35:16.775845Z","shell.execute_reply.started":"2024-05-20T20:34:08.641128Z","shell.execute_reply":"2024-05-20T20:35:16.775052Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6a09c81750444c3be54b857e78f1038"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ef6a5604d174272916e1ccbe5fbf268"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32a1bb73bfca43efb8eb28c9c03e498d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df598f6c4ef346faa5c763cf483c5bf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.63k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37b2993161744688a70a063e91b5e56d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.02G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b1a03393ab043888ae6c53e439a6775"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We want to generate a \"translation\" from natural language to SQL - use the following example text, and see what the model does without any training **and without any structured generation constraint of course either**","metadata":{}},{"cell_type":"code","source":"# imagine a kind of SQL query described in natural language:\nto_translate = 'customers emails from the us'\n\n# TODO: I have questions about this step but at end of article there is comment\n# about tokenization (I wonder if the split into words is because you want to ensure\n# that the output/constraints correspond to SQL-lang word-level constraints??)\nwords = to_translate.split()\ntokenized_text = tokenizer([words], is_split_into_words=True)\n\nout = pretrained_model.generate(\n    torch.tensor(tokenized_text[\"input_ids\"]),\n    max_new_tokens=20,\n)\n\nprint(out)\nprint(\"----\")\n\nprint(tokenizer.convert_ids_to_tokens(out[0], skip_special_tokens=True))\nprint(\"----\")\n\nprint(tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(out[0], skip_special_tokens=True)))","metadata":{"execution":{"iopub.status.busy":"2024-05-20T20:39:01.225689Z","iopub.execute_input":"2024-05-20T20:39:01.226495Z","iopub.status.idle":"2024-05-20T20:39:02.345051Z","shell.execute_reply.started":"2024-05-20T20:39:01.226464Z","shell.execute_reply":"2024-05-20T20:39:02.344113Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"tensor([[   2,    0, 9690, 5575,   31,    5,  201,    2]])\n----\n['More', 'Ġemails', 'Ġfrom', 'Ġthe', 'Ġus']\n----\nMore emails from the us\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Of course we shouldn't expect SQL at this step!\n\nWe won't train the model but we will see if we can actually guide it to return SQL queries.\n\nWe develop a function that maps each generated token to a list of permissible next tokens.\n\n**For simplicity here: we focus on the immediate predecessor token, but you can implement more advanced mechanisms**\n\nWe use a dictionary, which defines for each token (key) which tokens (values) are allowed to follow it.\n\nNOTE: the `<s>` is the start generate token, and we want our SQL queries to start with either SELECT or DELETE here.\n\nIn this dataset hypothesis, the columns are : `name, email, id` only.","metadata":{}},{"cell_type":"code","source":"column_names = [\"name\", \"email\", \"id\"]\ntable_names = [\"customers\", \"vendors\"]\n\nrules = {\n    \"<s>\": [\"SELECT\", \"DELETE\"],\n    'SELECT': column_names,  # names of columns in our schema\n    'DELETE': column_names,\n    'name': [',', 'FROM'],\n    'email': [',', 'FROM'],\n    'id': [',', 'FROM'],\n    ',': column_names,\n    'FROM': table_names,  # names of tables in our schema\n    'customers': ['</s>'],\n    'vendors': ['</s>'],  # end of the generation\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-20T21:30:17.177582Z","iopub.execute_input":"2024-05-20T21:30:17.177942Z","iopub.status.idle":"2024-05-20T21:30:17.183973Z","shell.execute_reply.started":"2024-05-20T21:30:17.177914Z","shell.execute_reply":"2024-05-20T21:30:17.183006Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Convert these tokens to IDs.\n\n**You do this in a class that inherits from `LogitsProcessor`**\n\nWe also then implement the `__call__` function in this class, which is called after the logits are calculated.\n\nThe function:\n1. creates a new tensor of `-infs` \n2. checks which IDs are legitimate according to the rules dict\n3. places their scores in the new tensor","metadata":{}},{"cell_type":"code","source":"def convert_token_to_id(token):\n    return tokenizer(token, add_special_tokens=False)['input_ids'][0]\n\nclass SQLLogitsProcessor(LogitsProcessor):\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n        self.rules = {convert_token_to_id(k): [convert_token_to_id(v0) for v0 in v] for k,v in rules.items()}\n\n    def __call__(self, input_ids, scores):\n        if not (input_ids == self.tokenizer.bos_token_id).any():\n        # we must allow the start token to appear before we start processing\n            #print(\"HERE------\")\n            #print(scores, scores.shape)\n            #print(\"====\")\n            return scores\n        \n        #print(scores, scores.shape, type(scores))\n        # create a new tensor of -inf\n        new_scores = torch.full((1, self.tokenizer.vocab_size), float('-inf'))\n        #print(new_scores, new_scores.shape, type(new_scores))\n        \n        # ids of legitimate tokens\n        legit_ids = self.rules[int(input_ids[0, -1])]\n        #print(legit_ids)\n        \n        # place their values in the new tensor\n        new_scores[:, legit_ids] = scores[0, legit_ids]\n        #print(new_scores, new_scores.shape, type(new_scores))\n        \n        debug_new_scores = new_scores.repeat(4,1) # SPENT A WHILE DEBUGGING - WITH THE ABOVE STATEMENTS I FOUND THAT THE print(scores) WAS GIVING TENSORS OF SHAPE (4, vocab_size) SO SOMEWHERE IN CODE THERE IS A STEP WHERE IT'S MAKING 4 COPIES!?!?! ALL THE CONTENTS SEEM TO BE THE SAME\n        \n        return debug_new_scores","metadata":{"execution":{"iopub.status.busy":"2024-05-20T21:55:04.252103Z","iopub.execute_input":"2024-05-20T21:55:04.252467Z","iopub.status.idle":"2024-05-20T21:55:04.260855Z","shell.execute_reply.started":"2024-05-20T21:55:04.252436Z","shell.execute_reply":"2024-05-20T21:55:04.259979Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"## Structured generation\n\nThat's all we need to run generation with a logits processor:","metadata":{}},{"cell_type":"code","source":"to_translate = 'customers emails from the us'\nwords = to_translate.split()\ntokenized_text = tokenizer([words], is_split_into_words=True, return_offsets_mapping=True)\n\n#print(torch.tensor(tokenized_text[\"input_ids\"]))\nlogits_processor = LogitsProcessorList([SQLLogitsProcessor(tokenizer)])\n\nout = pretrained_model.generate(\n    torch.tensor(tokenized_text[\"input_ids\"]),\n    max_new_tokens=20,\n    logits_processor=logits_processor,\n)\nprint(tokenizer.convert_tokens_to_string(\n    tokenizer.convert_ids_to_tokens(\n        out[0], skip_special_tokens=True)))","metadata":{"execution":{"iopub.status.busy":"2024-05-20T21:56:22.050380Z","iopub.execute_input":"2024-05-20T21:56:22.050781Z","iopub.status.idle":"2024-05-20T21:56:23.315261Z","shell.execute_reply.started":"2024-05-20T21:56:22.050748Z","shell.execute_reply":"2024-05-20T21:56:23.314321Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":" SELECT email , email , id , email FROM customers\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Note that the output is a bit strange for \"real\" SQL, but we **didn't train the model at this point!**\n\n---\n\n# Be careful of tokenization\n\nTokenization is crucial when using GenAI for structured output.\n\nArticle gives example of e.g. training to generate JSON - if your model assigns different token ids to `[` and `[[` or e.g. `my_var_name` depending on if it has adjacent bracket `{my_var` vs `{ my_var` etc. the results and training will be worse as the logic that the model is being asked to learn is more complicated (it has to learn that the 2 brackets-symbol is 2 copies of the 1-bracket, not a distinct concept etc.)\n\n**So when training (IE WHEN YOU DESIGN YOUR OWN TOKENIZER ??? TODO: CHECK THIS - OTHERWISE YOU HAVE NO CONTROL OVER IT IF YOU JUST USE WHICHEVER MODEL OFF THE SHELF?) ensure each concept and punctuation is consistently converted to the same token - by e.g. adding spaces before words and characters in this JSON example etc**\n\nThen during prediction model will output JSON with spaces which you can then remove before parsing.\n\n---\n\n## Further reading\n\n- recent bookmarks May 2024 on domain-specific tokenization\n- ZeTT : Zero-shot Tokenizer Transfer (arXiv paper)","metadata":{}}]}