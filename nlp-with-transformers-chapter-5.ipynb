{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Natural Language Processing with Transformers\n\n## Chapter 5 - Text Generation\n\n### Greedy search decoding example\n\nUse GPT-2 (very similar to Raschka LLM tutorial so not many notes needed here):\n\n**load with \"Language Modeling Head\" - this is the CausalLM class from HF**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_name = \"gpt2-xl\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(model_name).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T09:50:54.718188Z","iopub.execute_input":"2024-05-12T09:50:54.719045Z","iopub.status.idle":"2024-05-12T09:51:24.695487Z","shell.execute_reply.started":"2024-05-12T09:50:54.719010Z","shell.execute_reply":"2024-05-12T09:51:24.694418Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7892bda9911946b0a15ce1cd62ea7766"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc330201bd304fe1ad7c5204a06b2c51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d6a446e25e54a30b45cbd69755d9570"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec8d7fd6f94849bb9dcf5910949d94db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"034ba5e087284208bfe6577bc2d274a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3ce82ed837047918a69feafcd5e4499"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab0e0437345a442983c1c199616fc9a8"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Example using HF generate()\n\nHere I just used the inbuilt generate() method - we are going to be implementing this from scratch.\n\nTaken from [https://huggingface.co/docs/transformers/model_doc/gpt2](https://huggingface.co/docs/transformers/model_doc/gpt2)","metadata":{}},{"cell_type":"code","source":"#prompt = \"def hello_world():\"\nprompt = \"Transformers are the\" # This is the example prompt used in book\nmodel_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\ngenerated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n\ntokenizer.batch_decode(generated_ids)[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-12T10:23:04.638181Z","iopub.execute_input":"2024-05-12T10:23:04.638565Z","iopub.status.idle":"2024-05-12T10:23:08.899418Z","shell.execute_reply.started":"2024-05-12T10:23:04.638538Z","shell.execute_reply":"2024-05-12T10:23:08.898519Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"\"Transformers are the future of the Marvel Universe and the biggest threat to the human race – except of course when Unicron destroys the earth.\\n\\nThis is how he got his name and everything. But, that's a story for another day.\\n\\nWhat we want to know is what you make of him. Because if you've got a burning question, then maybe you can put it to rest.\\n\\nHere we go.\\n\\nHis name is Unicron.\\n\\nHis backstory was mentioned in the\""},"metadata":{}}]},{"cell_type":"markdown","source":"To warm up, we’ll take the same iterative approach shown in Figure 5-3 (basically shows the step by step word by word expansion): we’ll use “Transformers are the” as the input\nprompt and run the decoding for eight timesteps. At each timestep, we pick out the\nmodel’s logits for the last token in the prompt and wrap them with a softmax to get a\nprobability distribution. We then pick the next token with the highest probability, add\nit to the input sequence, and run the process again. The following code does the job,\nand also stores the five most probable tokens at each timestep so we can visualize the\nalternatives:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ninput_txt = \"Transformers are the\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\nprint(\"ORIGINAL INPUT IDS ----->\", input_ids)\n\niterations = []\nn_steps = 8\nchoices_per_step = 5\n\nwith torch.no_grad():\n    for _ in range(n_steps):\n        iteration = dict()\n        iteration[\"input\"] = tokenizer.decode(input_ids[0]) # i looked why the [0], it's because input_ids is like : tensor([[41762,   364,   389,   262]], device='cuda:0')\n        output = model(input_ids=input_ids)\n        \n        # select logits of the first batch (NOTE I THINK THIS IS BECAUSE ONLY 1 BATCH HERE)\n        # and the last token of that batch, then apply softmax\n        next_token_logits = output.logits[0, -1, : ]\n        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n        \n        # TODO: NOTE USE OF argsort, SEEMS OK/OBVIOUS WHAT IT IS DOING BUT READ DOCS \n        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n        \n        # store the tokens with the highest probabilities\n        for choice_idx in range(choices_per_step):\n            token_id = sorted_ids[choice_idx]\n            token_prob = next_token_probs[token_id].cpu().numpy() # need on cpu before numpy IIRC\n            token_choice = f\"{tokenizer.decode(token_id)} - prob: {100 * token_prob:.2f}%\"\n            iteration[f\"Choice {choice_idx + 1}\"] = token_choice\n        \n        # store the info for this iteration\n        iterations.append(iteration)\n        \n        # append predicted next token to the current input string\n        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n        \ndf = pd.DataFrame(iterations)\n\ndisplay(df)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-12T10:50:57.232503Z","iopub.execute_input":"2024-05-12T10:50:57.233414Z","iopub.status.idle":"2024-05-12T10:50:57.638535Z","shell.execute_reply.started":"2024-05-12T10:50:57.233372Z","shell.execute_reply":"2024-05-12T10:50:57.637462Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"ORIGINAL INPUT IDS -----> tensor([[41762,   364,   389,   262]], device='cuda:0')\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                                               input                 Choice 1  \\\n0                               Transformers are the       most - prob: 8.53%   \n1                          Transformers are the most   popular - prob: 16.78%   \n2                  Transformers are the most popular       toy - prob: 10.63%   \n3              Transformers are the most popular toy      line - prob: 34.38%   \n4         Transformers are the most popular toy line        in - prob: 46.28%   \n5      Transformers are the most popular toy line in       the - prob: 65.99%   \n6  Transformers are the most popular toy line in the     world - prob: 69.26%   \n7  Transformers are the most popular toy line in ...         , - prob: 39.73%   \n\n                  Choice 2                     Choice 3  \\\n0       only - prob: 4.96%           best - prob: 4.65%   \n1   powerful - prob: 5.37%         common - prob: 4.96%   \n2       toys - prob: 7.23%   Transformers - prob: 6.60%   \n3        in - prob: 18.20%            of - prob: 11.71%   \n4        of - prob: 15.09%              , - prob: 4.94%   \n5   history - prob: 12.42%        America - prob: 6.91%   \n6     United - prob: 4.55%        history - prob: 4.29%   \n7         . - prob: 30.64%            and - prob: 9.87%   \n\n                      Choice 4                   Choice 5  \n0   Transformers - prob: 4.37%     ultimate - prob: 2.16%  \n1         famous - prob: 3.72%   successful - prob: 3.20%  \n2             of - prob: 5.46%          and - prob: 3.76%  \n3          brand - prob: 6.10%         line - prob: 2.69%  \n4             on - prob: 4.40%         ever - prob: 2.72%  \n5          Japan - prob: 2.44%        North - prob: 1.40%  \n6             US - prob: 4.23%            U - prob: 2.30%  \n7           with - prob: 2.32%        today - prob: 1.74%  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>Choice 1</th>\n      <th>Choice 2</th>\n      <th>Choice 3</th>\n      <th>Choice 4</th>\n      <th>Choice 5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Transformers are the</td>\n      <td>most - prob: 8.53%</td>\n      <td>only - prob: 4.96%</td>\n      <td>best - prob: 4.65%</td>\n      <td>Transformers - prob: 4.37%</td>\n      <td>ultimate - prob: 2.16%</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Transformers are the most</td>\n      <td>popular - prob: 16.78%</td>\n      <td>powerful - prob: 5.37%</td>\n      <td>common - prob: 4.96%</td>\n      <td>famous - prob: 3.72%</td>\n      <td>successful - prob: 3.20%</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Transformers are the most popular</td>\n      <td>toy - prob: 10.63%</td>\n      <td>toys - prob: 7.23%</td>\n      <td>Transformers - prob: 6.60%</td>\n      <td>of - prob: 5.46%</td>\n      <td>and - prob: 3.76%</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Transformers are the most popular toy</td>\n      <td>line - prob: 34.38%</td>\n      <td>in - prob: 18.20%</td>\n      <td>of - prob: 11.71%</td>\n      <td>brand - prob: 6.10%</td>\n      <td>line - prob: 2.69%</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Transformers are the most popular toy line</td>\n      <td>in - prob: 46.28%</td>\n      <td>of - prob: 15.09%</td>\n      <td>, - prob: 4.94%</td>\n      <td>on - prob: 4.40%</td>\n      <td>ever - prob: 2.72%</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Transformers are the most popular toy line in</td>\n      <td>the - prob: 65.99%</td>\n      <td>history - prob: 12.42%</td>\n      <td>America - prob: 6.91%</td>\n      <td>Japan - prob: 2.44%</td>\n      <td>North - prob: 1.40%</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Transformers are the most popular toy line in the</td>\n      <td>world - prob: 69.26%</td>\n      <td>United - prob: 4.55%</td>\n      <td>history - prob: 4.29%</td>\n      <td>US - prob: 4.23%</td>\n      <td>U - prob: 2.30%</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Transformers are the most popular toy line in ...</td>\n      <td>, - prob: 39.73%</td>\n      <td>. - prob: 30.64%</td>\n      <td>and - prob: 9.87%</td>\n      <td>with - prob: 2.32%</td>\n      <td>today - prob: 1.74%</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Now do with inbuilt:**\n\nmake sure sampling is switched off\n\n**TODO: UNDERSTAND THIS - it's not clear in book what it is doing?? i looked up docs:**\n\nif set to True, this parameter enables decoding strategies such as multinomial sampling, beam-search multinomial sampling, Top-K sampling and Top-p sampling. All these strategies select the next token from the probability distribution over the entire vocabulary with various strategy-specific adjustments.","metadata":{}},{"cell_type":"code","source":"input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\noutput = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\nprint(tokenizer.decode(output[0]))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T11:03:03.036358Z","iopub.execute_input":"2024-05-12T11:03:03.037099Z","iopub.status.idle":"2024-05-12T11:03:03.391068Z","shell.execute_reply.started":"2024-05-12T11:03:03.037068Z","shell.execute_reply":"2024-05-12T11:03:03.390102Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Transformers are the most popular toy line in the world,\n","output_type":"stream"}]},{"cell_type":"code","source":"max_length = 128\n\ninput_txt = \"\"\"In a shocking finding, scientist discovered \\\na herd of unicorns living in a remote, previously unexplored \\\nvalley, in the Andes Mountains. Even more surprising to the \\\nresearchers was the fact that the unicorns spoke perfect English.\\n\\n\n\"\"\"\ninput_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\noutput_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\nprint(tokenizer.decode(output_greedy[0]))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T11:04:11.684056Z","iopub.execute_input":"2024-05-12T11:04:11.684742Z","iopub.status.idle":"2024-05-12T11:04:15.052989Z","shell.execute_reply.started":"2024-05-12T11:04:11.684711Z","shell.execute_reply":"2024-05-12T11:04:15.052045Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n\n\nThe researchers, from the University of California, Davis, and the University of Colorado, Boulder, were conducting a study on the Andean cloud forest, which is home to the rare species of cloud forest trees.\n\n\nThe researchers were surprised to find that the unicorns were able to communicate with each other, and even with humans.\n\n\nThe researchers were surprised to find that the unicorns were able\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Note in the above how greedy search leads to repetitive output sequences (common problem with greedy - they can miss \"high probability word SEQUENCES\" because of individual high probability WORDS being preceded by low prob ones **and hence not getting reached etc**)\n\n\n## Beam search\n\n- keep track of `b` most probable next tokens\n- at each step select the `b` most likely extensions\n- continue until reach EOS token or max_length criterion\n- **the most likely SEQUENCE is chosen by ranking the `b` BEAMS OVERALL according to LOG PROB**\n\nUse **log prob** for numerical stability (avoids products of 0.00001 numbers etc)\n\n\n---\n\nHF generate: `num_beams`\n\n`no_repeat_ngram_size` - an option to avoid repetition (if would produce a previously seen n-gram, sets next token prob to 0)\n\n\n---\n\nContinues about:\n\n- Sampling methods / temperature (T rescales logits before taking softmax)\n\nAnother way to adjust output is to **truncate the distribution of the vocabulary:**\n\n(avoid very unlikely tokens)\n\n- top-k\n- nucleus (also called top-p)\n\nTop-k avoids low probability tokens by only sampling from the k tokens with the highest probability. Choose k manually (by eg test with different k and use text quality metrics)\n\nTop-k is a **fixed cutoff**, same for every choice in the sequence.\n\nAlternative is to use a **dynamic cutoff** - set a condition of when to cutoff which tokens are allowed (this is nucleus/top-p):\n\nuse as condition when a certain probability mass in the selection is reached: say you impose 95% - then order all tokens in descending order by probability and add one token after another from the top of the list until the sum of the probabilities of the selected tokens is 95%. \n\nquote from book:\n\nReturning to Figure 5-6, the value for p defines a horizontal\nline on the cumulative sum of probabilities plot, and we sample only from tokens\nbelow the line. Depending on the output distribution, this could be just one (very\nlikely) token or a hundred (more equally likely) tokens.\n\n**HF implementation: `top_p` in `generate()`**\n\n---\n\nYou can even combine the two\nsampling approaches to get the best of both worlds. Setting top_k=50 and top_p=0.9\ncorresponds to the rule of choosing tokens with a probability mass of 90%, from a\npool of at most 50 token\n\nWe can also apply beam search when we use sampling. Instead of\nselecting the next batch of candidate tokens greedily, we can sample\nthem and build up the beams in the same way.","metadata":{}}]}