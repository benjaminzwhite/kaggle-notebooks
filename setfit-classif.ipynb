{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install setfit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:21:17.307243Z","iopub.execute_input":"2024-12-15T09:21:17.307547Z","iopub.status.idle":"2024-12-15T09:21:25.721831Z","shell.execute_reply.started":"2024-12-15T09:21:17.307519Z","shell.execute_reply":"2024-12-15T09:21:25.720983Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: setfit in /opt/conda/lib/python3.10/site-packages (1.1.0)\nRequirement already satisfied: datasets>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from setfit) (3.1.0)\nRequirement already satisfied: sentence-transformers>=3 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers[train]>=3->setfit) (3.3.1)\nRequirement already satisfied: transformers>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from setfit) (4.45.2)\nRequirement already satisfied: evaluate>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from setfit) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from setfit) (0.26.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from setfit) (1.2.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from setfit) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.15.0->setfit) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.15.0->setfit) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.15.0->setfit) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.15.0->setfit) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.15.0->setfit) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.15.0->setfit) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.15.0->setfit) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.15.0->setfit) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.15.0->setfit) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.15.0->setfit) (2024.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.15.0->setfit) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.15.0->setfit) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.23.0->setfit) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->setfit) (3.1.2)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (2.4.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (1.14.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (10.3.0)\nRequirement already satisfied: accelerate>=0.20.3 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers[train]>=3->setfit) (1.1.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.0->setfit) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.0->setfit) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.0->setfit) (0.20.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->setfit) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->setfit) (3.5.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.20.3->sentence-transformers[train]>=3->setfit) (5.9.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.15.0->setfit) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.15.0->setfit) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.15.0->setfit) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.15.0->setfit) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.15.0->setfit) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.15.0->setfit) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.15.0->setfit) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.15.0->setfit) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.15.0->setfit) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.15.0->setfit) (2024.6.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.15.0->setfit) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.15.0->setfit) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.15.0->setfit) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.15.0->setfit) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=3->sentence-transformers[train]>=3->setfit) (1.3.0)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# SETFIT ERROR as of December 2024\n# AttributeError: 'CallbackHandler' object has no attribute 'tokenizer'\n# https://github.com/huggingface/setfit/issues/564\n# solution is to install older version of transformers\n!pip install transformers==4.45.2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from setfit import SetFitModel\n\nmodel = SetFitModel.from_pretrained(\"BAAI/bge-small-en-v1.5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:21:25.723095Z","iopub.execute_input":"2024-12-15T09:21:25.723421Z","iopub.status.idle":"2024-12-15T09:21:34.500657Z","shell.execute_reply.started":"2024-12-15T09:21:25.723392Z","shell.execute_reply":"2024-12-15T09:21:34.500015Z"}},"outputs":[{"name":"stderr","text":"model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"data-is-better-together/10k_prompts_ranked\")\n\ndataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:21:34.501602Z","iopub.execute_input":"2024-12-15T09:21:34.502118Z","iopub.status.idle":"2024-12-15T09:21:36.190138Z","shell.execute_reply.started":"2024-12-15T09:21:34.502088Z","shell.execute_reply":"2024-12-15T09:21:36.189326Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'quality', 'metadata', 'avg_rating', 'num_responses', 'agreement_ratio', 'raw_responses', 'kind', 'cluster_description', 'topic'],\n        num_rows: 10331\n    })\n})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"dataset_tt = dataset[\"train\"].train_test_split(test_size=0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:21:36.191182Z","iopub.execute_input":"2024-12-15T09:21:36.191760Z","iopub.status.idle":"2024-12-15T09:21:36.211477Z","shell.execute_reply.started":"2024-12-15T09:21:36.191730Z","shell.execute_reply":"2024-12-15T09:21:36.210690Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"dataset_tt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:21:36.212438Z","iopub.execute_input":"2024-12-15T09:21:36.212685Z","iopub.status.idle":"2024-12-15T09:21:36.218411Z","shell.execute_reply.started":"2024-12-15T09:21:36.212660Z","shell.execute_reply":"2024-12-15T09:21:36.217754Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'quality', 'metadata', 'avg_rating', 'num_responses', 'agreement_ratio', 'raw_responses', 'kind', 'cluster_description', 'topic'],\n        num_rows: 5165\n    })\n    test: Dataset({\n        features: ['prompt', 'quality', 'metadata', 'avg_rating', 'num_responses', 'agreement_ratio', 'raw_responses', 'kind', 'cluster_description', 'topic'],\n        num_rows: 5166\n    })\n})"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from setfit import sample_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:21:36.219430Z","iopub.execute_input":"2024-12-15T09:21:36.220113Z","iopub.status.idle":"2024-12-15T09:21:36.232306Z","shell.execute_reply.started":"2024-12-15T09:21:36.220073Z","shell.execute_reply":"2024-12-15T09:21:36.231640Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Mimic only having a few labeled examples from each class\n\n- here i'm taking 20 of each class (`topic` is the `label` in our current dataset, will rename later)\n- so `sample_dataset` from setfit should take 20 **of each possible label** so we should see 20 * 10 = 200 samples in train dataset below (since there are 10 labels in this dataset)\n\n## Update\n\ndoing 20 samples for all 10 classes, with 500 steps, i get accuracy of 0.505 after 20 mins\n\n- the advice is that you get better results with more data, so i'll try 100 samples of each now","metadata":{}},{"cell_type":"code","source":"#train_dataset = sample_dataset(dataset_tt[\"train\"], label_column=\"topic\", num_samples=20) # get 0.505 accuracy with 500 steps\n\ntrain_dataset = sample_dataset(dataset_tt[\"train\"], label_column=\"topic\", num_samples=100) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:21:48.427135Z","iopub.execute_input":"2024-12-15T09:21:48.427583Z","iopub.status.idle":"2024-12-15T09:21:48.595235Z","shell.execute_reply.started":"2024-12-15T09:21:48.427529Z","shell.execute_reply":"2024-12-15T09:21:48.594346Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:21:48.596928Z","iopub.execute_input":"2024-12-15T09:21:48.597251Z","iopub.status.idle":"2024-12-15T09:21:48.602595Z","shell.execute_reply.started":"2024-12-15T09:21:48.597223Z","shell.execute_reply":"2024-12-15T09:21:48.601644Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'quality', 'metadata', 'avg_rating', 'num_responses', 'agreement_ratio', 'raw_responses', 'kind', 'cluster_description', 'topic'],\n    num_rows: 929\n})"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"test_dataset = dataset_tt[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:21:48.603558Z","iopub.execute_input":"2024-12-15T09:21:48.603805Z","iopub.status.idle":"2024-12-15T09:21:48.613153Z","shell.execute_reply.started":"2024-12-15T09:21:48.603782Z","shell.execute_reply":"2024-12-15T09:21:48.612283Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"test_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:21:48.614960Z","iopub.execute_input":"2024-12-15T09:21:48.615243Z","iopub.status.idle":"2024-12-15T09:21:48.627104Z","shell.execute_reply.started":"2024-12-15T09:21:48.615205Z","shell.execute_reply":"2024-12-15T09:21:48.626177Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'quality', 'metadata', 'avg_rating', 'num_responses', 'agreement_ratio', 'raw_responses', 'kind', 'cluster_description', 'topic'],\n    num_rows: 5166\n})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"from setfit import TrainingArguments\n\nargs = TrainingArguments(\n    batch_size=32,\n    #num_epochs=1, # adjusted down from 10\n    max_steps=500, # get 40% accuracy at 50 steps on the test dataset, tried small number to make sure it works\n    report_to=\"none\",\n)\n\n\"\"\"\nThe num_epochs and max_steps arguments are frequently used to increase and decrease the number of total training steps. Consider that with SetFit, better performance is reached with more data, not more training! Don’t be afraid to train for less than 1 epoch if you have a lot of data.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:21:48.627908Z","iopub.execute_input":"2024-12-15T09:21:48.628136Z","iopub.status.idle":"2024-12-15T09:21:48.640052Z","shell.execute_reply.started":"2024-12-15T09:21:48.628113Z","shell.execute_reply":"2024-12-15T09:21:48.639337Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'\\nThe num_epochs and max_steps arguments are frequently used to increase and decrease the number of total training steps. Consider that with SetFit, better performance is reached with more data, not more training! Don’t be afraid to train for less than 1 epoch if you have a lot of data.\\n'"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from setfit import Trainer\n\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    column_mapping={\"prompt\":\"text\", \"topic\":\"label\"} # <--- SETFIT EXPECTS text AND label RESPECTIVELY\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:21:48.640873Z","iopub.execute_input":"2024-12-15T09:21:48.641077Z","iopub.status.idle":"2024-12-15T09:21:49.479245Z","shell.execute_reply.started":"2024-12-15T09:21:48.641056Z","shell.execute_reply":"2024-12-15T09:21:49.478624Z"}},"outputs":[{"name":"stderr","text":"Applying column mapping to the training dataset\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/929 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4504ee902e314db0a657c330151dac36"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:21:49.480070Z","iopub.execute_input":"2024-12-15T09:21:49.480319Z","iopub.status.idle":"2024-12-15T09:21:49.484189Z","shell.execute_reply.started":"2024-12-15T09:21:49.480295Z","shell.execute_reply":"2024-12-15T09:21:49.483365Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import wandb\nwandb.init(mode=\"disabled\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:21:49.485115Z","iopub.execute_input":"2024-12-15T09:21:49.485377Z","iopub.status.idle":"2024-12-15T09:21:50.013006Z","shell.execute_reply.started":"2024-12-15T09:21:49.485354Z","shell.execute_reply":"2024-12-15T09:21:50.012253Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  return LooseVersion(v) >= LooseVersion(check)\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x790501f13f70>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:21:50.013841Z","iopub.execute_input":"2024-12-15T09:21:50.014373Z","iopub.status.idle":"2024-12-15T09:42:01.155723Z","shell.execute_reply.started":"2024-12-15T09:21:50.014346Z","shell.execute_reply":"2024-12-15T09:42:01.154336Z"}},"outputs":[{"name":"stderr","text":"***** Running training *****\n  Num unique pairs = 16000\n  Batch size = 32\n  Num epochs = 1\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 20:04, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.234300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.237100</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.221400</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.205200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.184400</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.171100</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.147000</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.137200</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.124300</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.123100</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.120500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/30 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ed3bf0990d649f29da8c26a5c479f4f"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"trainer.evaluate(test_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:42:01.164252Z","iopub.execute_input":"2024-12-15T09:42:01.164792Z","iopub.status.idle":"2024-12-15T09:42:17.363031Z","shell.execute_reply.started":"2024-12-15T09:42:01.164733Z","shell.execute_reply":"2024-12-15T09:42:17.362122Z"}},"outputs":[{"name":"stderr","text":"Applying column mapping to the evaluation dataset\n***** Running evaluation *****\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/162 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"009985bf62574acda22db4416d4cf76a"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{'accuracy': 0.5642663569492837}"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"model.predict([\"write a new novel for me\", \"what is the best place to go to visit in italy?\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:42:17.364199Z","iopub.execute_input":"2024-12-15T09:42:17.364582Z","iopub.status.idle":"2024-12-15T09:42:17.396157Z","shell.execute_reply.started":"2024-12-15T09:42:17.364532Z","shell.execute_reply":"2024-12-15T09:42:17.395324Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eb3595e285e4733ac48b04f0f4b50e1"}},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"array(['Literature and Arts', 'Travel and Leisure'], dtype='<U22')"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"# Summary\n\n- with 20 samples and 500 training steps, get 50.5 % accuracy (not shown in notebook, deleted test run)\n- with 100 samples from each class, and 500 training steps, get 56.4 % accuracy\n\n## Try much larger number of samples, then will compare with finetuning a BERT type model\n\n- use 200 samples from each class","metadata":{}},{"cell_type":"code","source":"train_dataset2 = sample_dataset(dataset_tt[\"train\"], label_column=\"topic\", num_samples=200) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:52:26.121949Z","iopub.execute_input":"2024-12-15T09:52:26.122263Z","iopub.status.idle":"2024-12-15T09:52:26.271771Z","shell.execute_reply.started":"2024-12-15T09:52:26.122237Z","shell.execute_reply":"2024-12-15T09:52:26.271116Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"args2 = TrainingArguments(\n    batch_size=32,\n    #num_epochs=1, # adjusted down from 10\n    max_steps=500, # get 40% accuracy at 50 steps on the test dataset, tried small number to make sure it works\n    report_to=\"none\",\n)\n\ntrainer2 = Trainer(\n    model=model,\n    args=args2,\n    train_dataset=train_dataset2,\n    column_mapping={\"prompt\":\"text\", \"topic\":\"label\"} # <--- SETFIT EXPECTS text AND label RESPECTIVELY\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:52:55.728184Z","iopub.execute_input":"2024-12-15T09:52:55.728534Z","iopub.status.idle":"2024-12-15T09:52:56.403161Z","shell.execute_reply.started":"2024-12-15T09:52:55.728507Z","shell.execute_reply":"2024-12-15T09:52:56.402540Z"}},"outputs":[{"name":"stderr","text":"Applying column mapping to the training dataset\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1797 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72c190a230cb426cbbcb3b2336fb67fb"}},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"trainer2.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T09:53:08.921658Z","iopub.execute_input":"2024-12-15T09:53:08.922013Z","iopub.status.idle":"2024-12-15T10:13:28.931811Z","shell.execute_reply.started":"2024-12-15T09:53:08.921986Z","shell.execute_reply":"2024-12-15T10:13:28.930588Z"}},"outputs":[{"name":"stderr","text":"***** Running training *****\n  Num unique pairs = 16000\n  Batch size = 32\n  Num epochs = 1\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 20:11, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.183000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.173900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.159200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.149700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.139500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.125600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.111000</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.107300</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.095700</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.095200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.095300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/57 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a111d9b9d0c54d79a101bfde65d0a4ee"}},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"trainer2.evaluate(test_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:17:49.970372Z","iopub.execute_input":"2024-12-15T10:17:49.971064Z","iopub.status.idle":"2024-12-15T10:18:05.332441Z","shell.execute_reply.started":"2024-12-15T10:17:49.971029Z","shell.execute_reply":"2024-12-15T10:18:05.331477Z"}},"outputs":[{"name":"stderr","text":"Applying column mapping to the evaluation dataset\n***** Running evaluation *****\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/162 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44d2f9e2acd14b08aab0de1788e84257"}},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"{'accuracy': 0.5687185443283004}"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"## Comments\n\n- 56.8 with way more samples compared to 56.4 with 100 samples per label","metadata":{}},{"cell_type":"markdown","source":"# Do a trained distilBERT comparison","metadata":{}},{"cell_type":"code","source":"dataset_tt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:48:51.540323Z","iopub.execute_input":"2024-12-15T10:48:51.541181Z","iopub.status.idle":"2024-12-15T10:48:51.546415Z","shell.execute_reply.started":"2024-12-15T10:48:51.541143Z","shell.execute_reply":"2024-12-15T10:48:51.545586Z"}},"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 5165\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 5166\n    })\n})"},"metadata":{}}],"execution_count":87},{"cell_type":"code","source":"dataset_tt[\"train\"].features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:42:57.375372Z","iopub.execute_input":"2024-12-15T10:42:57.375728Z","iopub.status.idle":"2024-12-15T10:42:57.382248Z","shell.execute_reply.started":"2024-12-15T10:42:57.375696Z","shell.execute_reply":"2024-12-15T10:42:57.381340Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"{'text': Value(dtype='string', id='field'),\n 'quality': [{'user_id': Value(dtype='string', id='question'),\n   'value': Value(dtype='string', id='suggestion'),\n   'status': Value(dtype='string', id='question')}],\n 'metadata': Value(dtype='string', id='metadata'),\n 'avg_rating': Value(dtype='float64', id=None),\n 'num_responses': Value(dtype='int64', id=None),\n 'agreement_ratio': Value(dtype='float64', id=None),\n 'raw_responses': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n 'kind': Value(dtype='string', id=None),\n 'cluster_description': Value(dtype='string', id=None),\n 'label': Value(dtype='string', id=None)}"},"metadata":{}}],"execution_count":62},{"cell_type":"code","source":"!pip install evaluate accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:22:54.135497Z","iopub.execute_input":"2024-12-15T10:22:54.135836Z","iopub.status.idle":"2024-12-15T10:23:02.347950Z","shell.execute_reply.started":"2024-12-15T10:22:54.135810Z","shell.execute_reply":"2024-12-15T10:23:02.347039Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (0.4.3)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (1.1.1)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.26.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.2)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.5)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.15.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.6.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"MODEL_NAME = \"distilbert/distilbert-base-uncased\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:53:32.445246Z","iopub.execute_input":"2024-12-15T10:53:32.445646Z","iopub.status.idle":"2024-12-15T10:53:32.449825Z","shell.execute_reply.started":"2024-12-15T10:53:32.445615Z","shell.execute_reply":"2024-12-15T10:53:32.448788Z"}},"outputs":[],"execution_count":120},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:53:29.576555Z","iopub.execute_input":"2024-12-15T10:53:29.577440Z","iopub.status.idle":"2024-12-15T10:53:29.714055Z","shell.execute_reply.started":"2024-12-15T10:53:29.577394Z","shell.execute_reply":"2024-12-15T10:53:29.713403Z"}},"outputs":[],"execution_count":119},{"cell_type":"code","source":"dataset_tt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:49:23.648542Z","iopub.execute_input":"2024-12-15T10:49:23.648865Z","iopub.status.idle":"2024-12-15T10:49:23.654493Z","shell.execute_reply.started":"2024-12-15T10:49:23.648838Z","shell.execute_reply":"2024-12-15T10:49:23.653582Z"}},"outputs":[{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 5165\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 5166\n    })\n})"},"metadata":{}}],"execution_count":90},{"cell_type":"code","source":"dataset_tt = dataset_tt.rename_column(\"prompt\",\"text\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:44:21.579063Z","iopub.execute_input":"2024-12-15T10:44:21.579449Z","iopub.status.idle":"2024-12-15T10:44:21.960694Z","shell.execute_reply.started":"2024-12-15T10:44:21.579414Z","shell.execute_reply":"2024-12-15T10:44:21.959509Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset_tt \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_tt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename_column\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:405\u001b[0m, in \u001b[0;36mDatasetDict.rename_column\u001b[0;34m(self, original_column_name, new_column_name)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03mRename a column in the dataset and move the features associated to the original column under the new column name.\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03mThe transformation is applied to all the datasets of the dataset dictionary.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 405\u001b[0m     {\n\u001b[1;32m    406\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mrename_column(original_column_name\u001b[38;5;241m=\u001b[39moriginal_column_name, new_column_name\u001b[38;5;241m=\u001b[39mnew_column_name)\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    408\u001b[0m     }\n\u001b[1;32m    409\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:406\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03mRename a column in the dataset and move the features associated to the original column under the new column name.\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03mThe transformation is applied to all the datasets of the dataset dictionary.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    405\u001b[0m     {\n\u001b[0;32m--> 406\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_column_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_column_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    408\u001b[0m     }\n\u001b[1;32m    409\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:2193\u001b[0m, in \u001b[0;36mDataset.rename_column\u001b[0;34m(self, original_column_name, new_column_name, new_fingerprint)\u001b[0m\n\u001b[1;32m   2191\u001b[0m dataset \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   2192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_column_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names:\n\u001b[0;32m-> 2193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal column name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_column_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2195\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2196\u001b[0m     )\n\u001b[1;32m   2197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_column_name \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names:\n\u001b[1;32m   2198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2199\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew column name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_column_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease choose a column name which is not already in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2202\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Original column name prompt not in the dataset. Current columns in the dataset: ['text', 'quality', 'metadata', 'avg_rating', 'num_responses', 'agreement_ratio', 'raw_responses', 'kind', 'cluster_description', 'label']"],"ename":"ValueError","evalue":"Original column name prompt not in the dataset. Current columns in the dataset: ['text', 'quality', 'metadata', 'avg_rating', 'num_responses', 'agreement_ratio', 'raw_responses', 'kind', 'cluster_description', 'label']","output_type":"error"}],"execution_count":66},{"cell_type":"code","source":"dataset_tt = dataset_tt.rename_column(\"topic\",\"label\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:44:39.823640Z","iopub.execute_input":"2024-12-15T10:44:39.823985Z","iopub.status.idle":"2024-12-15T10:44:39.884396Z","shell.execute_reply.started":"2024-12-15T10:44:39.823953Z","shell.execute_reply":"2024-12-15T10:44:39.883313Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[67], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset_tt \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_tt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename_column\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtopic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:405\u001b[0m, in \u001b[0;36mDatasetDict.rename_column\u001b[0;34m(self, original_column_name, new_column_name)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03mRename a column in the dataset and move the features associated to the original column under the new column name.\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03mThe transformation is applied to all the datasets of the dataset dictionary.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 405\u001b[0m     {\n\u001b[1;32m    406\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mrename_column(original_column_name\u001b[38;5;241m=\u001b[39moriginal_column_name, new_column_name\u001b[38;5;241m=\u001b[39mnew_column_name)\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    408\u001b[0m     }\n\u001b[1;32m    409\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/dataset_dict.py:406\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03mRename a column in the dataset and move the features associated to the original column under the new column name.\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03mThe transformation is applied to all the datasets of the dataset dictionary.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_values_type()\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    405\u001b[0m     {\n\u001b[0;32m--> 406\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrename_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moriginal_column_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_column_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    408\u001b[0m     }\n\u001b[1;32m    409\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:2193\u001b[0m, in \u001b[0;36mDataset.rename_column\u001b[0;34m(self, original_column_name, new_column_name, new_fingerprint)\u001b[0m\n\u001b[1;32m   2191\u001b[0m dataset \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   2192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m original_column_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names:\n\u001b[0;32m-> 2193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal column name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_column_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2195\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2196\u001b[0m     )\n\u001b[1;32m   2197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_column_name \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names:\n\u001b[1;32m   2198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2199\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew column name \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_column_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease choose a column name which is not already in the dataset. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent columns in the dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2202\u001b[0m     )\n","\u001b[0;31mValueError\u001b[0m: Original column name topic not in the dataset. Current columns in the dataset: ['text', 'quality', 'metadata', 'avg_rating', 'num_responses', 'agreement_ratio', 'raw_responses', 'kind', 'cluster_description', 'label']"],"ename":"ValueError","evalue":"Original column name topic not in the dataset. Current columns in the dataset: ['text', 'quality', 'metadata', 'avg_rating', 'num_responses', 'agreement_ratio', 'raw_responses', 'kind', 'cluster_description', 'label']","output_type":"error"}],"execution_count":67},{"cell_type":"code","source":"dataset_tt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:44:43.977346Z","iopub.execute_input":"2024-12-15T10:44:43.977700Z","iopub.status.idle":"2024-12-15T10:44:43.983163Z","shell.execute_reply.started":"2024-12-15T10:44:43.977669Z","shell.execute_reply":"2024-12-15T10:44:43.982232Z"}},"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'quality', 'metadata', 'avg_rating', 'num_responses', 'agreement_ratio', 'raw_responses', 'kind', 'cluster_description', 'label'],\n        num_rows: 5165\n    })\n    test: Dataset({\n        features: ['text', 'quality', 'metadata', 'avg_rating', 'num_responses', 'agreement_ratio', 'raw_responses', 'kind', 'cluster_description', 'label'],\n        num_rows: 5166\n    })\n})"},"metadata":{}}],"execution_count":68},{"cell_type":"code","source":"dataset_tt = dataset_tt.remove_columns(['quality', 'metadata', 'avg_rating', 'num_responses', 'agreement_ratio', 'raw_responses', 'kind', 'cluster_description'])\n\ndataset_tt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:45:07.829743Z","iopub.execute_input":"2024-12-15T10:45:07.830555Z","iopub.status.idle":"2024-12-15T10:45:07.840376Z","shell.execute_reply.started":"2024-12-15T10:45:07.830519Z","shell.execute_reply":"2024-12-15T10:45:07.839597Z"}},"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 5165\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 5166\n    })\n})"},"metadata":{}}],"execution_count":69},{"cell_type":"code","source":"def preprocess_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\") # adding padding = \"max_length\" here","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:53:03.440553Z","iopub.execute_input":"2024-12-15T10:53:03.441251Z","iopub.status.idle":"2024-12-15T10:53:03.445135Z","shell.execute_reply.started":"2024-12-15T10:53:03.441219Z","shell.execute_reply":"2024-12-15T10:53:03.444353Z"}},"outputs":[],"execution_count":108},{"cell_type":"code","source":"tokenized_ds = dataset_tt.map(preprocess_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:53:05.224257Z","iopub.execute_input":"2024-12-15T10:53:05.224620Z","iopub.status.idle":"2024-12-15T10:53:08.505024Z","shell.execute_reply.started":"2024-12-15T10:53:05.224591Z","shell.execute_reply":"2024-12-15T10:53:08.504143Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5165 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bbd386d531f48b0b872a20f5e0473ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5166 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c18b343fd68045d7acefc8e8caa89333"}},"metadata":{}}],"execution_count":109},{"cell_type":"markdown","source":"# deleted data collator, causing bugs i can't understand why (it's literally the docs on HF site)","metadata":{}},{"cell_type":"code","source":"import evaluate\n\naccuracy = evaluate.load(\"accuracy\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:53:11.306605Z","iopub.execute_input":"2024-12-15T10:53:11.307341Z","iopub.status.idle":"2024-12-15T10:53:11.753549Z","shell.execute_reply.started":"2024-12-15T10:53:11.307298Z","shell.execute_reply":"2024-12-15T10:53:11.752659Z"}},"outputs":[],"execution_count":110},{"cell_type":"code","source":"import numpy as np\n\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return accuracy.compute(predictions=predictions, references=labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:53:12.505119Z","iopub.execute_input":"2024-12-15T10:53:12.505466Z","iopub.status.idle":"2024-12-15T10:53:12.510036Z","shell.execute_reply.started":"2024-12-15T10:53:12.505431Z","shell.execute_reply":"2024-12-15T10:53:12.509116Z"}},"outputs":[],"execution_count":111},{"cell_type":"markdown","source":"# Get labels, they don't seem to be accessible in dataset itseflf","metadata":{}},{"cell_type":"code","source":"TVT_KEYS = (\"train\",\"test\")\nALL_LABELS_NAMES = set()\n\nfor key in TVT_KEYS:\n    for sample in dataset_tt[key]:\n        ALL_LABELS_NAMES.add(sample[\"label\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:53:15.336576Z","iopub.execute_input":"2024-12-15T10:53:15.336918Z","iopub.status.idle":"2024-12-15T10:53:15.897501Z","shell.execute_reply.started":"2024-12-15T10:53:15.336885Z","shell.execute_reply":"2024-12-15T10:53:15.896605Z"}},"outputs":[],"execution_count":112},{"cell_type":"code","source":"ALL_LABELS_NAMES = sorted(ALL_LABELS_NAMES)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:53:15.898865Z","iopub.execute_input":"2024-12-15T10:53:15.899137Z","iopub.status.idle":"2024-12-15T10:53:15.903025Z","shell.execute_reply.started":"2024-12-15T10:53:15.899113Z","shell.execute_reply":"2024-12-15T10:53:15.901986Z"}},"outputs":[],"execution_count":113},{"cell_type":"code","source":"print(ALL_LABELS_NAMES, len(ALL_LABELS_NAMES))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:53:15.904047Z","iopub.execute_input":"2024-12-15T10:53:15.904341Z","iopub.status.idle":"2024-12-15T10:53:15.914092Z","shell.execute_reply.started":"2024-12-15T10:53:15.904304Z","shell.execute_reply":"2024-12-15T10:53:15.913280Z"}},"outputs":[{"name":"stdout","text":"['Business and Marketing', 'Environmental Issues', 'Health and Wellness', 'Legal and Government', 'Literature and Arts', 'Math', 'Others', 'Science and Technology', 'Software Development', 'Travel and Leisure'] 10\n","output_type":"stream"}],"execution_count":114},{"cell_type":"code","source":"label2id = {label_name:idx for idx,label_name in enumerate(ALL_LABELS_NAMES)}\n\nprint(label2id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:53:17.177331Z","iopub.execute_input":"2024-12-15T10:53:17.178219Z","iopub.status.idle":"2024-12-15T10:53:17.182779Z","shell.execute_reply.started":"2024-12-15T10:53:17.178180Z","shell.execute_reply":"2024-12-15T10:53:17.181863Z"}},"outputs":[{"name":"stdout","text":"{'Business and Marketing': 0, 'Environmental Issues': 1, 'Health and Wellness': 2, 'Legal and Government': 3, 'Literature and Arts': 4, 'Math': 5, 'Others': 6, 'Science and Technology': 7, 'Software Development': 8, 'Travel and Leisure': 9}\n","output_type":"stream"}],"execution_count":115},{"cell_type":"code","source":"id2label = {idx:label_name for label_name,idx in label2id.items()}\n\nprint(id2label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:53:17.320735Z","iopub.execute_input":"2024-12-15T10:53:17.321002Z","iopub.status.idle":"2024-12-15T10:53:17.326226Z","shell.execute_reply.started":"2024-12-15T10:53:17.320975Z","shell.execute_reply":"2024-12-15T10:53:17.325422Z"}},"outputs":[{"name":"stdout","text":"{0: 'Business and Marketing', 1: 'Environmental Issues', 2: 'Health and Wellness', 3: 'Legal and Government', 4: 'Literature and Arts', 5: 'Math', 6: 'Others', 7: 'Science and Technology', 8: 'Software Development', 9: 'Travel and Leisure'}\n","output_type":"stream"}],"execution_count":116},{"cell_type":"code","source":"NUM_LABELS = len(label2id)\n\nprint(NUM_LABELS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:53:17.504795Z","iopub.execute_input":"2024-12-15T10:53:17.505052Z","iopub.status.idle":"2024-12-15T10:53:17.509111Z","shell.execute_reply.started":"2024-12-15T10:53:17.505025Z","shell.execute_reply":"2024-12-15T10:53:17.508283Z"}},"outputs":[{"name":"stdout","text":"10\n","output_type":"stream"}],"execution_count":117},{"cell_type":"code","source":"from transformers import DistilBertForSequenceClassification, TrainingArguments, Trainer\n\nmodel = DistilBertForSequenceClassification.from_pretrained(\n    MODEL_NAME, num_labels=NUM_LABELS, id2label=id2label, label2id=label2id\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:54:07.180344Z","iopub.execute_input":"2024-12-15T10:54:07.180693Z","iopub.status.idle":"2024-12-15T10:54:07.401240Z","shell.execute_reply.started":"2024-12-15T10:54:07.180662Z","shell.execute_reply":"2024-12-15T10:54:07.400631Z"}},"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":121},{"cell_type":"code","source":"tokenized_ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:54:13.401399Z","iopub.execute_input":"2024-12-15T10:54:13.401726Z","iopub.status.idle":"2024-12-15T10:54:13.407574Z","shell.execute_reply.started":"2024-12-15T10:54:13.401698Z","shell.execute_reply":"2024-12-15T10:54:13.406739Z"}},"outputs":[{"execution_count":122,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 5165\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 5166\n    })\n})"},"metadata":{}}],"execution_count":122},{"cell_type":"code","source":"#tokenized_ds = tokenized_ds.remove_columns(tokenized_ds[\"train\"].column_names)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:40:38.948617Z","iopub.execute_input":"2024-12-15T10:40:38.949640Z","iopub.status.idle":"2024-12-15T10:40:38.959140Z","shell.execute_reply.started":"2024-12-15T10:40:38.949578Z","shell.execute_reply":"2024-12-15T10:40:38.958264Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"tokenized_ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:47:09.020734Z","iopub.execute_input":"2024-12-15T10:47:09.021374Z","iopub.status.idle":"2024-12-15T10:47:09.026785Z","shell.execute_reply.started":"2024-12-15T10:47:09.021326Z","shell.execute_reply":"2024-12-15T10:47:09.025811Z"}},"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 5165\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'attention_mask'],\n        num_rows: 5166\n    })\n})"},"metadata":{}}],"execution_count":83},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding=\"max_length\", truncation=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:54:30.085291Z","iopub.execute_input":"2024-12-15T10:54:30.085649Z","iopub.status.idle":"2024-12-15T10:54:30.221079Z","shell.execute_reply.started":"2024-12-15T10:54:30.085618Z","shell.execute_reply":"2024-12-15T10:54:30.220458Z"}},"outputs":[],"execution_count":124},{"cell_type":"code","source":"tokenized_ds[\"train\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:54:34.162432Z","iopub.execute_input":"2024-12-15T10:54:34.162777Z","iopub.status.idle":"2024-12-15T10:54:34.168229Z","shell.execute_reply.started":"2024-12-15T10:54:34.162746Z","shell.execute_reply":"2024-12-15T10:54:34.167423Z"}},"outputs":[{"execution_count":125,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text', 'label', 'input_ids', 'attention_mask'],\n    num_rows: 5165\n})"},"metadata":{}}],"execution_count":125},{"cell_type":"code","source":"tokenized_ds[\"train\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:54:59.009322Z","iopub.execute_input":"2024-12-15T10:54:59.009689Z","iopub.status.idle":"2024-12-15T10:54:59.025122Z","shell.execute_reply.started":"2024-12-15T10:54:59.009659Z","shell.execute_reply":"2024-12-15T10:54:59.024143Z"}},"outputs":[{"execution_count":128,"output_type":"execute_result","data":{"text/plain":"{'text': 'there are only two days a year when there will never be a professional major league game (NFL, MLB, NHL, NBA).  what are these days?',\n 'label': 'Literature and Arts',\n 'input_ids': [101,\n  2045,\n  2024,\n  2069,\n  2048,\n  2420,\n  1037,\n  2095,\n  2043,\n  2045,\n  2097,\n  2196,\n  2022,\n  1037,\n  2658,\n  2350,\n  2223,\n  2208,\n  1006,\n  5088,\n  1010,\n  10901,\n  1010,\n  7097,\n  1010,\n  6452,\n  1007,\n  1012,\n  2054,\n  2024,\n  2122,\n  2420,\n  1029,\n  102,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0]}"},"metadata":{}}],"execution_count":128},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"finetuned-prompt-classifier\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    push_to_hub=False,\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_ds[\"train\"],\n    eval_dataset=tokenized_ds[\"test\"],\n    tokenizer=tokenizer,\n    #data_collator=data_collator, # <--- deleted during debug, doesn't work and answers are unclear on HF github\n    compute_metrics=compute_metrics,\n)\n\ntraining_metrics = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T10:54:41.480299Z","iopub.execute_input":"2024-12-15T10:54:41.481111Z","iopub.status.idle":"2024-12-15T10:54:42.059538Z","shell.execute_reply.started":"2024-12-15T10:54:41.481075Z","shell.execute_reply":"2024-12-15T10:54:42.058142Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:775\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[0;32m--> 775\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:737\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[0;34m(value, dtype)\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[0;32m--> 737\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[127], line 25\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinetuned-prompt-classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     17\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m training_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2345\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2342\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2344\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2345\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2346\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:552\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 552\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:271\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m--> 271\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m    280\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/data/data_collator.py:66\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[0;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     padded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpad_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3536\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3533\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   3534\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m-> 3536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:240\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[0;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[1;32m    236\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:791\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[0;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    788\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    789\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    790\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    792\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    793\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    794\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    796\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n","\u001b[0;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`label` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."],"ename":"ValueError","evalue":"Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`label` in this case) have excessive nesting (inputs type `list` where type `int` is expected).","output_type":"error"}],"execution_count":127},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}