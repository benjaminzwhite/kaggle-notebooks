{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Getting PEFT and SFT code to work on Phi-3\n\nhere I use the train split of the absa-quad dataset (after reformatting) to finetune Phi-3-mini-4k-instruct\n\n**UPDATE: model link after training AND MERGING is:**\n\n[https://huggingface.co/benjaminzwhite/phi-3-mini-4k-instruct-ABSA-QUAD](https://huggingface.co/benjaminzwhite/phi-3-mini-4k-instruct-ABSA-QUAD)","metadata":{}},{"cell_type":"code","source":"#from huggingface_hub import notebook_login\n\n#notebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\nabsa_quad = load_dataset(\"NEUDM/absa-quad\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# convert dataset to usable format","metadata":{}},{"cell_type":"code","source":"import ast","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"phi3_train_dataset = []\n\n\nfor example in absa_quad[\"train\"].iter(batch_size=1):\n    #print(example)\n    # get the raw text\n    example_text = example[\"input\"][0]\n\n    input_text = ast.literal_eval(example_text)[0]\n    \n    gold_labels = example[\"output\"][0]\n    \n    #print(gold_labels)\n    golds = ast.literal_eval(gold_labels)\n    tmp_list_of_quads = []\n    for quad in golds:\n        tmp_d = {}\n        tmp_d[\"opinion term\"] = quad[0]\n        tmp_d[\"aspect category\"] = quad[1]\n        tmp_d[\"sentiment\"] = quad[2]\n        tmp_d[\"justification\"] = quad[3]\n        # convert to str representation for making the prompt\n        str_x = str(tmp_d)\n        tmp_list_of_quads.append(str_x)\n    \n    # create the string representation of the gold answer\n    gold_answer = '[' + ','.join(tmp_list_of_quads) + ']'\n    \n    phi3template = f\"\"\"<|system|>\nYou are a computer program who only replies with valid JSON lists.<|end|>\n<|user|>\nPerform a full aspect-based sentiment analysis of the following restaurant review:\n\n{input_text}\n<|end|>\n<|assistant|>\n{gold_answer}\"\"\"\n    phi3_train_dataset.append(phi3template)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(phi3_train_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\n# convert to HF dataset\ndf = pd.DataFrame(phi3_train_dataset)\ntrain_data = Dataset.from_pandas(df.rename(columns={0: \"text\"}), split=\"train\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model preparation stuff","metadata":{}},{"cell_type":"code","source":"#!pip install -q -U torch --index-url https://download.pytorch.org/whl/cu117\n!pip install -q -U -i https://pypi.org/simple/ bitsandbytes\n#!pip install -q -U transformers==\"4.40.0\"\n!pip install -q -U accelerate\n#!pip install -q -U datasets\n!pip install -q -U trl\n!pip install -q -U peft\n!pip install -q -U tensorboard\n!pip install -q -U einops","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftConfig\nfrom trl import SFTTrainer\nfrom transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig, \n                          TrainingArguments, \n                          pipeline, \n                          logging)\nfrom sklearn.metrics import (accuracy_score, \n                             classification_report, \n                             confusion_matrix)\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n\ncompute_dtype = getattr(torch, \"float16\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    trust_remote_code=True,\n    device_map=\"auto\",\n    quantization_config=bnb_config, \n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\nmax_seq_length = 2048\ntokenizer = AutoTokenizer.from_pretrained(model_name, \n                                          trust_remote_code=True,\n                                          max_seq_length=max_seq_length,\n                                         )\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peft_config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    target_modules=\"all-linear\",\n    lora_dropout=0.00,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\ntraining_arguments = TrainingArguments(\n    output_dir=\"bzw_train_logs\",\n    num_train_epochs=1, # ADJUSTED TO 1 FOR TESTING CAN SET TO 4\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=8, # 4\n    optim=\"paged_adamw_32bit\",\n    save_steps=0,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",\n    report_to=\"tensorboard\",\n    #evaluation_strategy=\"epoch\"\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n    #eval_dataset=eval_data,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    max_seq_length=max_seq_length,\n    args=training_arguments,\n    packing=False,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# go\n\n**NOTE - FFS for some reason Kaggle bars dont show progress so I cancelled one run since it was stuck on like 32/4000 but then when I interrupted it showed taht there had been 3 epochs or so and only had 15mins left to finish**\n\nbasically just leave it and trust that it is working ok - 45 mins or so for 1 epoch","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_model_name = \"phi-3-mini-4k-instruct-ABSA-QUAD\"\n\ntrainer.model.save_pretrained(my_model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Use prompt template manually to test a few examples","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generation_template = \"\"\"<|system|>\nYou are a computer program who only replies with valid JSON lists.<|end|>\n<|user|>\nPerform a full aspect-based sentiment analysis of the following restaurant review:\n\n{review_text}\n<|end|>\n<|assistant|>\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example_review = \"This place's burgers are the absolute best in town, and even though the service is incredibly slow I'd definitely come back - I want to try the tomato sauce that my friend had which looked delicious!\"\n\nresult = pipe(generation_template.format(review_text=example_review))\nprint(result[0]['generated_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# toggle Return full text NO\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500, return_full_text=False) \nresult = pipe(generation_template.format(review_text=example_review))\nprint(result[0]['generated_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Merging back to model","metadata":{}},{"cell_type":"markdown","source":"# DEBUG \n\nafter reading Younes HF gist :\n\n[https://gist.github.com/younesbelkada/9f7f75c94bdc1981c8ca5cc937d4a4da](https://gist.github.com/younesbelkada/9f7f75c94bdc1981c8ca5cc937d4a4da)\n\nthe stuff with `gc` didn't work - to debug and get the model merging steps below to work, I found that I had to do as said in\n\nhttps://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html\n\nwhere he says to **restart notebook (Run > restart & clear cell outputs)**\n\nthen copy **first few cells** (basically in my case I copied below what needs to be copied - it's the stuff about where your trained adapter is saved `my_model_name` in this case\n\n# TODO - check\n\nbefore merging i'm 99% sure that /kaggle/working had `my_model_name` file being around 100 Mb or so, and seemed to be only the adapter\n\nafter running the restarted part of notebook below (i.e doing the model merge) i note that my output kaggle/working dir is now indeed 7 or 8 Gb so seems to have the **full model** ok ; **but i still want to be clear that the FIRST PART of this notebook/training up to `save_pretrained` <-- that this is saving only the adapter??? not clear yet to me**\n","metadata":{}},{"cell_type":"code","source":"# DIDNT WORK SEE ABOVE COMMENTS AND BELOW CELL FOR CODE THAT WORKS - AFTER RESTART NOTEBOOK\ndel model\ndel pipe\ndel trainer\nimport gc\ngc.collect()\ngc.collect()\ngc.collect()\ngc.collect()\ngc.collect()\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# This works (below) after restarting notebook\n\n**NOTE !!!!!!! YOU NEED TO HAVE DONE THE save_pretrained BIT AFTER YOUR TRAINING LOOP!!!! THAT IS THE ADAPTER THAT GETS SAVED (IN KAGGLE OUTPUT IF YOU WORK ON KAGGLE) AND THIS IS WHAT IS MERGED INTO THE FP16 MODEL THAT IS BEING \"CLEAN\" LOADED IN BELOW CODE**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftConfig\nfrom peft import LoraConfig, PeftModel # ADDED\nfrom trl import SFTTrainer\nfrom transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig, \n                          TrainingArguments, \n                          pipeline, \n                          logging)\n\nmy_model_name = \"phi-3-mini-4k-instruct-ABSA-QUAD\"\nmodel_name = \"microsoft/Phi-3-mini-4k-instruct\"\n\ndevice_map = {\"\": 0}\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=device_map,\n)\nmodel = PeftModel.from_pretrained(base_model, my_model_name)\nmodel = model.merge_and_unload()","metadata":{"execution":{"iopub.status.busy":"2024-09-18T22:00:14.874245Z","iopub.execute_input":"2024-09-18T22:00:14.874677Z","iopub.status.idle":"2024-09-18T22:00:36.209055Z","shell.execute_reply.started":"2024-09-18T22:00:14.874635Z","shell.execute_reply":"2024-09-18T22:00:36.207943Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"The repository for microsoft/Phi-3-mini-4k-instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/Phi-3-mini-4k-instruct.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\nThe repository for microsoft/Phi-3-mini-4k-instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/Phi-3-mini-4k-instruct.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c08eb48251a42f5887af46b0e2e7560"}},"metadata":{}}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-09-18T22:01:40.912871Z","iopub.execute_input":"2024-09-18T22:01:40.913335Z","iopub.status.idle":"2024-09-18T22:01:41.087172Z","shell.execute_reply.started":"2024-09-18T22:01:40.913292Z","shell.execute_reply":"2024-09-18T22:01:41.086249Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-09-18T22:01:22.392119Z","iopub.execute_input":"2024-09-18T22:01:22.392916Z","iopub.status.idle":"2024-09-18T22:01:22.429187Z","shell.execute_reply.started":"2024-09-18T22:01:22.392872Z","shell.execute_reply":"2024-09-18T22:01:22.428070Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb437df1d7bc4720b17403a2d16041f0"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Push to hub - NOTE THIS TAKES A WHILE, LIKE 5 MINS OR SO FOR BIG MODEL O_o","metadata":{}},{"cell_type":"code","source":"model.push_to_hub(my_model_name, use_temp_dir=False)\ntokenizer.push_to_hub(my_model_name, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-18T22:02:07.173261Z","iopub.execute_input":"2024-09-18T22:02:07.173735Z","iopub.status.idle":"2024-09-18T22:05:03.676920Z","shell.execute_reply.started":"2024-09-18T22:02:07.173685Z","shell.execute_reply":"2024-09-18T22:05:03.675760Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e351c37539ee4823903ccdb7587521c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c925f0282e7c4f0f8b5803c8c717604b"}},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/benjaminzwhite/phi-3-mini-4k-instruct-ABSA-QUAD/commit/1b51cedf7d9280c74e58740f5ed730a34650c46e', commit_message='Upload tokenizer', commit_description='', oid='1b51cedf7d9280c74e58740f5ed730a34650c46e', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]}]}