{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Test reloading saved model and use for inference\n","metadata":{}},{"cell_type":"code","source":"import torch\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline","metadata":{"execution":{"iopub.status.busy":"2024-09-19T05:42:36.422537Z","iopub.execute_input":"2024-09-19T05:42:36.423167Z","iopub.status.idle":"2024-09-19T05:42:36.427803Z","shell.execute_reply.started":"2024-09-19T05:42:36.423128Z","shell.execute_reply":"2024-09-19T05:42:36.426431Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Load finetuned model","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n#MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\nMODEL_NAME = \"benjaminzwhite/phi-3-mini-4k-instruct-ABSA-QUAD\"\n\nmodel = AutoModelForCausalLM.from_pretrained( \n    MODEL_NAME,  \n    device_map=device,  \n    torch_dtype=\"auto\",  \n    trust_remote_code=True,  \n) \n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T05:46:52.310353Z","iopub.execute_input":"2024-09-19T05:46:52.310766Z","iopub.status.idle":"2024-09-19T05:50:24.066178Z","shell.execute_reply.started":"2024-09-19T05:46:52.310728Z","shell.execute_reply":"2024-09-19T05:50:24.064220Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9652b446138d4db0bd50f8b81d7a6670"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c348919a253d4f57a794c1808197122a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8eea56605c964513868498fa232c0d16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e2fdd0601604ab68a0047008d3944b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/172 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"462f20cb913b4c5baf689d884f8c8f9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.31k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4c04b07e7f742e6a54b4f4a4a893dc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e4a3961e02046dc875f8afe1c237f18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f21ce8ce1ef347f8bbc8c6d2ec6ebf85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e37b42702d24c68b2acf168876a975b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/455 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ff05b1555ae4c9fa02e0f0ee1766605"}},"metadata":{}}]},{"cell_type":"code","source":"generation_template = \"\"\"<|system|>\nYou are a computer program who only replies with valid JSON lists.<|end|>\n<|user|>\nPerform a full aspect-based sentiment analysis of the following restaurant review:\n\n{review_text}\n<|end|>\n<|assistant|>\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-09-19T05:50:37.018967Z","iopub.execute_input":"2024-09-19T05:50:37.019874Z","iopub.status.idle":"2024-09-19T05:50:37.023893Z","shell.execute_reply.started":"2024-09-19T05:50:37.019833Z","shell.execute_reply":"2024-09-19T05:50:37.022945Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"example_review = \"This place's burgers are the absolute best in town, and even though the service is incredibly slow I'd definitely come back - I want to try the tomato sauce that my friend had which looked delicious!\"","metadata":{"execution":{"iopub.status.busy":"2024-09-19T05:50:41.992522Z","iopub.execute_input":"2024-09-19T05:50:41.993264Z","iopub.status.idle":"2024-09-19T05:50:41.997166Z","shell.execute_reply.started":"2024-09-19T05:50:41.993225Z","shell.execute_reply":"2024-09-19T05:50:41.996313Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# toggle Return full text NO\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500, return_full_text=False) \n\nresult = pipe(generation_template.format(review_text=example_review))\n\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-09-19T05:50:44.145848Z","iopub.execute_input":"2024-09-19T05:50:44.146772Z","iopub.status.idle":"2024-09-19T05:50:49.110353Z","shell.execute_reply.started":"2024-09-19T05:50:44.146732Z","shell.execute_reply":"2024-09-19T05:50:49.109269Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":" [{'opinion term': 'burgers', 'aspect category': 'food quality','sentiment': 'positive', 'justification': 'best'},{'opinion term':'service', 'aspect category':'service general','sentiment': 'negative', 'justification':'slow'},{'opinion term': 'tomato sauce', 'aspect category': 'food quality','sentiment': 'positive', 'justification': 'delicious'}]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Compare with baseline phi (non finetuned)\n\n- note that at the moment the finetuning process is very simple, only 2000 SFT examples 1 epoch etc\n- let's see if baseline phi3 understands output requirement etc","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n#MODEL_NAME = \"benjaminzwhite/phi-3-mini-4k-instruct-ABSA-QUAD\"\n\nmodel = AutoModelForCausalLM.from_pretrained( \n    MODEL_NAME,  \n    device_map=device,  \n    torch_dtype=\"auto\",  \n    trust_remote_code=True,  \n) \n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)","metadata":{"execution":{"iopub.status.busy":"2024-09-19T05:52:49.603736Z","iopub.execute_input":"2024-09-19T05:52:49.604356Z","iopub.status.idle":"2024-09-19T05:53:24.476701Z","shell.execute_reply.started":"2024-09-19T05:52:49.604314Z","shell.execute_reply":"2024-09-19T05:53:24.475908Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d6094f6e73c4189aa69ee5f1cf21cd7"}},"metadata":{}}]},{"cell_type":"code","source":"pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=500, return_full_text=False) \n\nresult = pipe(generation_template.format(review_text=example_review))\n\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-09-19T05:54:37.846935Z","iopub.execute_input":"2024-09-19T05:54:37.847329Z","iopub.status.idle":"2024-09-19T05:54:50.408631Z","shell.execute_reply.started":"2024-09-19T05:54:37.847294Z","shell.execute_reply":"2024-09-19T05:54:50.407682Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":" ```json\n\n{\n\n  \"aspects\": [\n\n    {\n\n      \"aspect\": \"burgers\",\n\n      \"sentiment\": \"positive\",\n\n      \"sentiment_justification\": \"The reviewer describes the burgers as the 'absolute best in town', indicating a very positive sentiment.\"\n\n    },\n\n    {\n\n      \"aspect\": \"service\",\n\n      \"sentiment\": \"negative\",\n\n      \"sentiment_justification\": \"The service is described as 'incredibly slow', which is a negative sentiment.\"\n\n    },\n\n    {\n\n      \"aspect\": \"returning\",\n\n      \"sentiment\": \"positive\",\n\n      \"sentiment_justification\": \"Despite the slow service, the reviewer states they 'would definitely come back', showing a positive sentiment towards the overall experience.\"\n\n    },\n\n    {\n\n      \"aspect\": \"tomato sauce\",\n\n      \"sentiment\": \"positive\",\n\n      \"sentiment_justification\": \"The reviewer expresses a desire to try the tomato sauce because it 'looked delicious', which is a positive sentiment.\"\n\n    }\n\n  ]\n\n}\n\n```\n","output_type":"stream"}]}]}