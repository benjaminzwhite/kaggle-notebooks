{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:26:40.754276Z","iopub.execute_input":"2024-11-30T22:26:40.755087Z","iopub.status.idle":"2024-11-30T22:30:00.866955Z","shell.execute_reply.started":"2024-11-30T22:26:40.755043Z","shell.execute_reply":"2024-11-30T22:30:00.865733Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from unsloth import FastLanguageModel\n\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:31:03.845192Z","iopub.execute_input":"2024-11-30T22:31:03.845535Z","iopub.status.idle":"2024-11-30T22:31:25.381464Z","shell.execute_reply.started":"2024-11-30T22:31:03.845502Z","shell.execute_reply":"2024-11-30T22:31:25.380479Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\nðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:31:49.111623Z","iopub.execute_input":"2024-11-30T22:31:49.112012Z","iopub.status.idle":"2024-11-30T22:32:09.087023Z","shell.execute_reply.started":"2024-11-30T22:31:49.111983Z","shell.execute_reply":"2024-11-30T22:32:09.086084Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2024.11.11: Fast Llama patching. Transformers:4.46.3.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adf6305548a64a33aa2a33c1e6e57dd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2786cbb621a4e43a4864dfdbfb4fdad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca882c7de18d463fb2aeadab426738f4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96b9bdb823724cbd8771e617943fad2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0018b62a5efc4e88bbefeb8beb9f2598"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"RANDOM_SEED = 1359","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:45:18.365451Z","iopub.execute_input":"2024-11-30T22:45:18.366157Z","iopub.status.idle":"2024-11-30T22:45:18.369855Z","shell.execute_reply.started":"2024-11-30T22:45:18.366114Z","shell.execute_reply":"2024-11-30T22:45:18.368995Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = RANDOM_SEED,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:45:21.518547Z","iopub.execute_input":"2024-11-30T22:45:21.519250Z","iopub.status.idle":"2024-11-30T22:45:26.886375Z","shell.execute_reply.started":"2024-11-30T22:45:21.519218Z","shell.execute_reply":"2024-11-30T22:45:26.885708Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2024.11.11 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# Dataset preparation","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:37:24.815438Z","iopub.execute_input":"2024-11-30T22:37:24.815817Z","iopub.status.idle":"2024-11-30T22:37:27.841098Z","shell.execute_reply.started":"2024-11-30T22:37:24.815786Z","shell.execute_reply":"2024-11-30T22:37:27.840202Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/982 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa7a0e6b37a7445b86f35e8b9d146af0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/117M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"443a76f2a2644008bd8c4b21e35b582d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"477a7aff5d344296beb843a09efb41f8"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:37:31.074506Z","iopub.execute_input":"2024-11-30T22:37:31.074868Z","iopub.status.idle":"2024-11-30T22:37:31.081230Z","shell.execute_reply.started":"2024-11-30T22:37:31.074836Z","shell.execute_reply":"2024-11-30T22:37:31.080335Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['conversations', 'source', 'score'],\n    num_rows: 100000\n})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:37:37.659365Z","iopub.execute_input":"2024-11-30T22:37:37.659734Z","iopub.status.idle":"2024-11-30T22:37:37.671479Z","shell.execute_reply.started":"2024-11-30T22:37:37.659702Z","shell.execute_reply":"2024-11-30T22:37:37.670616Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'conversations': [{'from': 'human',\n   'value': 'Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \\n\\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\\n\\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.'},\n  {'from': 'gpt',\n   'value': 'Boolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\\n\\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\\n\\n```python\\nx = 5\\nresult = not (x > 10)  # This expression evaluates to True\\n```\\n\\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\\n\\n```python\\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\\n```\\n\\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\\n\\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\\n\\n```python\\nif x != 0 and (y / x) > 10:\\n    # Perform some operation\\n```\\n\\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here\\'s an example in pseudocode:\\n\\n```\\nif x != 0 {\\n    if (y / x) > 10 {\\n        // Perform some operation\\n    }\\n}\\n```\\n\\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\\n\\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\\n\\n```python\\nx = 5\\nresult = x  # The value of x is truthy, so result is also truthy\\n```\\n\\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\\n\\n```python\\nx = 5\\nresult = bool(x)  # Explicitly converting x to a boolean value\\n```\\n\\nThis ensures that the result is always a boolean value, regardless of the language\\'s truthiness and falsiness rules.'}],\n 'source': 'infini-instruct-top-500k',\n 'score': 5.212620735168457}"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3.1\",\n)\n\ndef formatting_prompts_func(examples):\n    convos = examples[\"conversations\"]\n    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n    return { \"text\" : texts, }\npass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:38:27.604986Z","iopub.execute_input":"2024-11-30T22:38:27.605323Z","iopub.status.idle":"2024-11-30T22:38:27.611774Z","shell.execute_reply.started":"2024-11-30T22:38:27.605294Z","shell.execute_reply":"2024-11-30T22:38:27.610737Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from unsloth.chat_templates import standardize_sharegpt\n\ndataset = standardize_sharegpt(dataset) # converts dataset keys to standard ones for unlsoth : system/user/assistant + all have \"content\" as key for content\ndataset = dataset.map(formatting_prompts_func, batched = True,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:41:55.725256Z","iopub.execute_input":"2024-11-30T22:41:55.725604Z","iopub.status.idle":"2024-11-30T22:42:06.115013Z","shell.execute_reply.started":"2024-11-30T22:41:55.725578Z","shell.execute_reply":"2024-11-30T22:42:06.114123Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Standardizing format:   0%|          | 0/100000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf743299cd8c46b391df827571a70db8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f46a011be6b47aa98eb122ac399bcf1"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"dataset[5][\"conversations\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:42:08.882312Z","iopub.execute_input":"2024-11-30T22:42:08.882623Z","iopub.status.idle":"2024-11-30T22:42:08.888873Z","shell.execute_reply.started":"2024-11-30T22:42:08.882597Z","shell.execute_reply":"2024-11-30T22:42:08.888007Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"[{'content': 'How do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?',\n  'role': 'user'},\n {'content': 'Astronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.',\n  'role': 'assistant'}]"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"dataset[5] \n\n# Llama 3.1 Instruct's default chat template default adds \"Cutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\", so do not be alarmed!","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:43:07.282532Z","iopub.execute_input":"2024-11-30T22:43:07.282919Z","iopub.status.idle":"2024-11-30T22:43:07.289224Z","shell.execute_reply.started":"2024-11-30T22:43:07.282889Z","shell.execute_reply":"2024-11-30T22:43:07.288380Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"{'conversations': [{'content': 'How do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?',\n   'role': 'user'},\n  {'content': 'Astronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.',\n   'role': 'assistant'}],\n 'source': 'WebInstructSub_axolotl',\n 'score': 5.025244235992432,\n 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAstronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.<|eot_id|>'}"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"# Train\n\nset `num_train_epochs = 1` for full train instead of 60 steps","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments, DataCollatorForSeq2Seq\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer), # TODO READ DOCS FOR THIS\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        # num_train_epochs = 1, # Set this for 1 full training run (and delete max_steps=60 if so)\n        max_steps = 60,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = RANDOM_SEED,\n        output_dir = \"outputs\",\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:47:45.654400Z","iopub.execute_input":"2024-11-30T22:47:45.655382Z","iopub.status.idle":"2024-11-30T22:49:12.861201Z","shell.execute_reply.started":"2024-11-30T22:47:45.655342Z","shell.execute_reply":"2024-11-30T22:49:12.860392Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/100000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2e9ce6f937d4509a334a1e5a1ecb56c"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"We also use Unsloth's train_on_completions method to only train on the assistant outputs and ignore the loss on the user's inputs.","metadata":{}},{"cell_type":"code","source":"from unsloth.chat_templates import train_on_responses_only\n\ntrainer = train_on_responses_only(\n    trainer,\n    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:49:37.923118Z","iopub.execute_input":"2024-11-30T22:49:37.923496Z","iopub.status.idle":"2024-11-30T22:50:19.526202Z","shell.execute_reply.started":"2024-11-30T22:49:37.923463Z","shell.execute_reply":"2024-11-30T22:50:19.525237Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c5e6848c0364546ae6fc49e36773fcb"}},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"### Check that the train on response masking has actually worked","metadata":{}},{"cell_type":"code","source":"tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:52:45.152981Z","iopub.execute_input":"2024-11-30T22:52:45.153765Z","iopub.status.idle":"2024-11-30T22:52:45.162051Z","shell.execute_reply.started":"2024-11-30T22:52:45.153727Z","shell.execute_reply":"2024-11-30T22:52:45.161062Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow do astronomers determine the original wavelength of light emitted by a celestial body at rest, which is necessary for measuring its speed using the Doppler effect?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nAstronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.<|eot_id|>'"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n\nprint(space, tokenizer(\" \", add_special_tokens = False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:53:55.355169Z","iopub.execute_input":"2024-11-30T22:53:55.355522Z","iopub.status.idle":"2024-11-30T22:53:55.361461Z","shell.execute_reply.started":"2024-11-30T22:53:55.355490Z","shell.execute_reply":"2024-11-30T22:53:55.360458Z"}},"outputs":[{"name":"stdout","text":"220 {'input_ids': [220], 'attention_mask': [1]}\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])\n\n# BELOW:\n# We can see the System and Instruction prompts are successfully masked!\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:54:39.883279Z","iopub.execute_input":"2024-11-30T22:54:39.884020Z","iopub.status.idle":"2024-11-30T22:54:39.891591Z","shell.execute_reply.started":"2024-11-30T22:54:39.883984Z","shell.execute_reply":"2024-11-30T22:54:39.890768Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'                                                                \\n\\nAstronomers make use of the unique spectral fingerprints of elements found in stars. These elements emit and absorb light at specific, known wavelengths, forming an absorption spectrum. By analyzing the light received from distant stars and comparing it to the laboratory-measured spectra of these elements, astronomers can identify the shifts in these wavelengths due to the Doppler effect. The observed shift tells them the extent to which the light has been redshifted or blueshifted, thereby allowing them to calculate the speed of the star along the line of sight relative to Earth.<|eot_id|>'"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"## Get current memory stats","metadata":{}},{"cell_type":"code","source":"gpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:55:12.950974Z","iopub.execute_input":"2024-11-30T22:55:12.951852Z","iopub.status.idle":"2024-11-30T22:55:12.957419Z","shell.execute_reply.started":"2024-11-30T22:55:12.951818Z","shell.execute_reply":"2024-11-30T22:55:12.956517Z"}},"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.741 GB.\n2.635 GB of memory reserved.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:55:17.553901Z","iopub.execute_input":"2024-11-30T22:55:17.554260Z","iopub.status.idle":"2024-11-30T22:55:18.753116Z","shell.execute_reply.started":"2024-11-30T22:55:17.554228Z","shell.execute_reply":"2024-11-30T22:55:18.752161Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Sat Nov 30 22:55:18 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   45C    P0             29W /   70W |    2817MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   35C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# Go train","metadata":{}},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:55:47.633362Z","iopub.execute_input":"2024-11-30T22:55:47.634139Z","iopub.status.idle":"2024-11-30T23:05:16.039479Z","shell.execute_reply.started":"2024-11-30T22:55:47.634101Z","shell.execute_reply":"2024-11-30T23:05:16.038585Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 100,000 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 60\n \"-____-\"     Number of trainable parameters = 24,313,856\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 09:05, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.939500</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.160900</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.847300</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.748700</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.761400</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.620300</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.833000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.637200</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.818100</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.921500</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.846900</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.829000</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.099300</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.968200</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.828800</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>1.050400</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.735600</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.751200</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.136600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.714000</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.659900</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.047500</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.653300</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.881200</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.723800</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.619900</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.544400</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.950600</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.550600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.598000</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.777000</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.749900</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.871900</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.880000</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.641400</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.714300</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.706100</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.821800</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.745900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.734600</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.669100</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.807500</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.759100</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.664700</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.591200</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.557400</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.656400</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.979200</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.995300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.795300</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.709500</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.784200</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>1.137800</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.894500</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.891600</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.948300</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.690400</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.817200</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.894900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.807200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T23:10:50.321599Z","iopub.execute_input":"2024-11-30T23:10:50.322558Z","iopub.status.idle":"2024-11-30T23:10:50.329226Z","shell.execute_reply.started":"2024-11-30T23:10:50.322521Z","shell.execute_reply":"2024-11-30T23:10:50.328283Z"}},"outputs":[{"name":"stdout","text":"564.4638 seconds used for training.\n9.41 minutes used for training.\nPeak reserved memory = 4.363 GB.\nPeak reserved memory for training = 1.728 GB.\nPeak reserved memory % of max memory = 29.598 %.\nPeak reserved memory for training % of max memory = 11.722 %.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"# Inference\n\n- links to this tweet : https://x.com/menhguin/status/1826132708508213629\n\nHi! Guess the secretâ€™s out: Introducing Min P, a token sampling method for LLMs. Using < 10 lines of code, we achieve 10-20% better results on GSM8K and GPQA vs Top P at temperature=1.\r\n\r\nInterestingly, reasoning on GPQA and creative writing ability *improves* as temperature > 1","metadata":{}},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3.1\",\n)\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\noutputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n                         temperature = 1.5, min_p = 0.1) # see tweet above, read more about this min_p param\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T23:12:37.421493Z","iopub.execute_input":"2024-11-30T23:12:37.422199Z","iopub.status.idle":"2024-11-30T23:12:40.080474Z","shell.execute_reply.started":"2024-11-30T23:12:37.422162Z","shell.execute_reply":"2024-11-30T23:12:40.079608Z"}},"outputs":[{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nContinue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nSure, I can continue the Fibonacci sequence:\\n1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144.<|eot_id|>']"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"### Same with streaming","metadata":{}},{"cell_type":"code","source":"FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\nfrom transformers import TextStreamer\ntext_streamer = TextStreamer(tokenizer, skip_prompt = True)\n_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n                   use_cache = True, temperature = 1.5, min_p = 0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T23:13:44.143296Z","iopub.execute_input":"2024-11-30T23:13:44.143666Z","iopub.status.idle":"2024-11-30T23:13:51.192255Z","shell.execute_reply.started":"2024-11-30T23:13:44.143635Z","shell.execute_reply":"2024-11-30T23:13:51.191378Z"}},"outputs":[{"name":"stdout","text":"The Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding numbers, starting with 0 and 1. \n\nThe sequence given is: 1, 1, 2, 3, 5, 8,...\n\n1. The sequence starts with 1, 1.\n2. The next number is the sum of the previous two numbers, so 1 + 1 = 2.\n3. Then, 1 + 2 = 3.\n4. Next, 2 + 3 = 5.\n5. Finally, 3 + 5 = 8\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"# Saving","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T23:16:07.751248Z","iopub.execute_input":"2024-11-30T23:16:07.751608Z","iopub.status.idle":"2024-11-30T23:16:07.879845Z","shell.execute_reply.started":"2024-11-30T23:16:07.751577Z","shell.execute_reply":"2024-11-30T23:16:07.879206Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"SAVE_NAME = \"TEST-Llama-3.2-3B-Instruct-FineTome-100k\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T23:17:21.381429Z","iopub.execute_input":"2024-11-30T23:17:21.381839Z","iopub.status.idle":"2024-11-30T23:17:21.386351Z","shell.execute_reply.started":"2024-11-30T23:17:21.381807Z","shell.execute_reply":"2024-11-30T23:17:21.385391Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"model.push_to_hub(f\"benjaminzwhite/{SAVE_NAME}-LoRA\", token = hf_token) # Online saving\ntokenizer.push_to_hub(f\"benjaminzwhite/{SAVE_NAME}-LoRA\", token = hf_token) # Online saving","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T23:18:01.764742Z","iopub.execute_input":"2024-11-30T23:18:01.765768Z","iopub.status.idle":"2024-11-30T23:18:06.950146Z","shell.execute_reply.started":"2024-11-30T23:18:01.765712Z","shell.execute_reply":"2024-11-30T23:18:06.949343Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/603 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3512bfeb620d440ca67ea4daefdd8162"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82847a3f7bb0461c8ab93fc922df3b8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/97.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"332c6662a45f4626ae199c9c989c48da"}},"metadata":{}},{"name":"stdout","text":"Saved model to https://huggingface.co/benjaminzwhite/TEST-Llama-3.2-3B-Instruct-FineTome-100k-LoRA\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d7d36ed527740d68901ef9a5bbc8147"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8984471838f412c89d5649bbb581a39"}},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"model.push_to_hub_merged(f\"benjaminzwhite/{SAVE_NAME}\", tokenizer, save_method = \"merged_16bit\", token = hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T23:20:05.671573Z","iopub.execute_input":"2024-11-30T23:20:05.672021Z","iopub.status.idle":"2024-11-30T23:21:20.123277Z","shell.execute_reply.started":"2024-11-30T23:20:05.671988Z","shell.execute_reply":"2024-11-30T23:21:20.122542Z"}},"outputs":[{"name":"stderr","text":"Unsloth: You are pushing to hub in Kaggle environment.\nTo save memory, we shall move benjaminzwhite/TEST-Llama-3.2-3B-Instruct-FineTome-100k to /tmp/TEST-Llama-3.2-3B-Instruct-FineTome-100k\nUnsloth: You have 2 CPUs. Using `safe_serialization` is 10x slower.\nWe shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\nTo force `safe_serialization`, set it to `None` instead.\nUnsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\nmodel which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\nUnsloth: Will remove a cached repo with size 2.2G\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 18.8 out of 31.35 RAM for saving.\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28/28 [00:00<00:00, 31.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Saving tokenizer...","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71f17c9484884643a84ccdd204a79920"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98d902f8126b4400952fa924cfd56827"}},"metadata":{}},{"name":"stdout","text":" Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nUnsloth: Saving /tmp/TEST-Llama-3.2-3B-Instruct-FineTome-100k/pytorch_model-00001-of-00002.bin...\nUnsloth: Saving /tmp/TEST-Llama-3.2-3B-Instruct-FineTome-100k/pytorch_model-00002-of-00002.bin...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c12470fab3e44c88871694038ec87be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57716610b05f4099972e4dc444e8b762"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd20384fa20443c5bbf5888d222ebcdf"}},"metadata":{}},{"name":"stdout","text":"Done.\nSaved merged model to https://huggingface.co/benjaminzwhite/TEST-Llama-3.2-3B-Instruct-FineTome-100k\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"tokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3.1\",\n)\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is a Catalan number?\"},\n]\ninputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True, # Must add for generation\n    return_tensors = \"pt\",\n).to(\"cuda\")\n\noutputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True,\n                         temperature = 1.5, min_p = 0.1) # see tweet above, read more about this min_p param\ntokenizer.batch_decode(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T23:22:10.861642Z","iopub.execute_input":"2024-11-30T23:22:10.862079Z","iopub.status.idle":"2024-11-30T23:22:14.371782Z","shell.execute_reply.started":"2024-11-30T23:22:10.862048Z","shell.execute_reply":"2024-11-30T23:22:14.370885Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is a Catalan number?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe Catalan numbers are a sequence of natural numbers that occur in various counting problems. They are named after the mathematician EugÃ¨ne Charles Catalan, who studied these numbers in the 19th century. The sequence begins like this:\\n\\n0, 1, 2, 5, 14, 34, 89']"},"metadata":{}}],"execution_count":37},{"cell_type":"markdown","source":"# Not very good at Catalan numbers lol","metadata":{}}]}