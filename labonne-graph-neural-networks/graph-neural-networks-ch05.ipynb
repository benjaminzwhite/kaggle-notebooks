{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Labonne - Hands-On Graph Neural Networks Using Python\n\n## Chapter 5 - Including node features with vanilla neural networks\n\n---\n\n**NOTE: CHAPTER USES PyTorch SO RAN NOTEBOOK WITH ACCELERATOR (all previous ones can be CPU only)**\n\n---\n\n- The idea here is that we can add more than just the graph topology: nodes and edges might have features or information attached to them\n- Including this information can improve the quality of the embeddings we obtain\n- Node and edge features have the same structure as a tabular dataset, so standard ML can be applied\n\nWe will first build a vanilla NN on node features (**AFAICT before reading chapter : this means that we're not including the underlying graph topology**) to classify nodes\n\nWe will then include topological information of the graph - **this leads to building a GRAPH NEURAL NETWORK**\n\nThen we compare the performance of the 2 approaches (without / with graph topology in other words)\n\n---\n\n## Datasets\n\n- `Cora`\n- `Facebook Page-Page`\n\n## Cora dataset\n\n- This is most popular dataset for node classification in the literature.\n- network of 2708 (tbc if any updates since book) publications\n- each connection is a Reference (**I think he means citation???**)\n- each publication (node) is a binary vector of 1433 unique words where 0 or 1 indicate whether the corresponding word is present in that publication\n- (**this is known as a BINARY bag of words** in NLP)\n\n**Our goal is to classify each node into one of 7 categories**\n\n### Visualization \n\nGraph data viz : use yEd Live or Gephi (maybe others since book published)","metadata":{}},{"cell_type":"code","source":"!pip install torch_geometric","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:35:45.699683Z","iopub.execute_input":"2024-05-19T16:35:45.700529Z","iopub.status.idle":"2024-05-19T16:36:01.133524Z","shell.execute_reply.started":"2024-05-19T16:35:45.700496Z","shell.execute_reply":"2024-05-19T16:36:01.132522Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting torch_geometric\n  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (4.66.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.11.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2024.2.0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.9.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (2.31.0)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (3.1.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (1.2.2)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch_geometric) (5.9.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch_geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch_geometric) (2024.2.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (3.2.0)\nDownloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.5.3\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**he says PyTorch Geometric has a \"dedicated class\" to download Cora dataset (why is it called Planetoid????)**","metadata":{}},{"cell_type":"code","source":"from torch_geometric.datasets import Planetoid","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:36:53.715270Z","iopub.execute_input":"2024-05-19T16:36:53.716125Z","iopub.status.idle":"2024-05-19T16:37:00.503193Z","shell.execute_reply.started":"2024-05-19T16:36:53.716087Z","shell.execute_reply":"2024-05-19T16:37:00.502407Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"dataset = Planetoid(root=\".\", name=\"Cora\")","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:37:44.648395Z","iopub.execute_input":"2024-05-19T16:37:44.648934Z","iopub.status.idle":"2024-05-19T16:37:53.474202Z","shell.execute_reply.started":"2024-05-19T16:37:44.648905Z","shell.execute_reply":"2024-05-19T16:37:53.473225Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\nDownloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\nProcessing...\nDone!\n","output_type":"stream"}]},{"cell_type":"code","source":"# Cora has only one graph so can store it in dedicated variable\n# NOTE: TODO - IM GUESSING THE PLANETOID thing can store multiple graphs then???\n\ndata = dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:38:27.854022Z","iopub.execute_input":"2024-05-19T16:38:27.854956Z","iopub.status.idle":"2024-05-19T16:38:27.859453Z","shell.execute_reply.started":"2024-05-19T16:38:27.854925Z","shell.execute_reply":"2024-05-19T16:38:27.858430Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Dataset then graph info","metadata":{}},{"cell_type":"code","source":"print(f'Dataset: {dataset}')\nprint('---------------')\nprint(f'Number of graphs: {len(dataset)}')\nprint(f'Number of nodes: {data.x.shape[0]}')\nprint(f'Number of features: {dataset.num_features}')\nprint(f'Number of classes: {dataset.num_classes}')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:38:43.956437Z","iopub.execute_input":"2024-05-19T16:38:43.957179Z","iopub.status.idle":"2024-05-19T16:38:43.964244Z","shell.execute_reply.started":"2024-05-19T16:38:43.957135Z","shell.execute_reply":"2024-05-19T16:38:43.963216Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Dataset: Cora()\n---------------\nNumber of graphs: 1\nNumber of nodes: 2708\nNumber of features: 1433\nNumber of classes: 7\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f'Graph:')\nprint('------')\nprint(f'Edges are directed: {data.is_directed()}')\nprint(f'Graph has isolated nodes: {data.has_isolated_nodes()}')\nprint(f'Graph has loops: {data.has_self_loops()}')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:39:25.547270Z","iopub.execute_input":"2024-05-19T16:39:25.547637Z","iopub.status.idle":"2024-05-19T16:39:25.563150Z","shell.execute_reply.started":"2024-05-19T16:39:25.547609Z","shell.execute_reply":"2024-05-19T16:39:25.562139Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Graph:\n------\nEdges are directed: False\nGraph has isolated nodes: False\nGraph has loops: False\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Facebook Page-Page dataset\n\n- more representative of size of real world social networks\n- from Facebook graph API\n- 22470 nodes: each node represents a Facebook page\n- pages connected when there are mutual likes between them\n- node features are 128 dim vectors: created from textual descriptions written by the owners of the pages\n\n**Goal is to classify each node into one of 4 categories : politicians, companies, tv shows, government organisations**\n\n---\n\nTask is similar to Cora but 3 main diffs:\n\n- many more nodes\n- dimensionality of node features is much smaller (128 vs 1433)\n- 4 categories vs 7 (easier since fewer options)","metadata":{}},{"cell_type":"code","source":"from torch_geometric.datasets import FacebookPagePage\n\ndataset = FacebookPagePage(root=\".\")\n\ndata = dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:43:31.153409Z","iopub.execute_input":"2024-05-19T16:43:31.153816Z","iopub.status.idle":"2024-05-19T16:43:33.818924Z","shell.execute_reply.started":"2024-05-19T16:43:31.153786Z","shell.execute_reply":"2024-05-19T16:43:33.817969Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Downloading https://graphmining.ai/datasets/ptg/facebook.npz\nProcessing...\nDone!\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f'Dataset: {dataset}')\nprint('-----------------------')\nprint(f'Number of graphs: {len(dataset)}')\nprint(f'Number of nodes: {data.x.shape[0]}')\nprint(f'Number of features: {dataset.num_features}')\nprint(f'Number of classes: {dataset.num_classes}')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:43:36.618448Z","iopub.execute_input":"2024-05-19T16:43:36.618804Z","iopub.status.idle":"2024-05-19T16:43:36.625326Z","shell.execute_reply.started":"2024-05-19T16:43:36.618777Z","shell.execute_reply":"2024-05-19T16:43:36.624328Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Dataset: FacebookPagePage()\n-----------------------\nNumber of graphs: 1\nNumber of nodes: 22470\nNumber of features: 128\nNumber of classes: 4\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f'\\nGraph:')\nprint('------')\nprint(f'Edges are directed: {data.is_directed()}')\nprint(f'Graph has isolated nodes: {data.has_isolated_nodes()}')\nprint(f'Graph has loops: {data.has_self_loops()}')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:43:45.603544Z","iopub.execute_input":"2024-05-19T16:43:45.603893Z","iopub.status.idle":"2024-05-19T16:43:45.657205Z","shell.execute_reply.started":"2024-05-19T16:43:45.603867Z","shell.execute_reply":"2024-05-19T16:43:45.656222Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\nGraph:\n------\nEdges are directed: False\nGraph has isolated nodes: False\nGraph has loops: True\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Facebook dataset doesn't have TVT split by default so have to make them: create masks with range() as follows:","metadata":{}},{"cell_type":"code","source":"data.train_mask = range(18000)\ndata.val_mask = range(18001, 20000)\ndata.test_mask = range(20001, 22470)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:44:44.217951Z","iopub.execute_input":"2024-05-19T16:44:44.218325Z","iopub.status.idle":"2024-05-19T16:44:44.223256Z","shell.execute_reply.started":"2024-05-19T16:44:44.218298Z","shell.execute_reply":"2024-05-19T16:44:44.222313Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Was wondering what `data` is that it has these 3 methods - seems that torch_geometric type has it:","metadata":{}},{"cell_type":"code","source":"print(type(data))","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:44:54.987650Z","iopub.execute_input":"2024-05-19T16:44:54.988269Z","iopub.status.idle":"2024-05-19T16:44:54.993052Z","shell.execute_reply.started":"2024-05-19T16:44:54.988240Z","shell.execute_reply":"2024-05-19T16:44:54.992075Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"<class 'torch_geometric.data.data.Data'>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Classifying nodes with vanilla neural networks\n\n- Compared to Zach Karate Club, these graphs now have **node features**\n- we can classify these nodes by using the node features as though they were from a regular tabular dataset, and use a standard NN **NOTE TO BE 100% CLEAR: that this does not include/take into account the topology of the graph**\n\n---\n\ntorch_geometric `data` has `.x` as node features and `.y` as class labels","metadata":{}},{"cell_type":"code","source":"# HE SAID SOMETHING ABOUT HOW pytorch has transforms to calculate random masks, but then just\n# includes this code block\n\n# WE ARE GOING TO BE BACK TO Cora NOW SO I COPIED IT HERE - TODO: confirm later that we use T transform stuff later\n\nimport torch_geometric.transforms as T\n\n\ndataset = Planetoid(root=\".\", name=\"Cora\")\ndata = dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:50:07.953743Z","iopub.execute_input":"2024-05-19T16:50:07.954128Z","iopub.status.idle":"2024-05-19T16:50:07.966492Z","shell.execute_reply.started":"2024-05-19T16:50:07.954100Z","shell.execute_reply":"2024-05-19T16:50:07.965393Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"data.x, data.y","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:51:00.695554Z","iopub.execute_input":"2024-05-19T16:51:00.695989Z","iopub.status.idle":"2024-05-19T16:51:00.704353Z","shell.execute_reply.started":"2024-05-19T16:51:00.695962Z","shell.execute_reply":"2024-05-19T16:51:00.703321Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         ...,\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]),\n tensor([3, 4, 4,  ..., 3, 3, 3]))"},"metadata":{}}]},{"cell_type":"code","source":"data.x[0], type(data.x[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:52:48.011309Z","iopub.execute_input":"2024-05-19T16:52:48.011643Z","iopub.status.idle":"2024-05-19T16:52:48.018926Z","shell.execute_reply.started":"2024-05-19T16:52:48.011618Z","shell.execute_reply":"2024-05-19T16:52:48.017938Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(tensor([0., 0., 0.,  ..., 0., 0., 0.]), torch.Tensor)"},"metadata":{}}]},{"cell_type":"code","source":"# -- convert the x and y in torch_geometric data to pandas dataframe\n\nimport pandas as pd\n\ndf_x = pd.DataFrame(data.x.numpy()) # x contains torch.Tensors (see type call just above I did for myself to check)\n\n# add label from the data.y of torch_geometric object\ndf_x[\"label\"] = pd.DataFrame(data.y)\n\ndf_x","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:53:39.801082Z","iopub.execute_input":"2024-05-19T16:53:39.801881Z","iopub.status.idle":"2024-05-19T16:53:39.998303Z","shell.execute_reply.started":"2024-05-19T16:53:39.801845Z","shell.execute_reply":"2024-05-19T16:53:39.997260Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"        0    1    2    3    4    5  ...  1428  1429  1430  1431  1432  label\n0     0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0      3\n1     0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0      4\n2     0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0      4\n3     0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0      0\n4     0.0  0.0  0.0  1.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0      3\n...   ...  ...  ...  ...  ...  ...  ...   ...   ...   ...   ...   ...    ...\n2703  0.0  0.0  0.0  0.0  0.0  1.0  ...   0.0   0.0   0.0   0.0   0.0      3\n2704  0.0  0.0  1.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0      3\n2705  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0      3\n2706  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0      3\n2707  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   0.0   0.0   0.0   0.0      3\n\n[2708 rows x 1434 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>10</th>\n      <th>11</th>\n      <th>12</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>21</th>\n      <th>22</th>\n      <th>23</th>\n      <th>24</th>\n      <th>25</th>\n      <th>26</th>\n      <th>27</th>\n      <th>28</th>\n      <th>29</th>\n      <th>30</th>\n      <th>31</th>\n      <th>32</th>\n      <th>33</th>\n      <th>34</th>\n      <th>35</th>\n      <th>36</th>\n      <th>37</th>\n      <th>38</th>\n      <th>39</th>\n      <th>...</th>\n      <th>1394</th>\n      <th>1395</th>\n      <th>1396</th>\n      <th>1397</th>\n      <th>1398</th>\n      <th>1399</th>\n      <th>1400</th>\n      <th>1401</th>\n      <th>1402</th>\n      <th>1403</th>\n      <th>1404</th>\n      <th>1405</th>\n      <th>1406</th>\n      <th>1407</th>\n      <th>1408</th>\n      <th>1409</th>\n      <th>1410</th>\n      <th>1411</th>\n      <th>1412</th>\n      <th>1413</th>\n      <th>1414</th>\n      <th>1415</th>\n      <th>1416</th>\n      <th>1417</th>\n      <th>1418</th>\n      <th>1419</th>\n      <th>1420</th>\n      <th>1421</th>\n      <th>1422</th>\n      <th>1423</th>\n      <th>1424</th>\n      <th>1425</th>\n      <th>1426</th>\n      <th>1427</th>\n      <th>1428</th>\n      <th>1429</th>\n      <th>1430</th>\n      <th>1431</th>\n      <th>1432</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2703</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2704</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2705</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2706</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2707</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>2708 rows × 1434 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**NOTE TO MYSELF: I saw 0's everywhere so looked carefully and there are indeed 1s, note that this is a bag of words model so it seems in general that the vectors are very sparse?? anyway, just wanted to check that there's no mistake - matrix does indeed contain nonzero elements O_o**\n\n---\n\nWe build a simple MLP, train it on `data.x` to predict the labels in `data.y`\n\nWe create our own MLP class with 4 methods\n\n- init\n- forward\n- fit\n- test\n\n---\n\n**Which metric to evaluate with?:** we'll use simple accuracy. It's not the best for multiclass classification, but it is simpler to understand (CAN REPLACE WITH METRIC OF YOUR CHOICE LATER - **ROC AUC, f1** etc)","metadata":{}},{"cell_type":"code","source":"def accuracy(y_pred, y_true):\n    return torch.sum(y_pred == y_true) / len(y_true)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:57:59.462026Z","iopub.execute_input":"2024-05-19T16:57:59.462433Z","iopub.status.idle":"2024-05-19T16:57:59.467194Z","shell.execute_reply.started":"2024-05-19T16:57:59.462402Z","shell.execute_reply":"2024-05-19T16:57:59.466136Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# -- Custom simple MLP for classification --\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import Linear","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:16:03.625794Z","iopub.execute_input":"2024-05-19T17:16:03.626628Z","iopub.status.idle":"2024-05-19T17:16:03.632658Z","shell.execute_reply.started":"2024-05-19T17:16:03.626586Z","shell.execute_reply":"2024-05-19T17:16:03.631147Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class MLP(torch.nn.Module):\n    \n    def __init__(self, dim_in, dim_h, dim_out):\n        super().__init__()\n        \n        self.linear1 = Linear(dim_in, dim_h) #  h is the hidden layer\n        self.linear2 = Linear(dim_h, dim_out)\n        \n    def forward(self, x):\n        x = self.linear1(x)\n        x = torch.relu(x)\n        x = self.linear2(x)\n        \n        return F.log_softmax(x, dim=1)\n    \n    def fit(self, data, epochs):\n        # this is used for the training loop\n        criterion = torch.nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(self.parameters(),\n                                    lr=0.01,\n                                    weight_decay=5e-4,\n                                    )\n        \n        self.train()\n        for epoch in range(epochs+1):\n            optimizer.zero_grad()\n            out = self(data.x) # I DONT UNDERSTAND THIS?? WE ARE SENDING ALL DATA, NOT JUST THE TRAIN_MASK PART?!?!\n            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n            accuracy_score = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n            loss.backward()\n            optimizer.step()\n            \n            if epoch % 20 == 0:\n                # EVALUATE ON VALIDATION SPLIT\n                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n                val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n                print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc: {accuracy_score*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')\n    \n    def test(self, data):\n        # EVALUATES MODEL ON THE TEST SPLIT AND RETURNS ACCURACY\n        self.eval()\n        out = self(data.x)\n        accuracy_score = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n        return accuracy_score","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:27:22.812932Z","iopub.execute_input":"2024-05-19T17:27:22.813455Z","iopub.status.idle":"2024-05-19T17:27:22.825518Z","shell.execute_reply.started":"2024-05-19T17:27:22.813419Z","shell.execute_reply":"2024-05-19T17:27:22.824503Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Create a MLP and check its layers seem correct:","metadata":{}},{"cell_type":"code","source":"# FOR NOW THIS IS WITH Cora SINCE THAT IS MOST RECENTLY LOADED DATASET\n\n# 16 hidden neurons it seems he uses in example:\n\nmlp = MLP(dataset.num_features, 16, dataset.num_classes)\n\nprint(mlp)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:27:27.546092Z","iopub.execute_input":"2024-05-19T17:27:27.546476Z","iopub.status.idle":"2024-05-19T17:27:27.553146Z","shell.execute_reply.started":"2024-05-19T17:27:27.546449Z","shell.execute_reply":"2024-05-19T17:27:27.552089Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"MLP(\n  (linear1): Linear(in_features=1433, out_features=16, bias=True)\n  (linear2): Linear(in_features=16, out_features=7, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"mlp.fit(data, epochs=100)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:27:29.562628Z","iopub.execute_input":"2024-05-19T17:27:29.563513Z","iopub.status.idle":"2024-05-19T17:27:30.046503Z","shell.execute_reply.started":"2024-05-19T17:27:29.563479Z","shell.execute_reply":"2024-05-19T17:27:30.045410Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Epoch   0 | Train Loss: 1.950 | Train Acc: 12.86% | Val Loss: 1.91 | Val Acc: 28.60%\nEpoch  20 | Train Loss: 0.100 | Train Acc: 100.00% | Val Loss: 1.50 | Val Acc: 52.60%\nEpoch  40 | Train Loss: 0.011 | Train Acc: 100.00% | Val Loss: 1.59 | Val Acc: 51.80%\nEpoch  60 | Train Loss: 0.007 | Train Acc: 100.00% | Val Loss: 1.61 | Val Acc: 49.80%\nEpoch  80 | Train Loss: 0.007 | Train Acc: 100.00% | Val Loss: 1.49 | Val Acc: 52.40%\nEpoch 100 | Train Loss: 0.008 | Train Acc: 100.00% | Val Loss: 1.42 | Val Acc: 53.40%\n","output_type":"stream"}]},{"cell_type":"code","source":"# evaluate final model on test data split\n\naccuracy_score = mlp.test(data)\nprint(f'MLP test accuracy: {accuracy_score*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:28:41.648513Z","iopub.execute_input":"2024-05-19T17:28:41.649234Z","iopub.status.idle":"2024-05-19T17:28:41.657232Z","shell.execute_reply.started":"2024-05-19T17:28:41.649203Z","shell.execute_reply":"2024-05-19T17:28:41.656104Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"MLP test accuracy: 50.60%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now do same for the Facebook dataset - have to reload it:","metadata":{}},{"cell_type":"code","source":"fb_dataset = FacebookPagePage(root=\".\")\n\nfb_data = fb_dataset[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:29:28.528806Z","iopub.execute_input":"2024-05-19T17:29:28.529669Z","iopub.status.idle":"2024-05-19T17:29:28.542436Z","shell.execute_reply.started":"2024-05-19T17:29:28.529634Z","shell.execute_reply":"2024-05-19T17:29:28.541441Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# forgot to repeat this so got error\n# AttributeError: 'GlobalStorage' object has no attribute 'train_mask'\n# when tried training fb_mlp below\n\nfb_data.train_mask = range(18000)\nfb_data.val_mask = range(18001, 20000)\nfb_data.test_mask = range(20001, 22470)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:31:07.151340Z","iopub.execute_input":"2024-05-19T17:31:07.151714Z","iopub.status.idle":"2024-05-19T17:31:07.156940Z","shell.execute_reply.started":"2024-05-19T17:31:07.151688Z","shell.execute_reply":"2024-05-19T17:31:07.155960Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"fb_mlp = MLP(fb_dataset.num_features, 16, fb_dataset.num_classes)\n\nprint(fb_mlp)\n\nfb_mlp.fit(fb_data, epochs=100)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T17:31:10.532823Z","iopub.execute_input":"2024-05-19T17:31:10.533531Z","iopub.status.idle":"2024-05-19T17:31:14.776673Z","shell.execute_reply.started":"2024-05-19T17:31:10.533500Z","shell.execute_reply":"2024-05-19T17:31:14.775664Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"MLP(\n  (linear1): Linear(in_features=128, out_features=16, bias=True)\n  (linear2): Linear(in_features=16, out_features=4, bias=True)\n)\nEpoch   0 | Train Loss: 1.492 | Train Acc: 15.95% | Val Loss: 1.48 | Val Acc: 17.26%\nEpoch  20 | Train Loss: 0.682 | Train Acc: 73.68% | Val Loss: 0.70 | Val Acc: 73.39%\nEpoch  40 | Train Loss: 0.584 | Train Acc: 76.61% | Val Loss: 0.61 | Val Acc: 74.89%\nEpoch  60 | Train Loss: 0.555 | Train Acc: 78.09% | Val Loss: 0.60 | Val Acc: 75.94%\nEpoch  80 | Train Loss: 0.537 | Train Acc: 78.61% | Val Loss: 0.59 | Val Acc: 75.49%\nEpoch 100 | Train Loss: 0.525 | Train Acc: 79.07% | Val Loss: 0.59 | Val Acc: 75.69%\n","output_type":"stream"}]},{"cell_type":"code","source":"fb_accuracy_score = fb_mlp.test(fb_data)\nprint(f'MLP test accuracy: {fb_accuracy_score*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T19:35:45.932606Z","iopub.execute_input":"2024-05-19T19:35:45.933331Z","iopub.status.idle":"2024-05-19T19:35:45.945300Z","shell.execute_reply.started":"2024-05-19T19:35:45.933298Z","shell.execute_reply":"2024-05-19T19:35:45.944325Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"MLP test accuracy: 74.85%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Classifying the same nodes but with vanilla GRAPH neural networks\n\nBuild our own before using existing library ones\n\n- currently our input vectors are just node features\n- nodes are \"separate from eachother\": not good enough to get a good understanding of the graph\n- it's like with pixels in image/CV: to understand a node you need to look at its neighborhood\n\nLet's call `N_A` the set of neighbors of the node `A`\n\nOur **graph linear layer** will be taken as follows:\n\n`h_A = sum over i belonging to N_A of : x_i . W-transpose`\n\nwhere `x_i` are the input vectors for each node in `N_A`\n\n(aside, he says \"you can imagine more complex variants of this approach - like for example a weight matrix W_1 dedicated to the central node, and another one W_2 for the neighbors etc.\" - **basically seems can extend this simple approch TODO: later, read literature see what else is out there**)\n\n---\n\nNow instead of doing this node by node, we can rewrite the equation for a linear layer as follows:\n\n`H = X . W-transpose` \n\nwhere now X is the \"input matrix\" (matrix of all the input vectors of all the nodes in the graph)\n\n(**UPDATE: I think eqn above is just for the \"basic\" linear layer WITHOUT GRAPH INFO - it's unclear in book, because below we have another `H = ...` which is for the graph stuff this time**)\n\nFor the graph, the **adjacency matrix `A`** contains the connections between every node in the graph:\n\nmultiplying the input matrix, `X`, by this adjacency matrix `A` will directly sum up the \"neighboring node features\" as done earlier on the single vector example.\n\n**We can add \"self loops\" to `A` so that the central node is also considered (i.e. include \"current node\" in set of \"neighbors\" to sum over) - can do this by usual trick of ADDING IDENTITY MATRIX TO ADJACENCY MATRIX**\n\n`A_tilde = A + I`\n\nSo now our **graph** linear layer can be written as (I ADDED SUBSCRIPT `_g` TO BE CLEAR IT'S NOT THE SAME `H` AS ABOVE)\n\n`H_g = A_tilde-transpose . X . W-transpose`\n\n---\n\nLet's implement this in PyTorch Geometric, so we can use later as a layer when building GNNs:\n\n**note this is the basic \"component\" layer, it's a basic linear transformation WITHOUT BIAS ALSO**\n\n\n","metadata":{}},{"cell_type":"code","source":"class VanillaGNNLayer(torch.nn.Module):\n    \n    def __init__(self, dim_in, dim_out):\n        # NUMBER OF FEATURES OF THE INPUT , NUMBER OF FEATURES OF THE OUTPUT\n        super().__init__()\n        \n        self.linear = Linear(dim_in, dim_out, bias=False)\n        \n    def forward(self, x, adjacency_matrix):\n        # perform the linear transformation ...\n        x = self.linear(x)\n        # ... then the multiplication with the adjacency matrix TODO: NOT CLEAR WHAT THE sparse IS ABOUT HERE\n        x = torch.sparse.mm(adjacency_matrix, x)\n        return x\n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-19T19:06:58.931391Z","iopub.execute_input":"2024-05-19T19:06:58.931801Z","iopub.status.idle":"2024-05-19T19:06:58.938963Z","shell.execute_reply.started":"2024-05-19T19:06:58.931770Z","shell.execute_reply":"2024-05-19T19:06:58.937827Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"Before we can create our vanilla GNN, we need to:\n\n1. convert the edge_index from our dataset into dense adjacency matrix `A`\n2. create `A_tilde` by adding identity matrix (to account for self loops / current node being taken into account by their own embeddings - see above for discussion)\n\n---\n\n# NOTE:\n\nit's not clear in book since he uses `data` for everything but currently in notebook I have:\n\n- `data` is Cora dataset\n- `fb_data` is Facebook dataset","metadata":{}},{"cell_type":"code","source":"data.edge_index # I don't understand what this is 100% yet TODO: CHECK","metadata":{"execution":{"iopub.status.busy":"2024-05-19T19:10:53.019580Z","iopub.execute_input":"2024-05-19T19:10:53.020475Z","iopub.status.idle":"2024-05-19T19:10:53.026703Z","shell.execute_reply.started":"2024-05-19T19:10:53.020442Z","shell.execute_reply":"2024-05-19T19:10:53.025802Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"tensor([[ 633, 1862, 2582,  ...,  598, 1473, 2706],\n        [   0,    0,    0,  ..., 2707, 2707, 2707]])"},"metadata":{}}]},{"cell_type":"markdown","source":"[https://pytorch-geometric.readthedocs.io/en/1.4.3/modules/data.html](https://pytorch-geometric.readthedocs.io/en/1.4.3/modules/data.html)\n\nsays:\n\nedge_index (LongTensor, optional) – Graph connectivity in COO format with shape [2, num_edges]. (default: None)\n\n**So does this mean it's a \"list\" of the edges with (node1,node2) format???** I think that's what it means","metadata":{}},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2024-05-19T19:10:59.299262Z","iopub.execute_input":"2024-05-19T19:10:59.299613Z","iopub.status.idle":"2024-05-19T19:10:59.305793Z","shell.execute_reply.started":"2024-05-19T19:10:59.299588Z","shell.execute_reply":"2024-05-19T19:10:59.304893Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"},"metadata":{}}]},{"cell_type":"code","source":"from torch_geometric.utils import to_dense_adj\n\n# build basic adj matrix, A, using inbuilt function\nadjacency = to_dense_adj(data.edge_index)[0]\n\n# build A_tilde, which is A + I identity matrix\nadjacency += torch.eye(len(adjacency))\n\nadjacency","metadata":{"execution":{"iopub.status.busy":"2024-05-19T19:21:32.645315Z","iopub.execute_input":"2024-05-19T19:21:32.645687Z","iopub.status.idle":"2024-05-19T19:21:32.679875Z","shell.execute_reply.started":"2024-05-19T19:21:32.645660Z","shell.execute_reply":"2024-05-19T19:21:32.678903Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n        [0., 1., 1.,  ..., 0., 0., 0.],\n        [0., 1., 1.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 1., 0., 0.],\n        [0., 0., 0.,  ..., 0., 1., 1.],\n        [0., 0., 0.,  ..., 0., 1., 1.]])"},"metadata":{}}]},{"cell_type":"markdown","source":"Now we can implement the vanilla GNN, very similar to the MLP we built earlier:\n\n\n**we build our first GNN with 2 linear layers**","metadata":{}},{"cell_type":"code","source":"class VanillaGNN(torch.nn.Module):\n    \n    def __init__(self, dim_in, dim_h, dim_out):\n        super().__init__()\n        \n        self.gnn1 = VanillaGNNLayer(dim_in, dim_h)\n        self.gnn2 = VanillaGNNLayer(dim_h, dim_out)\n        \n    def forward(self, x, adjacency_matrix):\n        # include adj info now in forward()\n        h = self.gnn1(x, adjacency_matrix)\n        h = torch.relu(h)\n        h = self.gnn2(h, adjacency_matrix)\n        \n        return F.log_softmax(h, dim=1)\n    \n    # --------------------------\n    # fit and test are unchanged\n    # JUST NEED TO ADD ADJACENCY_MATRIX THOUGH\n    # --------------------------\n    def fit(self, data, epochs):\n        # this is used for the training loop\n        criterion = torch.nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(self.parameters(),\n                                    lr=0.01,\n                                    weight_decay=5e-4,\n                                    )\n\n        self.train()\n        for epoch in range(epochs+1):\n            optimizer.zero_grad()\n            #out = self(data.x) # I DONT UNDERSTAND THIS?? WE ARE SENDING ALL DATA, NOT JUST THE TRAIN_MASK PART?!?!\n            \n            out = self(data.x, adjacency) # NOTE: REALLY CONFUSING - THIS IS A \"GLOBAL\" VARIABLE IN HIS CODE - IT'S THE ACTUAL adjacency created in an earlier cell\n            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n            accuracy_score = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n            loss.backward()\n            optimizer.step()\n\n            if epoch % 20 == 0:\n                # EVALUATE ON VALIDATION SPLIT\n                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n                val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n                print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc: {accuracy_score*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')\n    \n    def test(self, data):\n        # EVALUATES MODEL ON THE TEST SPLIT AND RETURNS ACCURACY\n        self.eval()\n        #out = self(data.x)\n        out = self(data.x, adjacency) # NOTE: REALLY CONFUSING - THIS IS A \"GLOBAL\" VARIABLE IN HIS CODE - IT'S THE ACTUAL adjacency created in an earlier cell\n        accuracy_score = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n        return accuracy_score\n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-19T19:24:00.129559Z","iopub.execute_input":"2024-05-19T19:24:00.130332Z","iopub.status.idle":"2024-05-19T19:24:00.142402Z","shell.execute_reply.started":"2024-05-19T19:24:00.130299Z","shell.execute_reply":"2024-05-19T19:24:00.141453Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# create, train, evaluate our GNN\ngnn = VanillaGNN(dataset.num_features, 16, dataset.num_classes)\n\nprint(gnn)\n\ngnn.fit(data, epochs=100)\n\naccuracy_score = gnn.test(data)\nprint(f'\\nGNN test accuracy: {accuracy_score*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T19:24:02.383230Z","iopub.execute_input":"2024-05-19T19:24:02.383831Z","iopub.status.idle":"2024-05-19T19:24:04.206256Z","shell.execute_reply.started":"2024-05-19T19:24:02.383800Z","shell.execute_reply":"2024-05-19T19:24:04.205298Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"VanillaGNN(\n  (gnn1): VanillaGNNLayer(\n    (linear): Linear(in_features=1433, out_features=16, bias=False)\n  )\n  (gnn2): VanillaGNNLayer(\n    (linear): Linear(in_features=16, out_features=7, bias=False)\n  )\n)\nEpoch   0 | Train Loss: 2.299 | Train Acc: 14.29% | Val Loss: 2.18 | Val Acc: 15.80%\nEpoch  20 | Train Loss: 0.266 | Train Acc: 94.29% | Val Loss: 1.69 | Val Acc: 68.00%\nEpoch  40 | Train Loss: 0.011 | Train Acc: 100.00% | Val Loss: 2.61 | Val Acc: 70.00%\nEpoch  60 | Train Loss: 0.003 | Train Acc: 100.00% | Val Loss: 2.95 | Val Acc: 68.80%\nEpoch  80 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 2.95 | Val Acc: 68.40%\nEpoch 100 | Train Loss: 0.002 | Train Acc: 100.00% | Val Loss: 2.85 | Val Acc: 69.40%\n\nGNN test accuracy: 72.80%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## NOTE:\n\nIn book he says \"do the same with Facebook dataset\" but because the Vanilla GNN above has HARDCODED `adjacency` in the `fit` and `test` method you would have to copy everything out, and recreate the adjacency matrix for Facebook dataset\n\n**very unclear that this is lurking in the code - so I make my own V2 below, which accepts the SPECIFIC ADJ MATRIX YOU WANT TO USE IN THE ARGS OF THE CLASS**:","metadata":{}},{"cell_type":"code","source":"class VanillaGNN_v2_WithAdjacencyMatrix(torch.nn.Module):\n    \n    def __init__(self, dim_in, dim_h, dim_out, adjacency_matrix):\n        super().__init__()\n        \n        self.gnn1 = VanillaGNNLayer(dim_in, dim_h)\n        self.gnn2 = VanillaGNNLayer(dim_h, dim_out)\n        self.adjacency_matrix = adjacency_matrix\n        \n    def forward(self, x):\n        # include adj info now in forward()\n        h = self.gnn1(x, self.adjacency_matrix)\n        h = torch.relu(h)\n        h = self.gnn2(h, self.adjacency_matrix)\n        \n        return F.log_softmax(h, dim=1)\n    \n    # --------------------------\n    # fit and test are unchanged\n    # JUST NEED TO ADD ADJACENCY_MATRIX THOUGH\n    # --------------------------\n    def fit(self, data, epochs):\n        # this is used for the training loop\n        criterion = torch.nn.CrossEntropyLoss()\n        optimizer = torch.optim.Adam(self.parameters(),\n                                    lr=0.01,\n                                    weight_decay=5e-4,\n                                    )\n\n        self.train()\n        for epoch in range(epochs+1):\n            optimizer.zero_grad()\n            out = self(data.x) # we now use self.adjacency_matrix in forward() so dont need to pass it locally here\n            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n            accuracy_score = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n            loss.backward()\n            optimizer.step()\n\n            if epoch % 20 == 0:\n                # EVALUATE ON VALIDATION SPLIT\n                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n                val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n                print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc: {accuracy_score*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')\n    \n    def test(self, data):\n        # EVALUATES MODEL ON THE TEST SPLIT AND RETURNS ACCURACY\n        self.eval()\n        out = self(data.x) # we now use self.adjacency_matrix in forward() so dont need to pass it locally here\n        accuracy_score = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n        return accuracy_score","metadata":{"execution":{"iopub.status.busy":"2024-05-19T19:33:05.287023Z","iopub.execute_input":"2024-05-19T19:33:05.288073Z","iopub.status.idle":"2024-05-19T19:33:05.300671Z","shell.execute_reply.started":"2024-05-19T19:33:05.288038Z","shell.execute_reply":"2024-05-19T19:33:05.299800Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"Now create the adj matrix for the **Facebook dataset** following steps for the Cora dataset:","metadata":{}},{"cell_type":"code","source":"# build basic adj matrix, A, using inbuilt function\nfb_adjacency_matrix = to_dense_adj(fb_data.edge_index)[0]\n\n# build A_tilde, which is A + I identity matrix\nfb_adjacency_matrix += torch.eye(len(fb_adjacency_matrix))\n\nfb_adjacency_matrix # dont worry if it looks like only 1's on diag, there are indeed 1s elsewhere just sparse","metadata":{"execution":{"iopub.status.busy":"2024-05-19T19:33:08.019492Z","iopub.execute_input":"2024-05-19T19:33:08.020322Z","iopub.status.idle":"2024-05-19T19:33:09.563604Z","shell.execute_reply.started":"2024-05-19T19:33:08.020288Z","shell.execute_reply":"2024-05-19T19:33:09.562614Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n        [0., 1., 0.,  ..., 0., 0., 0.],\n        [0., 0., 1.,  ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0.,  ..., 1., 0., 0.],\n        [0., 0., 0.,  ..., 0., 1., 0.],\n        [0., 0., 0.,  ..., 0., 0., 1.]])"},"metadata":{}}]},{"cell_type":"markdown","source":"Now pass this specific Facebook adj matrix to our v2 GNN class, and train / evaluate as before:","metadata":{}},{"cell_type":"code","source":"# create, train, evaluate our GNN\nfb_gnn = VanillaGNN_v2_WithAdjacencyMatrix(fb_dataset.num_features,\n                                           16,\n                                           fb_dataset.num_classes,\n                                           fb_adjacency_matrix,\n                                          )\n\nprint(fb_gnn)\n\nfb_gnn.fit(fb_data, epochs=100)\n\nfb_accuracy_score = fb_gnn.test(fb_data)\nprint(f'\\nGNN test accuracy: {fb_accuracy_score*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T19:33:09.933089Z","iopub.execute_input":"2024-05-19T19:33:09.933976Z","iopub.status.idle":"2024-05-19T19:34:57.662120Z","shell.execute_reply.started":"2024-05-19T19:33:09.933942Z","shell.execute_reply":"2024-05-19T19:34:57.661193Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"VanillaGNN_v2_WithAdjacencyMatrix(\n  (gnn1): VanillaGNNLayer(\n    (linear): Linear(in_features=128, out_features=16, bias=False)\n  )\n  (gnn2): VanillaGNNLayer(\n    (linear): Linear(in_features=16, out_features=4, bias=False)\n  )\n)\nEpoch   0 | Train Loss: 44.444 | Train Acc: 30.32% | Val Loss: 39.78 | Val Acc: 28.86%\nEpoch  20 | Train Loss: 3.736 | Train Acc: 79.31% | Val Loss: 2.52 | Val Acc: 80.54%\nEpoch  40 | Train Loss: 1.536 | Train Acc: 81.63% | Val Loss: 1.31 | Val Acc: 83.34%\nEpoch  60 | Train Loss: 0.865 | Train Acc: 83.93% | Val Loss: 0.85 | Val Acc: 83.69%\nEpoch  80 | Train Loss: 0.647 | Train Acc: 85.47% | Val Loss: 0.67 | Val Acc: 84.94%\nEpoch 100 | Train Loss: 0.563 | Train Acc: 86.10% | Val Loss: 0.57 | Val Acc: 85.49%\n\nGNN test accuracy: 84.89%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Summary of results\n\nAt start of chapter, using basic MLP without graph information we got:\n\n- 50.4% accuracy for Cora and\n- 74.8% for Facebook\n\nWith our basic vanilla GNN we get:\n\n- 72.8% for Cora\n- 84.9% for Facebook\n\nEven with this basic implementation, considering the neighborhood of each node gives +10-20 points boost in performance !\n\nWe will extend and build on this in later chapters; in next chapter by correctly normalizing inputs and obtaining a **graph convolutional network** model.","metadata":{}}]}