{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Labonne - Hands-On Graph Neural Networks Using Python\n\n## Chapter 4 - Improving embeddings with biased random walks in Node2Vec\n\n- how to improve DeepWalk/the quality of the embeddings we produced in chapter 3??\n- we can do this by modifying how the random walks are generated\n- we are going to see how to find the best params for a given graph, and implement **Node2Vec**\n- we use Node2Vec to build a **movie recommendation system** \n\n## Node2Vec\n\n- 2016\n- random walks and word2vec like DeepWalk\n- difference is instead of obtaining sequences of nodes with a uniform distribution, the random walks are biased\n\n### Defining a neighborhood\n\n- the key concept in Node2Vec is the \"flexible\" idea of what the neighborhood of a node is\n\nSay you wannt to explore 3 nodes in neighborhood of a node, A.\n\n1. you could say \"get the 3 closest\" **this corresponds to doing BFS**\n2. or you could say \"get nodes which are not adjacent to the previous node\" **this is doing DFS**\n\nThese sampling strategies can be thought of as focusing on \"local\" vs \"global\" view of the graph around a node\n\nNode2Vec says that, counterintuitively (?), the different approaches capture different but valuable representations of the network\n\n### Introducing biases in random walks\n\nReminder; the idea here is that \"nodes that appear together often in random walks are like words that appear together often in sentences\" - under the **homophily hypothesis** they share a similar meaning, hence should share a similar representation.\n\nIn Node2Vec idea is to bias randomness of the walks to either:\n\n1. promote nodes that are close to the previous one (similar to BFS)\n2. promote nodes that are not connected to previous one (similar to DFS)\n\n#### Theory (again a bit unclear - just code it then search explanation)\n\nImagine a graph where you are at current node j, there is a previous node i, and future nodes k1 and k2 (both connected to j. k1 is also connected to i, but k2 is not)\n\n- call P_jk  the unnormalized transition prob from node j to node k\n- `P_jk = a(i,k) * W_jk` were `a(i,k)` is the **search bias** between node i and node k (**this is underexplained theoretically in book** basically seems to be an idea of how easy it is to get directly to \"potential next node k\" from the previous node, i. W_jk is the **weight of the edge** from j to k, **again underexplained in book (we haven't mentioned weights on edges at any point so far)**\n\nIn Node2Vec the **search bias terms, a, are defined based on:**\n\n- the distance between the 2 nodes in question and\n- two more parameters, p and q, (STUPID NAMES, WHY NOT DFS_COEFF and BFS_COEFF FOR EXAMPLE???) the \"return parameter\" and the \"in-out parameter\". They are approximating the importance of DFS and BFS.\n\nDefinition of a(x,y):\n\n```\na(x,y)   |  1/p   if dist(a,b) == 0\n         |  1     if dist(a,b) == 1\n         |  1/q   if dist(a,b) == 2\n```\n\nHere `dist(a,b)` is the shortest path distance between nodes a and b **NOTE: NOT STATED IN BOOK - but AFAICT, since you only compute this in the case WHERE YOU ALREADY HAVE A `i,j,k` TRIPLE OF NODES AND YOU ARE CURRENTLY AT `j`, THEN YOU KNOW THAT THE MAX DISTANCE FROM `i to k` IS ALWAYS GOING TO BE 2, SINCE AT WORST YOU CAN TAKE THE PATH `i -> j -> k` which exists by assumption**\n\n\n**NOTE: AGAIN REALLY UNCLEAR IN BOOK - IN ILLUSTRATION IT SHOWS `j -> i` BEING a = 1/p BUT WHY!?!??! THE DISTANCE BETWEEN THESE 2 NODES IS 1 ??!?! NOT 0 ?!?!** It says as explantion \"the walk starts at i and now arrives at j. The probability of going back to previous node i is controlled by parameter p\". (So is the stuff about dist==0 above incorrect????)\n\nLater on he says instead (p55): \"1/p if neighbor is previous node, 1 if neighbor is connected to previous node, 1/q otherwise\"\n\n---\n\n**SO BASICALLY: if DFS parameter `p` is BIG then `1/p` is SMALL so unlikely walk will return to previous node**\n\n---\n\n## Implementation\n\n**NOTE: HIS EXAMPLE NOW IS ON UNWEIGHTED GRAPH so we don't even use the W defined above, just the \"search bias\" `a` term**\n\n","metadata":{}},{"cell_type":"code","source":"import networkx as nx\nimport random\nimport numpy as np\n\nnp.random.seed(0)\nrandom.seed(0)\n\nG = nx.erdos_renyi_graph(10, # number of nodes \n                         0.3, # connection prob / probab of an edge between any 2 nodes\n                         seed=1,\n                         directed=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T13:10:49.710037Z","iopub.execute_input":"2024-05-19T13:10:49.710502Z","iopub.status.idle":"2024-05-19T13:10:50.181266Z","shell.execute_reply.started":"2024-05-19T13:10:49.710463Z","shell.execute_reply":"2024-05-19T13:10:50.180088Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"**NOTE: in above I've used `a` but this is ALPHA so in below code when you see ALPHA this is the same as `a` above**","metadata":{}},{"cell_type":"code","source":"list(G.neighbors(0)), list(G.neighbors(4)) # ok works","metadata":{"execution":{"iopub.status.busy":"2024-05-19T13:25:47.711112Z","iopub.execute_input":"2024-05-19T13:25:47.711653Z","iopub.status.idle":"2024-05-19T13:25:47.720507Z","shell.execute_reply.started":"2024-05-19T13:25:47.711616Z","shell.execute_reply":"2024-05-19T13:25:47.719253Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"([1, 4, 9], [0, 3, 5, 6, 7, 9])"},"metadata":{}}]},{"cell_type":"code","source":"# NOTE: recall that p and q are the \"DFS\" and \"BFS\" params respectively\ndef next_node(previous, current, p, q):\n    # get neighbors of current node\n    neighbors = list(G.neighbors(current))\n    \n    # calculate the appropriate alpha value for each neighbor\n    # \"1/p if neighbor is previous node, 1 if neighbor is connected to previous node, 1/q otherwise\"\n    alphas = []\n    for neighbor in neighbors:\n        if neighbor == previous:\n            alpha = 1/p\n        elif G.has_edge(neighbor, previous):\n            # neighbor is directly connected to previous node\n            alpha = 1\n        else:\n            alpha = 1/q\n        alphas.append(alpha)\n        \n    # normalize to create transition probabilities\n    probs = [alpha / sum(alphas) for alpha in alphas]\n    \n    # randomly select next node based on the transition probabilities, and return that node\n    next_node_ = np.random.choice(neighbors, size=1, p=probs)\n    \n    return next_node_ # NOTE BAD VARIABLE NAME: he calls this next but it shadows Python next. I would call the function get_next_node() or something O_o\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-19T13:26:35.231777Z","iopub.execute_input":"2024-05-19T13:26:35.232169Z","iopub.status.idle":"2024-05-19T13:26:35.240877Z","shell.execute_reply.started":"2024-05-19T13:26:35.232142Z","shell.execute_reply":"2024-05-19T13:26:35.239531Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"for test_num in range(10):\n    display(next_node(0,4, p=1,q=1)) # ok generates random choices","metadata":{"execution":{"iopub.status.busy":"2024-05-19T13:27:22.122280Z","iopub.execute_input":"2024-05-19T13:27:22.122699Z","iopub.status.idle":"2024-05-19T13:27:22.151212Z","shell.execute_reply.started":"2024-05-19T13:27:22.122667Z","shell.execute_reply":"2024-05-19T13:27:22.150135Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"array([6])"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"array([5])"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"array([9])"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"array([9])"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"array([5])"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"array([7])"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"array([6])"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"array([6])"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"array([9])"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"array([0])"},"metadata":{}}]},{"cell_type":"code","source":"xxx = str(next_node(0,4, p=1,q=1))\n\nprint(xxx)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T13:28:04.179618Z","iopub.execute_input":"2024-05-19T13:28:04.180031Z","iopub.status.idle":"2024-05-19T13:28:04.186720Z","shell.execute_reply.started":"2024-05-19T13:28:04.179999Z","shell.execute_reply":"2024-05-19T13:28:04.185561Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"[7]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Update the random_walk() function, needs to include p and q, and use our updated next_node\n\n**NOTE: CODE IN BOOK IS WRONG:**\n\nHe doesn't add `[0]` to access the content of the ndarray (of size 1 i.e. only index 0 contains the name of the next node you want)\n\nAlso for some reason he then converts the labels to str(x), but book examples show e.g. `[4,13,1,...]` the random walks as having `ints` in them. Maybe editor changed code or whatever O_o\n\nSo later you get an ndarray error","metadata":{}},{"cell_type":"code","source":"# update the random walk \ndef random_walk(start, length, p, q):\n    walk = [start]\n    \n    for _ in range(length):\n        current = walk[-1]\n        previous = walk[-2] if len(walk) > 1 else None\n        next_node_ = next_node(previous, current, p, q)[0] # I ADDED [0] HERE TO GET THE ELEMENT FROM THE ndarray\n        walk.append(next_node_)\n        \n    #return [str(x) for x in walk] # O_o this is in book, but his examples seem to want INTS\n    return walk ","metadata":{"execution":{"iopub.status.busy":"2024-05-19T13:29:36.827352Z","iopub.execute_input":"2024-05-19T13:29:36.827711Z","iopub.status.idle":"2024-05-19T13:29:36.835115Z","shell.execute_reply.started":"2024-05-19T13:29:36.827683Z","shell.execute_reply":"2024-05-19T13:29:36.833947Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# test generate some random walks now\n# ex: start at node 0, length 8\n# here p=q=1 so we are doing DeepWalk\n\nrandom_walk(0, 8, p=1, q=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T13:29:39.681200Z","iopub.execute_input":"2024-05-19T13:29:39.681568Z","iopub.status.idle":"2024-05-19T13:29:39.693159Z","shell.execute_reply.started":"2024-05-19T13:29:39.681541Z","shell.execute_reply":"2024-05-19T13:29:39.691999Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"[0, 1, 9, 1, 2, 1, 9, 1, 6]"},"metadata":{}}]},{"cell_type":"markdown","source":"Now let's bias to returing to previous node, by setting `q = 10` **RECALL, instead of the very unclear naming convention p and q, that q is BFS_COEFF and p is DFS_COEFF** so `q=10` means we are enforcing **BFS-type exploration**","metadata":{}},{"cell_type":"code","source":"random_walk(0, 8, p=1, q=10)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T13:33:44.467286Z","iopub.execute_input":"2024-05-19T13:33:44.467650Z","iopub.status.idle":"2024-05-19T13:33:44.476443Z","shell.execute_reply.started":"2024-05-19T13:33:44.467623Z","shell.execute_reply":"2024-05-19T13:33:44.475103Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"[0, 1, 9, 1, 9, 1, 9, 0, 4]"},"metadata":{}}]},{"cell_type":"markdown","source":"Same same for DFS exploration, setting p to 10 (**expect to explore more now rather than keep returning**):","metadata":{}},{"cell_type":"code","source":"random_walk(0, 8, p=10, q=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T13:34:46.010849Z","iopub.execute_input":"2024-05-19T13:34:46.011247Z","iopub.status.idle":"2024-05-19T13:34:46.020331Z","shell.execute_reply.started":"2024-05-19T13:34:46.011217Z","shell.execute_reply":"2024-05-19T13:34:46.018768Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"[0, 9, 1, 6, 5, 2, 1, 0, 4]"},"metadata":{}}]},{"cell_type":"markdown","source":"## Implementing Node2Vec\n\n- use Zach Karate Club dataset as in chapter 3\n- basically same as ch03 just change next_node() function and the p and q params","metadata":{}},{"cell_type":"code","source":"from gensim.models.word2vec import Word2Vec\n\n# for the classifier we build with the word embeddings\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2024-05-19T13:36:19.648701Z","iopub.execute_input":"2024-05-19T13:36:19.649131Z","iopub.status.idle":"2024-05-19T13:36:33.064282Z","shell.execute_reply.started":"2024-05-19T13:36:19.649100Z","shell.execute_reply":"2024-05-19T13:36:33.063150Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"G = nx.karate_club_graph()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T13:44:16.973953Z","iopub.execute_input":"2024-05-19T13:44:16.974991Z","iopub.status.idle":"2024-05-19T13:44:16.981279Z","shell.execute_reply.started":"2024-05-19T13:44:16.974952Z","shell.execute_reply":"2024-05-19T13:44:16.980191Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# transform labels to ints as before\n\nlabels = []\nfor node in G.nodes:\n    label = G.nodes[node]['club']\n    labels.append(1 if label == 'Officer' else 0)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T14:18:37.738424Z","iopub.execute_input":"2024-05-19T14:18:37.739095Z","iopub.status.idle":"2024-05-19T14:18:37.745369Z","shell.execute_reply.started":"2024-05-19T14:18:37.739039Z","shell.execute_reply":"2024-05-19T14:18:37.744382Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# generate list of random walks; 80 for each node as before\n# length 10 as before\n# NEW: here we use the p and q params (3 and 2 in this example)\n\nwalks = []\nfor node in G.nodes:\n    for _ in range(80):\n        walks.append(random_walk(node, 10, p=3, q=2)) # p=3, q=2","metadata":{"execution":{"iopub.status.busy":"2024-05-19T14:19:38.605988Z","iopub.execute_input":"2024-05-19T14:19:38.606392Z","iopub.status.idle":"2024-05-19T14:19:40.086787Z","shell.execute_reply.started":"2024-05-19T14:19:38.606364Z","shell.execute_reply":"2024-05-19T14:19:40.085389Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# create a skip-gram Word2Vec with hierarchical softmax\n\nnode2vec = Word2Vec(walks,\n                   hs=1,\n                   sg=1,\n                   vector_size=100,\n                   window=10,\n                   workers=2,\n                   min_count=1,\n                   seed=0,\n                   )","metadata":{"execution":{"iopub.status.busy":"2024-05-19T14:20:52.591320Z","iopub.execute_input":"2024-05-19T14:20:52.591707Z","iopub.status.idle":"2024-05-19T14:20:52.806847Z","shell.execute_reply.started":"2024-05-19T14:20:52.591679Z","shell.execute_reply":"2024-05-19T14:20:52.805870Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# train the node2vec on the random walks we generated\n\nnode2vec.train(walks,\n              total_examples=node2vec.corpus_count,\n              epochs=30,\n              report_delay=1,\n              )","metadata":{"execution":{"iopub.status.busy":"2024-05-19T14:21:36.922548Z","iopub.execute_input":"2024-05-19T14:21:36.922923Z","iopub.status.idle":"2024-05-19T14:21:38.022833Z","shell.execute_reply.started":"2024-05-19T14:21:36.922896Z","shell.execute_reply":"2024-05-19T14:21:38.021490Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"(185842, 897600)"},"metadata":{}}]},{"cell_type":"code","source":"# --- NOW USE THESE IN A CLASSIFICATION TASK ---\n\n# CODE IN BOOK IS WRONG, the stuff to string doesn't work - just use the ints O_o\n\ntrain_mask = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24]\n#train_mask_str = [str(x) for x in train_mask]\ntest_mask = [0, 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n#test_mask_str = [str(x) for x in test_mask]\nlabels = np.array(labels)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T14:22:07.611365Z","iopub.execute_input":"2024-05-19T14:22:07.611848Z","iopub.status.idle":"2024-05-19T14:22:07.620611Z","shell.execute_reply.started":"2024-05-19T14:22:07.611750Z","shell.execute_reply":"2024-05-19T14:22:07.619276Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"clf = RandomForestClassifier(random_state=0)\nclf.fit(node2vec.wv[train_mask], labels[train_mask])","metadata":{"execution":{"iopub.status.busy":"2024-05-19T14:22:30.686644Z","iopub.execute_input":"2024-05-19T14:22:30.687008Z","iopub.status.idle":"2024-05-19T14:22:30.931772Z","shell.execute_reply.started":"2024-05-19T14:22:30.686979Z","shell.execute_reply":"2024-05-19T14:22:30.930679Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"RandomForestClassifier(random_state=0)","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=0)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"y_pred = clf.predict(node2vec.wv[test_mask])\nacc = accuracy_score(y_pred, labels[test_mask])\nprint(f'Node2Vec accuracy = {acc*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T14:22:49.188926Z","iopub.execute_input":"2024-05-19T14:22:49.189380Z","iopub.status.idle":"2024-05-19T14:22:49.208121Z","shell.execute_reply.started":"2024-05-19T14:22:49.189347Z","shell.execute_reply":"2024-05-19T14:22:49.206813Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"Node2Vec accuracy = 95.45%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In the book he shows multiple different versions with p,q varying.\n\nAlso says to repeat each experiment e.g. 100 times to get a std_dev on the accuracy and a mean value.\n\n(Note that p=q=1 is the basic DeepWalk implementation)\n\n**he finds that high `p` leads to better performance:** which validates the \"homophily hypothesis\" - knowing the graph is a social network, we anticipated that biasing towards homophily (using DFS - NOTE THAT MANY PEOPLE ASSUME BFS IS THE \"homophily\" IMPLEMENTATION BUT IT IS INDEED DFS that captures this property)\n\n\n---\n\n# Builing a movie recommendation system\n\n- if we encoded movies instead of words, we can ask for movies that are similar to a given input\n\nBut how to get a graph dataset of movies where \"similar movies\" are connected to each other in some way??\n\n**This bit is again unclear on first reading, but will just code it and see:** one approach is to look at ratings - we will create a graph where *movies that are liked by the same users are connected*, then use this graph to learn movie embeddings.\n\n(The part that isn't clear is the yes/no edge question - you have 2 movies M, N. Each has a list of users who liked that movie. What, clearly explained please, is the condition for an edge between M and N then?? Do the 2 lists have to be exactly identical??? Then this will create just fully connected subgraphs of movies, indexed/labeled by their specific list of users that like all of them ??? Or are we saying that as long as 1 user is common to the 2 lists, then create an edge M to N. Or some intermediate number of \"Jaccard similarity\" between the 2 user lists??)\n\n---\n\nWe use MovieLens[2] dataset: 100,000+ ratings of ~10,000 movies by 600+ users","metadata":{}},{"cell_type":"code","source":"# get MovieLens[2] dataset\n\nfrom io import BytesIO\nfrom urllib.request import urlopen\nfrom zipfile import ZipFile\n\nurl = 'https://files.grouplens.org/datasets/movielens/ml-100k.zip'\n\nwith urlopen(url) as zurl:\n    with ZipFile(BytesIO(zurl.read())) as zfile:\n        zfile.extractall('.')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T14:58:08.367243Z","iopub.execute_input":"2024-05-19T14:58:08.367982Z","iopub.status.idle":"2024-05-19T14:58:09.296033Z","shell.execute_reply.started":"2024-05-19T14:58:08.367908Z","shell.execute_reply":"2024-05-19T14:58:09.294597Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"ratings.csv has all the users' ratings, movies.csv has movie_ids -> titles","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nratings = pd.read_csv('ml-100k/u.data', sep='\\t',names=['user_id', 'movie_id', 'rating', 'unix_timestamp'])\n\nratings","metadata":{"execution":{"iopub.status.busy":"2024-05-19T14:58:39.968038Z","iopub.execute_input":"2024-05-19T14:58:39.969271Z","iopub.status.idle":"2024-05-19T14:58:40.080582Z","shell.execute_reply.started":"2024-05-19T14:58:39.969224Z","shell.execute_reply":"2024-05-19T14:58:40.079158Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"       user_id  movie_id  rating  unix_timestamp\n0          196       242       3       881250949\n1          186       302       3       891717742\n2           22       377       1       878887116\n3          244        51       2       880606923\n4          166       346       1       886397596\n...        ...       ...     ...             ...\n99995      880       476       3       880175444\n99996      716       204       5       879795543\n99997      276      1090       1       874795795\n99998       13       225       2       882399156\n99999       12       203       3       879959583\n\n[100000 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>movie_id</th>\n      <th>rating</th>\n      <th>unix_timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>196</td>\n      <td>242</td>\n      <td>3</td>\n      <td>881250949</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>186</td>\n      <td>302</td>\n      <td>3</td>\n      <td>891717742</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>22</td>\n      <td>377</td>\n      <td>1</td>\n      <td>878887116</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>244</td>\n      <td>51</td>\n      <td>2</td>\n      <td>880606923</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>166</td>\n      <td>346</td>\n      <td>1</td>\n      <td>886397596</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>99995</th>\n      <td>880</td>\n      <td>476</td>\n      <td>3</td>\n      <td>880175444</td>\n    </tr>\n    <tr>\n      <th>99996</th>\n      <td>716</td>\n      <td>204</td>\n      <td>5</td>\n      <td>879795543</td>\n    </tr>\n    <tr>\n      <th>99997</th>\n      <td>276</td>\n      <td>1090</td>\n      <td>1</td>\n      <td>874795795</td>\n    </tr>\n    <tr>\n      <th>99998</th>\n      <td>13</td>\n      <td>225</td>\n      <td>2</td>\n      <td>882399156</td>\n    </tr>\n    <tr>\n      <th>99999</th>\n      <td>12</td>\n      <td>203</td>\n      <td>3</td>\n      <td>879959583</td>\n    </tr>\n  </tbody>\n</table>\n<p>100000 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"movies = pd.read_csv('ml-100k/u.item', sep='|', usecols=range(2), names=['movie_id', 'title'], encoding='latin-1')\n\nmovies","metadata":{"execution":{"iopub.status.busy":"2024-05-19T14:59:46.256859Z","iopub.execute_input":"2024-05-19T14:59:46.257304Z","iopub.status.idle":"2024-05-19T14:59:46.285040Z","shell.execute_reply.started":"2024-05-19T14:59:46.257272Z","shell.execute_reply":"2024-05-19T14:59:46.283572Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"      movie_id                                      title\n0            1                           Toy Story (1995)\n1            2                           GoldenEye (1995)\n2            3                          Four Rooms (1995)\n3            4                          Get Shorty (1995)\n4            5                             Copycat (1995)\n...        ...                                        ...\n1677      1678                          Mat' i syn (1997)\n1678      1679                           B. Monkey (1998)\n1679      1680                       Sliding Doors (1998)\n1680      1681                        You So Crazy (1994)\n1681      1682  Scream of Stone (Schrei aus Stein) (1991)\n\n[1682 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>movie_id</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Toy Story (1995)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>GoldenEye (1995)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>Four Rooms (1995)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>Get Shorty (1995)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Copycat (1995)</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1677</th>\n      <td>1678</td>\n      <td>Mat' i syn (1997)</td>\n    </tr>\n    <tr>\n      <th>1678</th>\n      <td>1679</td>\n      <td>B. Monkey (1998)</td>\n    </tr>\n    <tr>\n      <th>1679</th>\n      <td>1680</td>\n      <td>Sliding Doors (1998)</td>\n    </tr>\n    <tr>\n      <th>1680</th>\n      <td>1681</td>\n      <td>You So Crazy (1994)</td>\n    </tr>\n    <tr>\n      <th>1681</th>\n      <td>1682</td>\n      <td>Scream of Stone (Schrei aus Stein) (1991)</td>\n    </tr>\n  </tbody>\n</table>\n<p>1682 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## NOTE: READING AHEAD\n\nI had to read ahead because it was so unclear what is actually going to be measured:\n\nWe are going to:\n\n- for each user, make a list of the movies he likes\n- for each PAIR in this list, increment a global dict that COUNTS how many times a MOVIE PAIR occurs: this count is maintained globally across ALL THE USERS\n- so basically, if there are 3 people overall who like both \"movie XA\" and \"movie YF\" then the pair (XA,YF) has value 3\n\n\n**NOTE: keys are wrong in book, should be `user_id` , `movie_id` etc. maybe older version of dataset was used**","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\n\n# THIS IS THE \"movie pairs\" dict\npairs = defaultdict(int)\n\n# group the dataframe by users, to get each users LIKED MOVIES\nfor individual_user_info in ratings.groupby(\"user_id\"):\n    # individual_user_info is a 2-ple (number_here, the_info_of_dataframe_here) where number_here seems to be an index/enumerator\n    # so only need the [1] index of this 2-ple to get the groupby dataframe for this particular user\n    user_movies = list(individual_user_info[1][\"movie_id\"])\n    \n    # increment the counter for the pairs that are seen together in this particuar users' list\n    for first_movie in range(len(user_movies)):\n        for second_movie in range(first_movie+1, len(user_movies)):\n            current_movie_pair = (first_movie, second_movie)\n            pairs[current_movie_pair] += 1\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-19T15:12:12.383122Z","iopub.execute_input":"2024-05-19T15:12:12.383541Z","iopub.status.idle":"2024-05-19T15:12:18.493385Z","shell.execute_reply.started":"2024-05-19T15:12:12.383510Z","shell.execute_reply":"2024-05-19T15:12:18.491864Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"G = nx.Graph()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T15:13:13.251595Z","iopub.execute_input":"2024-05-19T15:13:13.252073Z","iopub.status.idle":"2024-05-19T15:13:13.257325Z","shell.execute_reply.started":"2024-05-19T15:13:13.252022Z","shell.execute_reply":"2024-05-19T15:13:13.256072Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"### WEIGHTED GRAPH\n\nSo finally it's explained what the edge is going to be - it is **NOT** just a yes/no edge, as I as wondering earlier\n\n**WE ARE CREATING A WEIGHTED GRAPH (first time this is mentioned in the book!!!!!!!!)**\n\nWe connect 2 movies with a weighted edge, where the weight is the co-occurence count from the above `pairs` dict\n\n**We impose a threshold of > 20 score/co-occurence count to include a weighted edge - otherwise the graph would be huge and connections would be less meaningful** (NOTE: I'm guessing this threshold is a hyperparameter you can play with later)","metadata":{}},{"cell_type":"code","source":"movie_cooccurence_threshold = 20\n\nfor pair in pairs:\n    first_movie, second_movie = pair # can just unpack directly above O_o\n    score = pairs[pair]\n    \n    if score >= movie_cooccurence_threshold:\n        G.add_edge(first_movie, second_movie, weight=score) # FIRST TIME WE USE WEIGHTED GRAPHS\n        \n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-19T15:18:18.440026Z","iopub.execute_input":"2024-05-19T15:18:18.440464Z","iopub.status.idle":"2024-05-19T15:18:18.779992Z","shell.execute_reply.started":"2024-05-19T15:18:18.440434Z","shell.execute_reply":"2024-05-19T15:18:18.779113Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"Note that since update to dataset, we have much larger graph that he does in book (he says it has 410 nodes but 14_936 edges - I find 75_078 edges O_o)\n\nMight need to increase threshold for the exercise to not time out","metadata":{}},{"cell_type":"code","source":"len(G.nodes), len(G.edges) ","metadata":{"execution":{"iopub.status.busy":"2024-05-19T15:19:58.437703Z","iopub.execute_input":"2024-05-19T15:19:58.438137Z","iopub.status.idle":"2024-05-19T15:19:58.446910Z","shell.execute_reply.started":"2024-05-19T15:19:58.438105Z","shell.execute_reply":"2024-05-19T15:19:58.445595Z"},"trusted":true},"execution_count":60,"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"(388, 75078)"},"metadata":{}}]},{"cell_type":"markdown","source":"**We could use our node2vec implementation - but let's try the Python library now:**","metadata":{}},{"cell_type":"code","source":"!pip install node2vec","metadata":{"execution":{"iopub.status.busy":"2024-05-19T15:22:30.561079Z","iopub.execute_input":"2024-05-19T15:22:30.561468Z","iopub.status.idle":"2024-05-19T15:22:51.879586Z","shell.execute_reply.started":"2024-05-19T15:22:30.561440Z","shell.execute_reply":"2024-05-19T15:22:51.877366Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"Collecting node2vec\n  Downloading node2vec-0.4.6-py3-none-any.whl.metadata (743 bytes)\nRequirement already satisfied: gensim<5.0.0,>=4.1.2 in /opt/conda/lib/python3.10/site-packages (from node2vec) (4.3.2)\nRequirement already satisfied: joblib<2.0.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from node2vec) (1.4.0)\nCollecting networkx<3.0,>=2.5 (from node2vec)\n  Downloading networkx-2.8.8-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: numpy<2.0.0,>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from node2vec) (1.26.4)\nRequirement already satisfied: tqdm<5.0.0,>=4.55.1 in /opt/conda/lib/python3.10/site-packages (from node2vec) (4.66.1)\nRequirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (1.11.4)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (6.4.0)\nDownloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\nDownloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: networkx, node2vec\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.2.1\n    Uninstalling networkx-3.2.1:\n      Successfully uninstalled networkx-3.2.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed networkx-2.8.8 node2vec-0.4.6\n","output_type":"stream"}]},{"cell_type":"code","source":"from node2vec import Node2Vec","metadata":{"execution":{"iopub.status.busy":"2024-05-19T15:23:06.851878Z","iopub.execute_input":"2024-05-19T15:23:06.852428Z","iopub.status.idle":"2024-05-19T15:23:07.025980Z","shell.execute_reply.started":"2024-05-19T15:23:06.852387Z","shell.execute_reply":"2024-05-19T15:23:07.024930Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"Create Node2Vec instance that will generate biased random walks, based on p and q params:","metadata":{}},{"cell_type":"code","source":"node2vec = Node2Vec(G,\n                    dimensions=64, # IM GUESSING THIS IS THE SAME AS vector_size IN WORD2VEC API - WHY CAN'T PEOPLE STANDARDIZE THINGS EVEN IN SIMPLE LIBRARIES!??!?!\n                    walk_length=20,\n                    num_walks=200,\n                    p=2,\n                    q=1,\n                    workers=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T15:24:32.945864Z","iopub.execute_input":"2024-05-19T15:24:32.946310Z","iopub.status.idle":"2024-05-19T15:32:17.019740Z","shell.execute_reply.started":"2024-05-19T15:24:32.946278Z","shell.execute_reply":"2024-05-19T15:32:17.018509Z"},"trusted":true},"execution_count":63,"outputs":[{"output_type":"display_data","data":{"text/plain":"Computing transition probabilities:   0%|          | 0/388 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85c84e49007a47489b322db36dd130bb"}},"metadata":{}},{"name":"stderr","text":"Generating walks (CPU: 1): 100%|██████████| 200/200 [02:32<00:00,  1.31it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Train a model on these biased random walks\n\n**Window of 10 means 5 nodes before and 5 nodes after**","metadata":{}},{"cell_type":"code","source":"model = node2vec.fit(window=10,\n                     min_count=1,\n                     batch_words=4)","metadata":{"execution":{"iopub.status.busy":"2024-05-19T15:43:27.619789Z","iopub.execute_input":"2024-05-19T15:43:27.620675Z","iopub.status.idle":"2024-05-19T15:44:54.709450Z","shell.execute_reply.started":"2024-05-19T15:43:27.620615Z","shell.execute_reply":"2024-05-19T15:44:54.708344Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"Now model is trained so use it same way as previous objects we built ourselves.\n\nWe create a function to recommend similar movies based on an input title:\n\n`recommend()` starts by converting movie title to a movie ID we can use to query the model","metadata":{}},{"cell_type":"code","source":"movie = \"GoldenEye (1995)\"\n\nstr(movies[movies.title == movie].movie_id.values[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:08:20.704518Z","iopub.execute_input":"2024-05-19T16:08:20.704956Z","iopub.status.idle":"2024-05-19T16:08:20.715143Z","shell.execute_reply.started":"2024-05-19T16:08:20.704923Z","shell.execute_reply":"2024-05-19T16:08:20.713767Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"'2'"},"metadata":{}}]},{"cell_type":"code","source":"def recommend(movie):\n    movie_id = str(movies[movies.title == movie].movie_id.values[0])\n    \n    # loop through 5 most similar word (i.e. NODE) vectors\n    # convert these back into movie titles and output them:\n    # === ALSO OUTPUT THEIR SIMILARITY SCORE ===\n    for recommended_movie_id in model.wv.most_similar(movie_id)[:5]:\n        #print(type(recommended_movie_id[0]), recommended_movie_id)\n        \n        id2title = movies[movies.movie_id == int(recommended_movie_id[0])].title.values[0]\n\n        print(f\"{id2title} : {recommended_movie_id[1]}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:18:16.746475Z","iopub.execute_input":"2024-05-19T16:18:16.746870Z","iopub.status.idle":"2024-05-19T16:18:16.754164Z","shell.execute_reply.started":"2024-05-19T16:18:16.746840Z","shell.execute_reply":"2024-05-19T16:18:16.753235Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"recommend('Star Wars (1977)') # NOTE I DON'T GET SAME RESULTS AS HIM \n\n# he gets Return of the Jedi and Raiders of the lost ark which seems more plausible","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:18:25.676940Z","iopub.execute_input":"2024-05-19T16:18:25.677648Z","iopub.status.idle":"2024-05-19T16:18:25.691336Z","shell.execute_reply.started":"2024-05-19T16:18:25.677615Z","shell.execute_reply":"2024-05-19T16:18:25.689853Z"},"trusted":true},"execution_count":96,"outputs":[{"name":"stdout","text":"Mad Love (1995) : 0.7250682711601257\nMadness of King George, The (1994) : 0.7220710515975952\nToy Story (1995) : 0.7145920991897583\nBelle de jour (1967) : 0.706836462020874\nOutbreak (1995) : 0.6971874237060547\n","output_type":"stream"}]},{"cell_type":"code","source":"recommend('GoldenEye (1995)')","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:18:19.655515Z","iopub.execute_input":"2024-05-19T16:18:19.656473Z","iopub.status.idle":"2024-05-19T16:18:19.674037Z","shell.execute_reply.started":"2024-05-19T16:18:19.656439Z","shell.execute_reply":"2024-05-19T16:18:19.672621Z"},"trusted":true},"execution_count":95,"outputs":[{"name":"stdout","text":"Angels and Insects (1995) : 0.6641371250152588\nDead Man Walking (1995) : 0.6608666181564331\nMuppet Treasure Island (1996) : 0.6513866782188416\nUsual Suspects, The (1995) : 0.6510480642318726\nDolores Claiborne (1994) : 0.6439917683601379\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# End of chapter outlook\n\nWe are now going to address one overlooked issue in DeepWalk and Node2Vec - nodes don't have proper features yet\n\nWe will try to address this (? not clear yet what this means) with traditional neural networks, which cannot understand network topology though. This will lead us to study, finally, Graph Neural Networks","metadata":{}}]}