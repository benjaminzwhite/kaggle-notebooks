{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# QLoRA - How to Fine-tune an LLM on a Single GPU\n\n[https://www.youtube.com/watch?v=XpoKB3usmKc](https://www.youtube.com/watch?v=XpoKB3usmKc)\n\n---\n\nThe dataset here is from youtube comments - the idea is to finetune to respond to youtube comments","metadata":{}},{"cell_type":"code","source":"!pip install auto-gptq\n!pip install optimum\n!pip install bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:30:53.780161Z","iopub.execute_input":"2024-05-20T15:30:53.780442Z","iopub.status.idle":"2024-05-20T15:31:32.795918Z","shell.execute_reply.started":"2024-05-20T15:30:53.780415Z","shell.execute_reply":"2024-05-20T15:31:32.794729Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: auto-gptq in /opt/conda/lib/python3.10/site-packages (0.7.1)\nRequirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.29.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (2.18.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (1.26.4)\nRequirement already satisfied: rouge in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (1.0.1)\nRequirement already satisfied: gekko in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (1.1.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (2.1.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.4.3)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (4.39.3)\nRequirement already satisfied: peft>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (0.11.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from auto-gptq) (4.66.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.26.0->auto-gptq) (0.22.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->auto-gptq) (2024.2.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->auto-gptq) (0.15.2)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->auto-gptq) (3.9.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge->auto-gptq) (1.16.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate>=0.26.0->auto-gptq) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->auto-gptq) (2023.4)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\nRequirement already satisfied: optimum in /opt/conda/lib/python3.10/site-packages (1.19.2)\nRequirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from optimum) (15.0.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum) (1.12)\nRequirement already satisfied: transformers<4.41.0,>=4.26.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (4.39.3)\nRequirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from optimum) (2.1.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from optimum) (21.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from optimum) (1.26.4)\nRequirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from optimum) (0.22.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum) (2.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.2.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->optimum) (3.1.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.41.0,>=4.26.0->transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<4.41.0,>=4.26.0->transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.41.0,>=4.26.0->transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (0.4.3)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (0.2.0)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.41.0,>=4.26.0->optimum) (3.20.3)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.9.1)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.43.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import transformers\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\nfrom datasets import load_dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:31:48.322348Z","iopub.execute_input":"2024-05-20T15:31:48.323143Z","iopub.status.idle":"2024-05-20T15:32:08.441939Z","shell.execute_reply.started":"2024-05-20T15:31:48.323104Z","shell.execute_reply":"2024-05-20T15:32:08.441107Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-05-20 15:31:55.938632: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-20 15:31:55.938752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-20 15:31:56.074245: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:32:51.782027Z","iopub.execute_input":"2024-05-20T15:32:51.783305Z","iopub.status.idle":"2024-05-20T15:32:51.811908Z","shell.execute_reply.started":"2024-05-20T15:32:51.783269Z","shell.execute_reply":"2024-05-20T15:32:51.811032Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32a22c5acb7f488c9e3e60fa4338fddd"}},"metadata":{}}]},{"cell_type":"code","source":"# load quantized model\nmodel_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                             device_map=\"auto\",\n                                             trust_remote_code=False,\n                                             revision=\"main\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:33:09.061145Z","iopub.execute_input":"2024-05-20T15:33:09.061517Z","iopub.status.idle":"2024-05-20T15:33:32.812393Z","shell.execute_reply.started":"2024-05-20T15:33:09.061488Z","shell.execute_reply":"2024-05-20T15:33:32.811521Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"357216c1dafb4288b148ff1b89603458"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.16G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e176c55d45b8495fb5a15aa3781229cc"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f247f553c0e340dcb02414eb283f8a28"}},"metadata":{}}]},{"cell_type":"code","source":"# load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:33:59.532609Z","iopub.execute_input":"2024-05-20T15:33:59.533388Z","iopub.status.idle":"2024-05-20T15:34:00.486832Z","shell.execute_reply.started":"2024-05-20T15:33:59.533352Z","shell.execute_reply":"2024-05-20T15:34:00.485800Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb263fa3525b407cbe38fa68fc2482dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19b8af0837344e6895ec05efc36580cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02fc586e4c874b25bf9eb477534c929b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a7f451a6c2242b58f0d7e6870d1b543"}},"metadata":{}}]},{"cell_type":"code","source":"# baseline performance of model when responding to youtube comments\n\nmodel.eval()\n\n# Example prompt i.e. youtube comment\ncomment = \"Great content, thanks a lot it was good\"\nprompt = f'''[INST] {comment} [/INST]'''\n\n# tokenize prompt\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\n# generate output\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=140)\n\nprint(tokenizer.batch_decode(outputs)[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:36:23.769150Z","iopub.execute_input":"2024-05-20T15:36:23.770198Z","iopub.status.idle":"2024-05-20T15:37:09.585054Z","shell.execute_reply.started":"2024-05-20T15:36:23.770163Z","shell.execute_reply":"2024-05-20T15:37:09.583952Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<s> [INST] Great content, thanks a lot it was good [/INST] I'm glad you found the content helpful! If you have any specific questions or topics you'd like me to cover in the future, feel free to ask. I'm here to help. Have a great day!</s>\n","output_type":"stream"}]},{"cell_type":"code","source":"# prompt engineering part\n\nintstructions_string = f\"\"\"ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\nIt reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\nShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\nthus keeping the interaction natural and engaging.\n\nPlease respond to the following comment.\n\"\"\"\n\nprompt_template = lambda comment: f'''[INST] {intstructions_string} \\n{comment} \\n[/INST]'''\n\nprompt = prompt_template(comment)\nprint(prompt)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:43:50.038077Z","iopub.execute_input":"2024-05-20T15:43:50.039161Z","iopub.status.idle":"2024-05-20T15:43:50.046878Z","shell.execute_reply.started":"2024-05-20T15:43:50.039117Z","shell.execute_reply":"2024-05-20T15:43:50.045762Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n\nPlease respond to the following comment.\n \nGreat content, thanks a lot it was good \n[/INST]\n","output_type":"stream"}]},{"cell_type":"code","source":"# try interrogate nonfinetuned model but with prompt now\n\n# tokenize new prompt\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\n# generate output\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=140)\n\nprint(tokenizer.batch_decode(outputs)[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:44:24.259635Z","iopub.execute_input":"2024-05-20T15:44:24.260010Z","iopub.status.idle":"2024-05-20T15:45:15.754224Z","shell.execute_reply.started":"2024-05-20T15:44:24.259983Z","shell.execute_reply":"2024-05-20T15:45:15.753251Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<s> [INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n\nPlease respond to the following comment.\n \nGreat content, thanks a lot it was good \n[/INST] Thank you for your kind words! I'm glad you found the content helpful. If you have any specific questions or topics you'd like me to cover in more detail, just let me know and I'll be happy to help –ShawGPT.</s>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Model finetuning with QLoRA","metadata":{}},{"cell_type":"code","source":"model.train() # model in training mode (dropout modules are activated)\n\n# enable gradient check pointing - this is for memory optimisation TODO: read more\nmodel.gradient_checkpointing_enable()\n\n# enable quantized training\nmodel = prepare_model_for_kbit_training(model)\n\n# CARE! the base model is in 4 bit, but we want to do LoRA in higher precision - this is what the prepare_model_.. is doing","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:46:55.581366Z","iopub.execute_input":"2024-05-20T15:46:55.582127Z","iopub.status.idle":"2024-05-20T15:46:55.596276Z","shell.execute_reply.started":"2024-05-20T15:46:55.582094Z","shell.execute_reply":"2024-05-20T15:46:55.595403Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Set up LoRA config","metadata":{}},{"cell_type":"code","source":"# LoRA config\nconfig = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=[\"q_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\n# LoRA trainable version of model\nmodel = get_peft_model(model, config)\n\n# trainable parameter count\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:48:04.124658Z","iopub.execute_input":"2024-05-20T15:48:04.125066Z","iopub.status.idle":"2024-05-20T15:48:04.226124Z","shell.execute_reply.started":"2024-05-20T15:48:04.125035Z","shell.execute_reply":"2024-05-20T15:48:04.225193Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"trainable params: 2,097,152 || all params: 264,507,392 || trainable%: 0.7929\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Dataset\n\nIt's a series of real comments + replies from his channel (small dataset of 50 examples, formatted with the previous prompt engineering also","metadata":{}},{"cell_type":"code","source":"# load dataset\ndata = load_dataset(\"shawhin/shawgpt-youtube-comments\")","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:50:47.745872Z","iopub.execute_input":"2024-05-20T15:50:47.746290Z","iopub.status.idle":"2024-05-20T15:50:52.305850Z","shell.execute_reply.started":"2024-05-20T15:50:47.746259Z","shell.execute_reply":"2024-05-20T15:50:52.305012Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/531 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05c264d379fa44d79e7345d2fed9a04b"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 18.0k/18.0k [00:00<00:00, 69.8kB/s]\nDownloading data: 100%|██████████| 8.09k/8.09k [00:00<00:00, 36.3kB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3be67cd1672c47b3a23aa1898f4708e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/9 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f1001a1a6924f99a5089952c620a46a"}},"metadata":{}}]},{"cell_type":"code","source":"# create tokenize function\ndef tokenize_function(examples):\n    # extract text\n    text = examples[\"example\"]\n\n    #tokenize and truncate text\n    tokenizer.truncation_side = \"left\"\n    tokenized_inputs = tokenizer(\n        text,\n        return_tensors=\"np\",\n        truncation=True,\n        max_length=512\n    )\n\n    return tokenized_inputs\n\n# tokenize training and validation datasets\ntokenized_data = data.map(tokenize_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:50:56.936851Z","iopub.execute_input":"2024-05-20T15:50:56.937696Z","iopub.status.idle":"2024-05-20T15:50:57.080550Z","shell.execute_reply.started":"2024-05-20T15:50:56.937662Z","shell.execute_reply":"2024-05-20T15:50:57.079713Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4b563b1113248a8bbddee9ed4c77de1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a34b0a1f5ed8402ea21da4e5d09196af"}},"metadata":{}}]},{"cell_type":"code","source":"# setting pad token\ntokenizer.pad_token = tokenizer.eos_token\n# data collator\ndata_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:51:05.145095Z","iopub.execute_input":"2024-05-20T15:51:05.145491Z","iopub.status.idle":"2024-05-20T15:51:05.150317Z","shell.execute_reply.started":"2024-05-20T15:51:05.145450Z","shell.execute_reply":"2024-05-20T15:51:05.149339Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Finetuning\n\n### Hyperparameters","metadata":{}},{"cell_type":"code","source":"# hyperparameters\nlr = 2e-4\nbatch_size = 4\nnum_epochs = 10\n\n# define training arguments\ntraining_args = transformers.TrainingArguments(\n    output_dir= \"shawgpt-ft\",\n    learning_rate=lr,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_epochs,\n    weight_decay=0.01,\n    logging_strategy=\"epoch\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    gradient_accumulation_steps=4,\n    warmup_steps=2,\n    fp16=True, # <---------------- NOTE IMPORTANT !! TODO: BE 100% CLEAR ON THIS - WE USE 16bit FOR TRAINING (but only the LoRA adapters are being trained AFAICT)\n    optim=\"paged_adamw_8bit\", # <=== THIS IS THE \"PAGED OPTIMIZER\" IDEA which involves moving examples around from GPU to CPU\n\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:53:17.781623Z","iopub.execute_input":"2024-05-20T15:53:17.782565Z","iopub.status.idle":"2024-05-20T15:53:17.809737Z","shell.execute_reply.started":"2024-05-20T15:53:17.782527Z","shell.execute_reply":"2024-05-20T15:53:17.809004Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# configure trainer\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=tokenized_data[\"train\"],\n    eval_dataset=tokenized_data[\"test\"],\n    args=training_args,\n    data_collator=data_collator\n)\n\n\n# train model\nmodel.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()\n\n# renable warnings\nmodel.config.use_cache = True","metadata":{"execution":{"iopub.status.busy":"2024-05-20T15:55:05.699911Z","iopub.execute_input":"2024-05-20T15:55:05.700599Z","iopub.status.idle":"2024-05-20T16:16:37.689210Z","shell.execute_reply.started":"2024-05-20T15:55:05.700569Z","shell.execute_reply":"2024-05-20T16:16:37.688048Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240520_155801-yba2xh9j</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/benjaminzwhite/huggingface/runs/yba2xh9j' target=\"_blank\">olive-valley-1</a></strong> to <a href='https://wandb.ai/benjaminzwhite/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/benjaminzwhite/huggingface' target=\"_blank\">https://wandb.ai/benjaminzwhite/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/benjaminzwhite/huggingface/runs/yba2xh9j' target=\"_blank\">https://wandb.ai/benjaminzwhite/huggingface/runs/yba2xh9j</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [30/30 17:43, Epoch 9/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>4.591300</td>\n      <td>3.958954</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>4.035700</td>\n      <td>3.427283</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>3.456400</td>\n      <td>2.973857</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.646900</td>\n      <td>2.283207</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.316100</td>\n      <td>2.077054</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.047100</td>\n      <td>1.890489</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.786200</td>\n      <td>1.714866</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.247100</td>\n      <td>1.710340</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# push to hub -- make clear / confirm that you are only saving the PEFT config ???\n\n\nhf_name = 'benjaminzwhite'\nmodel_id = hf_name + \"/\" + \"shawgpt-ft\"\n\nmodel.push_to_hub(model_id)\ntrainer.push_to_hub(model_id)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:21:00.439355Z","iopub.execute_input":"2024-05-20T16:21:00.440097Z","iopub.status.idle":"2024-05-20T16:21:02.518537Z","shell.execute_reply.started":"2024-05-20T16:21:00.440063Z","shell.execute_reply":"2024-05-20T16:21:02.517468Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.95k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"187ecf126e414388807353cbfc99113b"}},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/benjaminzwhite/shawgpt-ft/commit/caa5dd06ad57da04f4b4d2a138c158c884cd0d84', commit_message='benjaminzwhite/shawgpt-ft', commit_description='', oid='caa5dd06ad57da04f4b4d2a138c158c884cd0d84', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"# practice loading the model from hub \n# still not 100% clear what is being saved - just the PEFT config AFAICT??\n\n\nfrom peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM\n\nmodel_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name,\n                                             device_map=\"auto\",\n                                             trust_remote_code=False,\n                                             revision=\"main\")\n\nconfig = PeftConfig.from_pretrained(\"benjaminzwhite/shawgpt-ft\")\nmodel = PeftModel.from_pretrained(model, \"benjaminzwhite/shawgpt-ft\")\n\n# load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:24:00.296803Z","iopub.execute_input":"2024-05-20T16:24:00.297526Z","iopub.status.idle":"2024-05-20T16:24:07.743103Z","shell.execute_reply.started":"2024-05-20T16:24:00.297492Z","shell.execute_reply":"2024-05-20T16:24:07.741855Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/649 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4848a5ebff14786b82e30191574e2f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/8.40M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16e18cc8aca8470a99345174e0d667ea"}},"metadata":{}}]},{"cell_type":"code","source":"# testing the finetuned model\n\ninstructions_string = f\"\"\"ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. \\\nIt reacts to feedback aptly and ends responses with its signature '–ShawGPT'. \\\nShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, \\\nthus keeping the interaction natural and engaging.\n\nPlease respond to the following comment.\n\"\"\"\nprompt_template = lambda comment: f'''[INST] {instructions_string} \\n{comment} \\n[/INST]'''\n\ncomment = \"Nice video but can you please do a video about LLMs ? Thanks\"\n\nprompt = prompt_template(comment)","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:27:27.727822Z","iopub.execute_input":"2024-05-20T16:27:27.728705Z","iopub.status.idle":"2024-05-20T16:27:27.735536Z","shell.execute_reply.started":"2024-05-20T16:27:27.728650Z","shell.execute_reply":"2024-05-20T16:27:27.734404Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"prompt","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:27:34.719804Z","iopub.execute_input":"2024-05-20T16:27:34.720189Z","iopub.status.idle":"2024-05-20T16:27:34.728293Z","shell.execute_reply.started":"2024-05-20T16:27:34.720158Z","shell.execute_reply":"2024-05-20T16:27:34.727185Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"\"[INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\\n\\nPlease respond to the following comment.\\n \\nNice video but can you please do a video about LLMs ? Thanks \\n[/INST]\""},"metadata":{}}]},{"cell_type":"code","source":"model.eval()\n\ninputs = tokenizer(prompt, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n\nprint(tokenizer.batch_decode(outputs)[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-20T16:27:37.042185Z","iopub.execute_input":"2024-05-20T16:27:37.042868Z","iopub.status.idle":"2024-05-20T16:28:34.296141Z","shell.execute_reply.started":"2024-05-20T16:27:37.042839Z","shell.execute_reply":"2024-05-20T16:28:34.295218Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"<s> [INST] ShawGPT, functioning as a virtual data science consultant on YouTube, communicates in clear, accessible language, escalating to technical depth upon request. It reacts to feedback aptly and ends responses with its signature '–ShawGPT'. ShawGPT will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback, thus keeping the interaction natural and engaging.\n\nPlease respond to the following comment.\n \nNice video but can you please do a video about LLMs ? Thanks \n[/INST]\n\nThanks for the feedback!\n\nI'd be happy to do a video on LLMs (Large Language Models) in the future. I'll make sure to cover the differences between them and the models I've discussed in this video.\n\n–ShawGPT</s>\n","output_type":"stream"}]}]}