{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HuggingFace NLP Course Chapter 5\n\n**NOTE/COMMENT AFTER FINISHING -- skipped first part in the end since get to a stage with Github API that times out requests/reaches rate limit so I just use his dataset instead. ALSO THE FAISS install caused problems, had to restart Kaggle and run the pip install FAISS stuff as first commands not sure why**\n\n---\n\nNotes from \n\n[https://huggingface.co/learn/nlp-course/chapter5/4?fw=pt](https://huggingface.co/learn/nlp-course/chapter5/4?fw=pt)\n\non working with Datasets library etc\n\n\n---\n\n- read the first sections, mainly on how to use batched=True, stream dataset etc\n\n## Creating your own dataset\n\n- We are going to create a dataset of issues from Github, then in 2nd part create a **semantic search engine** to find which issues match a user's query\n\n## Getting the data\n\n**the repo we are targetting is the Datasets HF issue page**\n\nTo download all the repository’s issues, we’ll use the GitHub REST API to poll the Issues endpoint. This endpoint returns a list of JSON objects, with each object containing a large number of fields that include the title and description as well as metadata about the status of the issue and so on.\n\nA convenient way to download the issues is via the requests library, which is the standard way for making HTTP requests in Python.","metadata":{}},{"cell_type":"code","source":"!pip install requests","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once the library is installed, you can make GET requests to the Issues endpoint by invoking the requests.get() function. For example, you can run the following command to retrieve the first issue on the first page:","metadata":{}},{"cell_type":"code","source":"import requests\n\nurl = \"https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1\"\nresponse = requests.get(url)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The response object contains a lot of useful information about the request, including the HTTP status code:","metadata":{}},{"cell_type":"code","source":"response.status_code # 200 if succesful","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"What we are really interested in, though, is the payload, which can be accessed in various formats like bytes, strings, or JSON. Since we know our issues are in JSON format, let’s inspect the payload as follows:","metadata":{}},{"cell_type":"code","source":"response.json()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As described in the GitHub documentation, unauthenticated requests are limited to 60 requests per hour. Although you can increase the per_page query parameter to reduce the number of requests you make, you will still hit the rate limit on any repository that has more than a few thousand issues. So instead, you should follow GitHub’s instructions on creating a personal access token so that you can boost the rate limit to 5,000 requests per hour. Once you have your token, you can include it as part of the request header:","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nGITHUB_TOKEN = user_secrets.get_secret(\"GH_API_KEY\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that we have our access token, let’s create a function that can download all the issues from a GitHub repository:","metadata":{}},{"cell_type":"code","source":"import time\nimport math\nfrom pathlib import Path\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n\ndef fetch_issues(\n    owner=\"huggingface\",\n    repo=\"datasets\",\n    num_issues=10_000,\n    rate_limit=5_000,\n    issues_path=Path(\".\"),\n):\n    if not issues_path.is_dir():\n        issues_path.mkdir(exist_ok=True)\n\n    batch = []\n    all_issues = []\n    per_page = 100  # Number of issues to return per page\n    num_pages = math.ceil(num_issues / per_page)\n    base_url = \"https://api.github.com/repos\"\n\n    for page in tqdm(range(num_pages)):\n        # Query with state=all to get both open and closed issues\n        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n        batch.extend(issues.json())\n\n        if len(batch) > rate_limit and len(all_issues) < num_issues:\n            all_issues.extend(batch)\n            batch = []  # Flush batch for next time period\n            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n            time.sleep(60 * 60 + 1)\n\n    all_issues.extend(batch)\n    df = pd.DataFrame.from_records(all_issues)\n    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n    print(\n        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fetch_issues()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# STUPID - it times out the API requests so would take multiple hours to get\n\nI just read the tutorial, will copy his final uploaded version (doesn't add anything to do it yourself really, just basic processing steps)","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nremote_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\n\nremote_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"He also says to (practice) :  For bonus points, fine-tune a multilabel classifier to predict the tags present in the labels field.\n\nbut the labels field (see below) is incomprehensible and it also seems that there is at most 1 label per entry, so not a great tutorial - I will do this separately :\n\n[https://huggingface.co/blog/Valerii-Knowledgator/multi-label-classification](https://huggingface.co/blog/Valerii-Knowledgator/multi-label-classification)","metadata":{}},{"cell_type":"code","source":"remote_dataset[:10][\"labels\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Section 5 contd - Semantic search with FAISS\n\nOk so will use his dataset (due to API requests timeout for creating \"my\" own version - it's the same data) from HF\n\n---\n\nIn section 5, we created a dataset of GitHub issues and comments from the 🤗 Datasets repository. In this section we’ll use this information to build a search engine that can help us find answers to our most pressing questions about the library!\n\n---\n\nAs we saw in Chapter 1, Transformer-based language models represent each token in a span of text as an embedding vector. It turns out that one can “pool” the individual embeddings to create a vector representation for whole sentences, paragraphs, or (in some cases) documents. These embeddings can then be used to find similar documents in the corpus by computing the dot-product similarity (or some other similarity metric) between each embedding and returning the documents with the greatest overlap.\n\nIn this section we’ll use embeddings to develop a semantic search engine. These search engines offer several advantages over conventional approaches that are based on matching keywords in a query with the documents.","metadata":{}},{"cell_type":"code","source":"!pip install datasets transformers[sentencepiece] -qq\n!pip install faiss-gpu -qq","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:26:49.467276Z","iopub.execute_input":"2024-05-05T16:26:49.467634Z","iopub.status.idle":"2024-05-05T16:27:14.006483Z","shell.execute_reply.started":"2024-05-05T16:26:49.467607Z","shell.execute_reply":"2024-05-05T16:27:14.005143Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\nissues_dataset = load_dataset(\"lewtun/github-issues\", split=\"train\")\nissues_dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:27:24.065293Z","iopub.execute_input":"2024-05-05T16:27:24.066193Z","iopub.status.idle":"2024-05-05T16:27:27.124717Z","shell.execute_reply.started":"2024-05-05T16:27:24.066125Z","shell.execute_reply":"2024-05-05T16:27:27.123765Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Repo card metadata block was not found. Setting CardData to empty.\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n    num_rows: 3019\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Here we’ve specified the default train split in load_dataset(), so it returns a Dataset instead of a DatasetDict. The first order of business is to filter out the pull requests, as these tend to be rarely used for answering user queries and will introduce noise in our search engine. As should be familiar by now, we can use the Dataset.filter() function to exclude these rows in our dataset. While we’re at it, let’s also filter out rows with no comments, since these provide no answers to user queries:","metadata":{}},{"cell_type":"code","source":"issues_dataset = issues_dataset.filter(\n    lambda x: (x[\"is_pull_request\"] == False and len(x[\"comments\"]) > 0)\n)\nissues_dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:27:29.409761Z","iopub.execute_input":"2024-05-05T16:27:29.410716Z","iopub.status.idle":"2024-05-05T16:27:29.429078Z","shell.execute_reply.started":"2024-05-05T16:27:29.410682Z","shell.execute_reply":"2024-05-05T16:27:29.428065Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'pull_request', 'body', 'timeline_url', 'performed_via_github_app', 'is_pull_request'],\n    num_rows: 808\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"We can see that there are a lot of columns in our dataset, most of which we don’t need to build our search engine. From a search perspective, the most informative columns are title, body, and comments, while html_url provides us with a link back to the source issue. Let’s use the Dataset.remove_columns() function to drop the rest:","metadata":{}},{"cell_type":"code","source":"columns = issues_dataset.column_names\ncolumns_to_keep = [\"title\", \"body\", \"html_url\", \"comments\"]\ncolumns_to_remove = set(columns_to_keep).symmetric_difference(columns)\nissues_dataset = issues_dataset.remove_columns(columns_to_remove)\nissues_dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:27:31.611228Z","iopub.execute_input":"2024-05-05T16:27:31.611607Z","iopub.status.idle":"2024-05-05T16:27:31.627853Z","shell.execute_reply.started":"2024-05-05T16:27:31.611580Z","shell.execute_reply":"2024-05-05T16:27:31.626875Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['html_url', 'title', 'comments', 'body'],\n    num_rows: 808\n})"},"metadata":{}}]},{"cell_type":"code","source":"issues_dataset[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**had to reread this several times to understand wtf he means: basically just that for each current item in the dataset the body/title is the same but the COMMENTS are a list of distinct comments. We want to make a larger dataset such that each of the COMMENTS appears separately, with its original body/title copied for it** \n\nSo expand in pandas to create len(dataset_entry_comments) copies of each dataset_entry\n\n---\n\nTo create our embeddings we’ll augment each comment with the issue’s title and body, since these fields often include useful contextual information. Because our comments column is currently a list of comments for each issue, we need to “explode” the column so that each row consists of an (html_url, title, body, comment) tuple. In Pandas we can do this with the DataFrame.explode() function, which creates a new row for each element in a list-like column, while replicating all the other column values. To see this in action, let’s first switch to the Pandas DataFrame format:","metadata":{}},{"cell_type":"code","source":"issues_dataset.set_format(\"pandas\")\ndf = issues_dataset[:]","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:27:35.670484Z","iopub.execute_input":"2024-05-05T16:27:35.670850Z","iopub.status.idle":"2024-05-05T16:27:35.691937Z","shell.execute_reply.started":"2024-05-05T16:27:35.670820Z","shell.execute_reply":"2024-05-05T16:27:35.691121Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(len(df[\"comments\"][0].tolist()))\ndf[\"comments\"][0].tolist() # THERE ARE 2 COMMENTS ASSOCIATED WITH THIS ISSUE","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:27:38.052391Z","iopub.execute_input":"2024-05-05T16:27:38.053133Z","iopub.status.idle":"2024-05-05T16:27:38.060181Z","shell.execute_reply.started":"2024-05-05T16:27:38.053100Z","shell.execute_reply":"2024-05-05T16:27:38.059236Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"2\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['Cool, I think we can do both :)',\n '@lhoestq now the 2 are implemented.\\r\\n\\r\\nPlease note that for the the second protection, finally I have chosen to protect the master branch only from **merge commits** (see update comment above), so no need to disable/re-enable the protection on each release (direct commits, different from merge commits, can be pushed to the remote master branch; and eventually reverted without messing up the repo history).']"},"metadata":{}}]},{"cell_type":"markdown","source":"When we explode df, we expect to get one row for each of these comments. Let’s check if that’s the case:","metadata":{}},{"cell_type":"code","source":"comments_df = df.explode(\"comments\", ignore_index=True)\ncomments_df.head() # CHECK FIRST 2 SHOULD BE SAME TITLE/BODY/HTML_URL ","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:27:39.998025Z","iopub.execute_input":"2024-05-05T16:27:39.998918Z","iopub.status.idle":"2024-05-05T16:27:40.017090Z","shell.execute_reply.started":"2024-05-05T16:27:39.998883Z","shell.execute_reply":"2024-05-05T16:27:40.016120Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                            html_url  \\\n0  https://github.com/huggingface/datasets/issues...   \n1  https://github.com/huggingface/datasets/issues...   \n2  https://github.com/huggingface/datasets/issues...   \n3  https://github.com/huggingface/datasets/issues...   \n4  https://github.com/huggingface/datasets/issues...   \n\n                                               title  \\\n0                              Protect master branch   \n1                              Protect master branch   \n2  Backwards compatibility broken for cached data...   \n3  Backwards compatibility broken for cached data...   \n4  Backwards compatibility broken for cached data...   \n\n                                            comments  \\\n0                    Cool, I think we can do both :)   \n1  @lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...   \n2  Hi ! I guess the caching mechanism should have...   \n3  If it's easy enough to implement, then yes ple...   \n4  Well it can cause issue with anyone that updat...   \n\n                                                body  \n0  After accidental merge commit (91c55355b634d0d...  \n1  After accidental merge commit (91c55355b634d0d...  \n2  ## Describe the bug\\r\\nAfter upgrading to data...  \n3  ## Describe the bug\\r\\nAfter upgrading to data...  \n4  ## Describe the bug\\r\\nAfter upgrading to data...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>html_url</th>\n      <th>title</th>\n      <th>comments</th>\n      <th>body</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://github.com/huggingface/datasets/issues...</td>\n      <td>Protect master branch</td>\n      <td>Cool, I think we can do both :)</td>\n      <td>After accidental merge commit (91c55355b634d0d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://github.com/huggingface/datasets/issues...</td>\n      <td>Protect master branch</td>\n      <td>@lhoestq now the 2 are implemented.\\r\\n\\r\\nPle...</td>\n      <td>After accidental merge commit (91c55355b634d0d...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://github.com/huggingface/datasets/issues...</td>\n      <td>Backwards compatibility broken for cached data...</td>\n      <td>Hi ! I guess the caching mechanism should have...</td>\n      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://github.com/huggingface/datasets/issues...</td>\n      <td>Backwards compatibility broken for cached data...</td>\n      <td>If it's easy enough to implement, then yes ple...</td>\n      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://github.com/huggingface/datasets/issues...</td>\n      <td>Backwards compatibility broken for cached data...</td>\n      <td>Well it can cause issue with anyone that updat...</td>\n      <td>## Describe the bug\\r\\nAfter upgrading to data...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Great, we can see the rows have been replicated, with the comments column containing the individual comments! Now that we’re finished with Pandas, we can quickly switch back to a Dataset by loading the DataFrame in memory:","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\n\ncomments_dataset = Dataset.from_pandas(comments_df)\ncomments_dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:27:51.628538Z","iopub.execute_input":"2024-05-05T16:27:51.629431Z","iopub.status.idle":"2024-05-05T16:27:51.696370Z","shell.execute_reply.started":"2024-05-05T16:27:51.629397Z","shell.execute_reply":"2024-05-05T16:27:51.695482Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['html_url', 'title', 'comments', 'body'],\n    num_rows: 2964\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Now that we have one comment per row, let’s create a new comments_length column that contains the number of words per comment:","metadata":{}},{"cell_type":"code","source":"comments_dataset = comments_dataset.map(\n    lambda x: {\"comment_length\": len(x[\"comments\"].split())}\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:27:53.639140Z","iopub.execute_input":"2024-05-05T16:27:53.639904Z","iopub.status.idle":"2024-05-05T16:27:53.925049Z","shell.execute_reply.started":"2024-05-05T16:27:53.639872Z","shell.execute_reply":"2024-05-05T16:27:53.924013Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2964 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e1292c57c734553a8d063332bd0e78f"}},"metadata":{}}]},{"cell_type":"markdown","source":"We can use this new column to filter out short comments, which typically include things like “cc @lewtun” or “Thanks!” that are not relevant for our search engine. There’s no precise number to select for the filter, but around 15 words seems like a good start:","metadata":{}},{"cell_type":"code","source":"comments_dataset = comments_dataset.filter(lambda x: x[\"comment_length\"] > 15)\ncomments_dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:28:02.699669Z","iopub.execute_input":"2024-05-05T16:28:02.700037Z","iopub.status.idle":"2024-05-05T16:28:02.762128Z","shell.execute_reply.started":"2024-05-05T16:28:02.700006Z","shell.execute_reply":"2024-05-05T16:28:02.761112Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/2964 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1450e0e38ae94bcbaee1825e5a97ce46"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['html_url', 'title', 'comments', 'body', 'comment_length'],\n    num_rows: 2175\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"Having cleaned up our dataset a bit, let’s concatenate the issue title, description, and comments together in a new text column. As usual, we’ll write a simple function that we can pass to Dataset.map():","metadata":{}},{"cell_type":"code","source":"def concatenate_text(examples):\n    return {\n        \"text\": examples[\"title\"]\n        + \" \\n \"\n        + examples[\"body\"]\n        + \" \\n \"\n        + examples[\"comments\"]\n    }\n\n\ncomments_dataset = comments_dataset.map(concatenate_text)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:28:08.251278Z","iopub.execute_input":"2024-05-05T16:28:08.251958Z","iopub.status.idle":"2024-05-05T16:28:08.653688Z","shell.execute_reply.started":"2024-05-05T16:28:08.251929Z","shell.execute_reply":"2024-05-05T16:28:08.652713Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2175 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9903cc924554b8bac2753e73d14f230"}},"metadata":{}}]},{"cell_type":"code","source":"comments_dataset[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating text embeddings\n\nWe saw in Chapter 2 that we can obtain token embeddings by using the AutoModel class. All we need to do is pick a suitable checkpoint to load the model from. Fortunately, there’s a library called sentence-transformers that is dedicated to creating embeddings. As described in the library’s documentation, our use case is an example of asymmetric semantic search because we have a short query whose answer we’d like to find in a longer document, like a an issue comment. The handy model overview table in the documentation indicates that the multi-qa-mpnet-base-dot-v1 checkpoint has the best performance for semantic search, so we’ll use that for our application. We’ll also load the tokenizer using the same checkpoint:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\n\nmodel_ckpt = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModel.from_pretrained(model_ckpt)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:28:18.533053Z","iopub.execute_input":"2024-05-05T16:28:18.533923Z","iopub.status.idle":"2024-05-05T16:28:22.293557Z","shell.execute_reply.started":"2024-05-05T16:28:18.533889Z","shell.execute_reply":"2024-05-05T16:28:22.292712Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"To speed up the embedding process, it helps to place the model and inputs on a GPU device, so let’s do that now:","metadata":{}},{"cell_type":"code","source":"import torch\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:28:27.537475Z","iopub.execute_input":"2024-05-05T16:28:27.538551Z","iopub.status.idle":"2024-05-05T16:28:27.785267Z","shell.execute_reply.started":"2024-05-05T16:28:27.538515Z","shell.execute_reply":"2024-05-05T16:28:27.784203Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"MPNetModel(\n  (embeddings): MPNetEmbeddings(\n    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n    (position_embeddings): Embedding(514, 768, padding_idx=1)\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): MPNetEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x MPNetLayer(\n        (attention): MPNetAttention(\n          (attn): MPNetSelfAttention(\n            (q): Linear(in_features=768, out_features=768, bias=True)\n            (k): Linear(in_features=768, out_features=768, bias=True)\n            (v): Linear(in_features=768, out_features=768, bias=True)\n            (o): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (intermediate): MPNetIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): MPNetOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (relative_attention_bias): Embedding(32, 12)\n  )\n  (pooler): MPNetPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"print(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:28:36.136608Z","iopub.execute_input":"2024-05-05T16:28:36.137297Z","iopub.status.idle":"2024-05-05T16:28:36.141663Z","shell.execute_reply.started":"2024-05-05T16:28:36.137264Z","shell.execute_reply":"2024-05-05T16:28:36.140772Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As we mentioned earlier, we’d like to represent each entry in our GitHub issues corpus as a single vector, so we need to “pool” or average our token embeddings in some way. One popular approach is to perform CLS pooling on our model’s outputs, where we simply collect the last hidden state for the special [CLS] token. The following function does the trick for us:","metadata":{}},{"cell_type":"code","source":"def cls_pooling(model_output):\n    #print( model_output.last_hidden_state, model_output.last_hidden_state.shape)\n    return model_output.last_hidden_state[:, 0]","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:28:47.655165Z","iopub.execute_input":"2024-05-05T16:28:47.656118Z","iopub.status.idle":"2024-05-05T16:28:47.660383Z","shell.execute_reply.started":"2024-05-05T16:28:47.656083Z","shell.execute_reply":"2024-05-05T16:28:47.659462Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Next, we’ll create a helper function that will tokenize a list of documents, place the tensors on the GPU, feed them to the model, and finally apply CLS pooling to the outputs:","metadata":{}},{"cell_type":"code","source":"def get_embeddings(text_list):\n    encoded_input = tokenizer(\n        text_list, padding=True, truncation=True, return_tensors=\"pt\"\n    )\n    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n    model_output = model(**encoded_input)\n    return cls_pooling(model_output)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:28:52.806277Z","iopub.execute_input":"2024-05-05T16:28:52.806639Z","iopub.status.idle":"2024-05-05T16:28:52.812041Z","shell.execute_reply.started":"2024-05-05T16:28:52.806612Z","shell.execute_reply":"2024-05-05T16:28:52.811105Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"We can test the function works by feeding it the first text entry in our corpus and inspecting the output shape:","metadata":{}},{"cell_type":"code","source":"embedding = get_embeddings(comments_dataset[\"text\"][0])\nembedding.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:28:57.839051Z","iopub.execute_input":"2024-05-05T16:28:57.839878Z","iopub.status.idle":"2024-05-05T16:28:58.312974Z","shell.execute_reply.started":"2024-05-05T16:28:57.839843Z","shell.execute_reply":"2024-05-05T16:28:58.312043Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 768])"},"metadata":{}}]},{"cell_type":"markdown","source":"Great, we’ve converted the first entry in our corpus into a 768-dimensional vector! We can use Dataset.map() to apply our get_embeddings() function to each row in our corpus, so let’s create a new embeddings column as follows:","metadata":{}},{"cell_type":"code","source":"embeddings_dataset = comments_dataset.map(\n    lambda x: {\"embeddings\": get_embeddings(x[\"text\"]).detach().cpu().numpy()[0]}\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:29:05.488938Z","iopub.execute_input":"2024-05-05T16:29:05.489821Z","iopub.status.idle":"2024-05-05T16:30:03.133405Z","shell.execute_reply.started":"2024-05-05T16:29:05.489787Z","shell.execute_reply":"2024-05-05T16:30:03.132460Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2175 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11d739c89d7e4d94abd19f4b892c2acf"}},"metadata":{}}]},{"cell_type":"markdown","source":"**Notice that we’ve converted the embeddings to NumPy arrays — that’s because 🤗 Datasets requires this format when we try to index them with FAISS, which we’ll do next.**\n\n---\n\n## Using FAISS for efficient similarity search\n\nNow that we have a dataset of embeddings, we need some way to search over them. To do this, we’ll use a special data structure in 🤗 Datasets called a FAISS index. FAISS (short for Facebook AI Similarity Search) is a library that provides efficient algorithms to quickly search and cluster embedding vectors.\n\nThe basic idea behind FAISS is to create a special data structure called an index that allows one to find which embeddings are similar to an input embedding. Creating a FAISS index in 🤗 Datasets is simple — we use the Dataset.add_faiss_index() function and specify which column of our dataset we’d like to index:","metadata":{}},{"cell_type":"code","source":"embeddings_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(len(embeddings_dataset[0][\"embeddings\"]))\n#display(embeddings_dataset[0][\"embeddings\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ADD FAISS INDEX\nembeddings_dataset.add_faiss_index(column=\"embeddings\")","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:30:10.757248Z","iopub.execute_input":"2024-05-05T16:30:10.758051Z","iopub.status.idle":"2024-05-05T16:30:10.880635Z","shell.execute_reply.started":"2024-05-05T16:30:10.758014Z","shell.execute_reply":"2024-05-05T16:30:10.879637Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d3a23ee943d4b19af047e6bf8f65c93"}},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['html_url', 'title', 'comments', 'body', 'comment_length', 'text', 'embeddings'],\n    num_rows: 2175\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"We can now perform queries on this index by doing a nearest neighbor lookup with the Dataset.get_nearest_examples() function. Let’s test this out by first embedding a question as follows:","metadata":{}},{"cell_type":"code","source":"question = \"How can I load a dataset offline?\"\nquestion_embedding = get_embeddings([question]).cpu().detach().numpy()\nquestion_embedding.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:30:28.306354Z","iopub.execute_input":"2024-05-05T16:30:28.307049Z","iopub.status.idle":"2024-05-05T16:30:28.340152Z","shell.execute_reply.started":"2024-05-05T16:30:28.307015Z","shell.execute_reply":"2024-05-05T16:30:28.339107Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(1, 768)"},"metadata":{}}]},{"cell_type":"markdown","source":"Just like with the documents, we now have a 768-dimensional vector representing the query, which we can compare against the whole corpus to find the most similar embeddings:","metadata":{}},{"cell_type":"code","source":"scores, samples = embeddings_dataset.get_nearest_examples(\n    \"embeddings\", question_embedding, k=5\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:30:43.834841Z","iopub.execute_input":"2024-05-05T16:30:43.835614Z","iopub.status.idle":"2024-05-05T16:30:43.844545Z","shell.execute_reply.started":"2024-05-05T16:30:43.835580Z","shell.execute_reply":"2024-05-05T16:30:43.843668Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"The Dataset.get_nearest_examples() function returns a tuple of scores that rank the overlap between the query and the document, and a corresponding set of samples (here, the 5 best matches). Let’s collect these in a pandas.DataFrame so we can easily sort them:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nsamples_df = pd.DataFrame.from_dict(samples)\nsamples_df[\"scores\"] = scores\nsamples_df.sort_values(\"scores\", ascending=False, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:31:19.455278Z","iopub.execute_input":"2024-05-05T16:31:19.456457Z","iopub.status.idle":"2024-05-05T16:31:19.464645Z","shell.execute_reply.started":"2024-05-05T16:31:19.456412Z","shell.execute_reply":"2024-05-05T16:31:19.463713Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Now we can iterate over the first few rows to see how well our query matched the available comments:\n\n","metadata":{}},{"cell_type":"code","source":"for _, row in samples_df.iterrows():\n    print(f\"COMMENT: {row.comments}\")\n    print(f\"SCORE: {row.scores}\")\n    print(f\"TITLE: {row.title}\")\n    print(f\"URL: {row.html_url}\")\n    print(\"=\" * 50)\n    print()","metadata":{"execution":{"iopub.status.busy":"2024-05-05T16:31:52.398343Z","iopub.execute_input":"2024-05-05T16:31:52.398790Z","iopub.status.idle":"2024-05-05T16:31:52.406069Z","shell.execute_reply.started":"2024-05-05T16:31:52.398752Z","shell.execute_reply":"2024-05-05T16:31:52.405109Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"COMMENT: Requiring online connection is a deal breaker in some cases unfortunately so it'd be great if offline mode is added similar to how `transformers` loads models offline fine.\n\n@mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\nSCORE: 25.505037307739258\nTITLE: Discussion using datasets in offline mode\nURL: https://github.com/huggingface/datasets/issues/824\n==================================================\n\nCOMMENT: The local dataset builders (csv, text , json and pandas) are now part of the `datasets` package since #1726 :)\nYou can now use them offline\n```python\ndatasets = load_dataset('text', data_files=data_files)\n```\n\nWe'll do a new release soon\nSCORE: 24.555490493774414\nTITLE: Discussion using datasets in offline mode\nURL: https://github.com/huggingface/datasets/issues/824\n==================================================\n\nCOMMENT: I opened a PR that allows to reload modules that have already been loaded once even if there's no internet.\n\nLet me know if you know other ways that can make the offline mode experience better. I'd be happy to add them :) \n\nI already note the \"freeze\" modules option, to prevent local modules updates. It would be a cool feature.\n\n----------\n\n> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?\n\nIndeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.\nFor example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do\n```python\nload_dataset(\"./my_dataset\")\n```\nand the dataset script will generate your dataset once and for all.\n\n----------\n\nAbout I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.\ncf #1724 \nSCORE: 24.14897918701172\nTITLE: Discussion using datasets in offline mode\nURL: https://github.com/huggingface/datasets/issues/824\n==================================================\n\nCOMMENT: > here is my way to load a dataset offline, but it **requires** an online machine\n> \n> 1. (online machine)\n> \n> ```\n> \n> import datasets\n> \n> data = datasets.load_dataset(...)\n> \n> data.save_to_disk(/YOUR/DATASET/DIR)\n> \n> ```\n> \n> 2. copy the dir from online to the offline machine\n> \n> 3. (offline machine)\n> \n> ```\n> \n> import datasets\n> \n> data = datasets.load_from_disk(/SAVED/DATA/DIR)\n> \n> ```\n> \n> \n> \n> HTH.\n\n\nSCORE: 22.894004821777344\nTITLE: Discussion using datasets in offline mode\nURL: https://github.com/huggingface/datasets/issues/824\n==================================================\n\nCOMMENT: here is my way to load a dataset offline, but it **requires** an online machine\n1. (online machine)\n```\nimport datasets\ndata = datasets.load_dataset(...)\ndata.save_to_disk(/YOUR/DATASET/DIR)\n```\n2. copy the dir from online to the offline machine\n3. (offline machine)\n```\nimport datasets\ndata = datasets.load_from_disk(/SAVED/DATA/DIR)\n```\n\nHTH.\nSCORE: 22.406648635864258\nTITLE: Discussion using datasets in offline mode\nURL: https://github.com/huggingface/datasets/issues/824\n==================================================\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}