{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Natural Language Processing with Transformers\n\n## Chapter 6 - Summarization\n\n- classic sequence to sequence task\n- we use an encoder-decoder model\n\ndataset used: CNN/DailyMail corpus - 300_000 pairs of news articles + summaries. **The summaries are abstractive not extractive** i.e. new sentences, not just excerpts from larger article.\n\nVersion 3.0.0 of dataset is NONANONYMIZED and used for summarization models\n\n","metadata":{}},{"cell_type":"code","source":"!pip install -U huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:28:07.194073Z","iopub.execute_input":"2024-05-12T16:28:07.194888Z","iopub.status.idle":"2024-05-12T16:28:21.626097Z","shell.execute_reply.started":"2024-05-12T16:28:07.194854Z","shell.execute_reply":"2024-05-12T16:28:21.625000Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (0.22.2)\nCollecting huggingface_hub\n  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.66.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub) (2024.2.2)\nDownloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface_hub\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.22.2\n    Uninstalling huggingface-hub-0.22.2:\n      Successfully uninstalled huggingface-hub-0.22.2\nSuccessfully installed huggingface_hub-0.23.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U datasets","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:28:40.933357Z","iopub.execute_input":"2024-05-12T16:28:40.934305Z","iopub.status.idle":"2024-05-12T16:28:54.625351Z","shell.execute_reply.started":"2024-05-12T16:28:40.934257Z","shell.execute_reply":"2024-05-12T16:28:54.624296Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nCollecting datasets\n  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.21.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.23.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-2.19.1-py3-none-any.whl (542 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: datasets\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.18.0\n    Uninstalling datasets-2.18.0:\n      Successfully uninstalled datasets-2.18.0\nSuccessfully installed datasets-2.19.1\n","output_type":"stream"}]},{"cell_type":"code","source":"# See AyhamAlom comment on\n# https://github.com/huggingface/datasets/issues/3465\n# there is a backup version of dataset\n# https://github.com/huggingface/datasets/issues/873\n\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"ccdv/cnn_dailymail\", version='3.0.0') # NEED TO DOWNLOAD DIFFERENT VERSION THAN IN HF BOOK\n\nprint(f\"Features: {dataset['train'].column_names}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:30:11.029378Z","iopub.execute_input":"2024-05-12T16:30:11.029781Z","iopub.status.idle":"2024-05-12T16:34:21.026646Z","shell.execute_reply.started":"2024-05-12T16:30:11.029751Z","shell.execute_reply":"2024-05-12T16:34:21.025747Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/159M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50b559d0a0724bd9aa3f08a5165f1bc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/376M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65b7c039882c4c7594aeebd292f38f07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/572k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31d1b3adc3f34ac19900102ee3b41d30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/12.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89e4365fd09140b7a9b6d0bec27813af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/661k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fba99620669746daa5d7e7837712ee31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"167d9fcdec3541cba6b9c49c1c30d3e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83911665e79e4d0abf9ac227915d48db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23cb223702e7457fb564caa05c43beba"}},"metadata":{}},{"name":"stdout","text":"Features: ['article', 'highlights', 'id']\n","output_type":"stream"}]},{"cell_type":"code","source":"sample = dataset[\"train\"][1]\nprint(f\"\"\"\nArticle (excerpt of 500 characters, total length: {len(sample[\"article\"])}):\n\"\"\")\nprint(sample[\"article\"][:500])\nprint(f'\\nSummary (length: {len(sample[\"highlights\"])}):')\nprint(sample[\"highlights\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:35:11.737413Z","iopub.execute_input":"2024-05-12T16:35:11.737800Z","iopub.status.idle":"2024-05-12T16:35:11.748371Z","shell.execute_reply.started":"2024-05-12T16:35:11.737772Z","shell.execute_reply":"2024-05-12T16:35:11.747379Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\nArticle (excerpt of 500 characters, total length: 3192):\n\n(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay. The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds. The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover. The 26-year-old Bolt has n\n\nSummary (length: 180):\nUsain Bolt wins third gold of world championship .\nAnchors Jamaica to 4x100m relay victory .\nEighth gold at the championships for Bolt .\nJamaica double up in women's 4x100m relay .\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We see that the articles can be very long compared to the target summary; in this par‐\nticular case the difference is 17-fold. Long articles pose a challenge to most trans‐\nformer models since the context size is usually limited to 1,000 tokens or so, which is\nequivalent to a few paragraphs of text. The standard, yet crude way to deal with this\nfor summarization is to simply truncate the texts beyond the model’s context size.\nObviously there could be important information for the summary toward the end of\nthe text, but for now we need to live with this limitation of the model architectures.\n\n---\n\n## Examples with text summary pipelines and different existing models\n\n- here look at behavior/outputs from existing models\n- take the same example for all models, and first 2000 chars of the paragraph (the above article on sprinters)","metadata":{}},{"cell_type":"code","source":"sample_text = dataset[\"train\"][1][\"article\"][:2000]\n# We'll collect the generated summaries of each model in a dictionary\nsummaries = {}","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:36:27.441955Z","iopub.execute_input":"2024-05-12T16:36:27.442544Z","iopub.status.idle":"2024-05-12T16:36:27.447170Z","shell.execute_reply.started":"2024-05-12T16:36:27.442515Z","shell.execute_reply":"2024-05-12T16:36:27.446390Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**IMPORTANT:**\n\nA convention in summarization is to separate the summary sentences by a newline.\nWe could add a newline token after each full stop, but this simple heuristic would fail\nfor strings like “U.S.” or “U.N.” The Natural Language Toolkit (NLTK) package\nincludes a more sophisticated algorithm that can differentiate the end of a sentence\nfrom punctuation that occurs in abbreviations:","metadata":{}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import sent_tokenize\n\nnltk.download(\"punkt\")\n\nstring = \"The U.S. are a country. The U.N. is an organization.\"\n\nsent_tokenize(string)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:37:08.460488Z","iopub.execute_input":"2024-05-12T16:37:08.460832Z","iopub.status.idle":"2024-05-12T16:37:08.611687Z","shell.execute_reply.started":"2024-05-12T16:37:08.460808Z","shell.execute_reply":"2024-05-12T16:37:08.610694Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"['The U.S. are a country.', 'The U.N. is an organization.']"},"metadata":{}}]},{"cell_type":"markdown","source":"### Summarization baseline - take first 3 sentences\n\nThis is standard practice / \"Dummy Summary\" kind of approach","metadata":{}},{"cell_type":"code","source":"def three_sentence_summary(text):\n    return \"\\n\".join(sent_tokenize(text)[:3])\n\nsummaries[\"baseline\"] = three_sentence_summary(sample_text)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:37:54.711506Z","iopub.execute_input":"2024-05-12T16:37:54.712451Z","iopub.status.idle":"2024-05-12T16:37:54.717964Z","shell.execute_reply.started":"2024-05-12T16:37:54.712419Z","shell.execute_reply":"2024-05-12T16:37:54.717007Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### GPT-2, T5, BART, PEGASUS models now:\n\n- for gpt-2, get summary by appending \"TL;DR\" to input\n- for T5: **TODO: reread** talks about how it was trained on a variety of tasks, all formulated as text-to-text tasks; and all you do is prepend your data with an appropriate prompt e.g. `summarize:<ARTICLE>` or `translate English to German:<TEXT>`\n- BART - **NOTE THAT WE ARE USING bart-large-cnn THAT HAS BEEN FINETUNED ON THIS DATASET ALREADY**\n- PEGASUS: is a SOTA (2020) model for text summarization - pretraining is predict masked sentence in MULTISENTENCE tasks (**TODO: read paper / description**)\n\nNOTE: PEGASUS has special token for newlines so you don't need the NLTK `sent_tokenize()` function","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, set_seed\n\nset_seed(42)\n\npipe = pipeline(\"text-generation\", model=\"gpt2-xl\")\ngpt2_query = sample_text + \"\\nTL;DR:\\n\"\n\npipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n\nsummaries[\"gpt2\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query) :]))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:39:15.352432Z","iopub.execute_input":"2024-05-12T16:39:15.353333Z","iopub.status.idle":"2024-05-12T16:40:23.471834Z","shell.execute_reply.started":"2024-05-12T16:39:15.353288Z","shell.execute_reply":"2024-05-12T16:40:23.470827Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"2024-05-12 16:39:20.336825: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-12 16:39:20.336958: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-12 16:39:20.470686: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e623a8509ff49fb8dfc4ea2742d1095"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fa8ca33f0be46b5b66f3099941871ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ef04dea7bc44ea8b1d6a197b211028d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dec0c3d8996d4161aa8446a006b123b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"145b2dd6b1024ee987d52c0a1ae6d367"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83d4881cca014cfe98187015633de86b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cea87a2b639f4b1c8489d6d69551fdf3"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"}]},{"cell_type":"code","source":"pipe = pipeline(\"summarization\", model=\"t5-large\")\npipe_out = pipe(sample_text)\nsummaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:41:11.975762Z","iopub.execute_input":"2024-05-12T16:41:11.976121Z","iopub.status.idle":"2024-05-12T16:41:54.339671Z","shell.execute_reply.started":"2024-05-12T16:41:11.976093Z","shell.execute_reply":"2024-05-12T16:41:54.338823Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d2ca118f22c4fa5aa1c7ea5f840675e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7ec5383ac8e4a859989a179e14cb7d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb439f3adbb04a12917288e541eb7f47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e6a889dd3e64b439ddb58c0bd7071a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1c0d6d94cc54da59894f1ebfbab0dd2"}},"metadata":{}}]},{"cell_type":"code","source":"pipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\npipe_out = pipe(sample_text)\nsummaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:43:26.136918Z","iopub.execute_input":"2024-05-12T16:43:26.137735Z","iopub.status.idle":"2024-05-12T16:43:44.900398Z","shell.execute_reply.started":"2024-05-12T16:43:26.137703Z","shell.execute_reply":"2024-05-12T16:43:44.899384Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3256709a35d74110be60f5a2e5feef4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1010cc1673f3451cb3c6232878a25336"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0a0e8c949244dc585ed44d66a70ccde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a5e2b749cf94b08a58d011ffc0fde01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4d043c440d14099b81af10809c9d05e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8676a8bf53c4f5c8bf1f24060df7784"}},"metadata":{}}]},{"cell_type":"code","source":"#\n# NOTE: PEGASUS has special token for newlines so you don't need the NLTK `sent_tokenize()` function\n#\n\npipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\npipe_out = pipe(sample_text)\nsummaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:45:52.731634Z","iopub.execute_input":"2024-05-12T16:45:52.732350Z","iopub.status.idle":"2024-05-12T16:46:31.165015Z","shell.execute_reply.started":"2024-05-12T16:45:52.732305Z","shell.execute_reply":"2024-05-12T16:46:31.164097Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1d0d03c49f64e29bc0faf930b48c372"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88082b41880f4899b2d7919c806637cd"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2956c05c40104cee80fc1728d7da2277"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d147305c8ae246d58e55125f9fbde397"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"879c9e6bce3a4b83a0b3d7757902a523"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a41ba4d272348c5881daae5387ae513"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Overview of Summarization performance\n\nKeep in mind that one model has not been trained on the dataset at all\n(GPT-2), one model has been fine-tuned on this task among others (T5), and two\nmodels have exclusively been fine-tuned on this task (BART and PEGASUS). ","metadata":{}},{"cell_type":"code","source":"print(\"GROUND TRUTH\")\nprint(dataset[\"train\"][1][\"highlights\"])\nprint(\"\")\n\nfor model_name in summaries:\n    print(model_name.upper())\n    print(summaries[model_name])\n    print(\"\")","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:46:39.262114Z","iopub.execute_input":"2024-05-12T16:46:39.262781Z","iopub.status.idle":"2024-05-12T16:46:39.303749Z","shell.execute_reply.started":"2024-05-12T16:46:39.262729Z","shell.execute_reply":"2024-05-12T16:46:39.302721Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"GROUND TRUTH\nUsain Bolt wins third gold of world championship .\nAnchors Jamaica to 4x100m relay victory .\nEighth gold at the championships for Bolt .\nJamaica double up in women's 4x100m relay .\n\nBASELINE\n(CNN) -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men's 4x100m relay.\nThe fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter, Kemar Bailey-Cole, Nickel Ashmeade and Bolt won in 37.36 seconds.\nThe U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover.\n\nGPT2\nBolt finished in style by winning the final sprint event in Moscow.\nThere is still a few sprint events to go before Friday's final but so far this final gold should stand as a world record.\n\nT5\nusain bolt wins his third gold medal of the world championships in the men's 4x100m relay .\nthe 26-year-old anchored Jamaica to victory in the event in the Russian capital .\nhe has now collected eight gold medals at the championships, equaling the record .\n\nBART\nUsain Bolt wins his third gold of the world championships in Moscow.\nBolt anchors Jamaica to victory in the men's 4x100m relay.\nThe 26-year-old has now won eight gold medals at world championships.\nJamaica's women also win gold in the relay, beating France in the process.\n\nPEGASUS\nUsain Bolt wins third gold of world championships.\nAnchors Jamaica to victory in men's 4x100m relay.\nEighth gold at the championships for Bolt.\nJamaica also win women's 4x100m relay .\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Measuring the Quality of Generated Text\n\n- by eye, all the above seem OK - PEGASUS subjectively looks the best to me\n\nNeed to develop metrics to do this systematically.\n\nBLEU and ROUGE explanations:\n\n### BLEU\n\n- precision-based metric\n- instead of looking at alignment of tokens, we look at words or n-grams (??? clear this up after reading)\n- count the number of words in the generation that occur in the reference and divide it by length of reference\n\n**important correction:** we clip to make sure the limit count is based on how many times the word ACTUALLY occurs in the reference.\n\nexample: sentence is \"the cat is on the mat\" . Prediction is \"the the the the the the\" -> then before clipping, we have 6 correct predictions / length 6 = precision 100%. After clipping we have 2 correct predictions.\n\n- there is then some further modifications:\n\nYou do a `prec_n` for all possible n-grams of degree n, and use the above clipping in all cases\n\nAlso:\n\nSince we are not looking at recall, all generated sequences that are\nshort but precise have a benefit compared to sentences that are longer. Therefore, the\nprecision score favors short generations. To compensate for that the authors of BLEU\nintroduced an additional term, the brevity penalty\n\n**here a formula which just has length as an exponent - simple**\n\nPeople report the BLEU-N score where N is \"we use n-grams up to N\"\n\n---\n\nHowever, you can probably already see\nthat this metric has many limitations; for instance, it doesn’t take synonyms into\naccount, and many steps in the derivation seem like ad hoc and rather fragile heuristics. You can find a wonderful exposition of BLEU’s flaws in Rachel Tatman’s blog\npost “Evaluating Text Output in NLP: BLEU at Your Own Risk”.\n\nIn general, the field of text generation is still looking for better evaluation metrics,\nand finding ways to overcome the limits of metrics like BLEU is an active area of\nresearch. Another weakness of the BLEU metric is that it expects the text to already\nbe tokenized. This can lead to varying results if the exact same method for text toke‐\nnization is not used. The SacreBLEU metric addresses this issue by internalizing the\ntokenization step; for this reason, it is the preferred metric for benchmarking.\n\n\n---\n\nUsing metrics from datasets library:","metadata":{}},{"cell_type":"code","source":"!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:58:24.171120Z","iopub.execute_input":"2024-05-12T16:58:24.171467Z","iopub.status.idle":"2024-05-12T16:58:37.446791Z","shell.execute_reply.started":"2024-05-12T16:58:24.171440Z","shell.execute_reply":"2024-05-12T16:58:37.445660Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.19.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.2.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:59:06.337937Z","iopub.execute_input":"2024-05-12T16:59:06.338309Z","iopub.status.idle":"2024-05-12T16:59:19.338792Z","shell.execute_reply.started":"2024-05-12T16:59:06.338279Z","shell.execute_reply":"2024-05-12T16:59:19.337669Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2023.12.25)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.2.1)\nDownloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-2.8.2-py3-none-any.whl (17 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-2.8.2 sacrebleu-2.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"#from datasets import load_metric\n#\n# NOTE BOOK CODE IS DEPRECATED - HAVE TO USE EVALUATE INSTEAD NOW\n#\n\nimport evaluate\n\nbleu_metric = evaluate.load(\"sacrebleu\")","metadata":{"execution":{"iopub.status.busy":"2024-05-12T16:59:24.072044Z","iopub.execute_input":"2024-05-12T16:59:24.072441Z","iopub.status.idle":"2024-05-12T16:59:24.767948Z","shell.execute_reply.started":"2024-05-12T16:59:24.072409Z","shell.execute_reply":"2024-05-12T16:59:24.766871Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"The bleu_metric object is an instance of the Metric class, and works like an aggregator: you can add single instances with add() or whole batches via add_batch(). Once\nyou have added all the samples you need to evaluate, you then call compute() and the\nmetric is calculated. This returns a dictionary with several values, such as the precision for each n-gram, the length penalty, as well as the final BLEU score. Let’s look at\nthe example from before:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nbleu_metric.add(prediction=\"the the the the the the\", reference=[\"the cat is on the mat\"])\n\nresults = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)\n\nresults[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\n\npd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-12T17:00:47.552375Z","iopub.execute_input":"2024-05-12T17:00:47.553929Z","iopub.status.idle":"2024-05-12T17:00:47.592735Z","shell.execute_reply.started":"2024-05-12T17:00:47.553895Z","shell.execute_reply":"2024-05-12T17:00:47.591843Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"                             Value\nscore                          0.0\ncounts                [2, 0, 0, 0]\ntotals                [6, 5, 4, 3]\nprecisions  [33.33, 0.0, 0.0, 0.0]\nbp                             1.0\nsys_len                          6\nref_len                          6","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>score</th>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>counts</th>\n      <td>[2, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>totals</th>\n      <td>[6, 5, 4, 3]</td>\n    </tr>\n    <tr>\n      <th>precisions</th>\n      <td>[33.33, 0.0, 0.0, 0.0]</td>\n    </tr>\n    <tr>\n      <th>bp</th>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>sys_len</th>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>ref_len</th>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**INTERPRETATION - THE ABOVE 4 VALUES ARE FOR n=1,2,3,4 n-GRAMS**","metadata":{}},{"cell_type":"code","source":"bleu_metric.add(prediction=\"the cat is on mat\", reference=[\"the cat is on the mat\"])\n\nresults = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)\nresults[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\npd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-12T17:02:25.087014Z","iopub.execute_input":"2024-05-12T17:02:25.087419Z","iopub.status.idle":"2024-05-12T17:02:25.108868Z","shell.execute_reply.started":"2024-05-12T17:02:25.087390Z","shell.execute_reply":"2024-05-12T17:02:25.107721Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"                                 Value\nscore                        57.893007\ncounts                    [5, 3, 2, 1]\ntotals                    [5, 4, 3, 2]\nprecisions  [100.0, 75.0, 66.67, 50.0]\nbp                            0.818731\nsys_len                              5\nref_len                              6","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>score</th>\n      <td>57.893007</td>\n    </tr>\n    <tr>\n      <th>counts</th>\n      <td>[5, 3, 2, 1]</td>\n    </tr>\n    <tr>\n      <th>totals</th>\n      <td>[5, 4, 3, 2]</td>\n    </tr>\n    <tr>\n      <th>precisions</th>\n      <td>[100.0, 75.0, 66.67, 50.0]</td>\n    </tr>\n    <tr>\n      <th>bp</th>\n      <td>0.818731</td>\n    </tr>\n    <tr>\n      <th>sys_len</th>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>ref_len</th>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## ROUGE\n\nThe BLEU score is widely used for evaluating text, especially in machine translation,\nsince precise translations are usually favored over translations that include all possible\nand appropriate words.\n\nThere are other applications, such as summarization, where the situation is different.\nThere, we want all the important information in the generated text, so we favor high\nrecall. This is where the ROUGE score is usually used.\n\nThe ROUGE score was specifically developed for applications like summarization\nwhere high recall is more important than just precision\n\nThe approach is very similar\nto the BLEU score in that we look at different n-grams and compare their occurrences\nin the generated text and the reference texts. The difference is that with ROUGE we\ncheck how many n-grams in the reference text also occur in the generated text. For\nBLEU we looked at how many n-grams in the generated text appear in the reference,\nso we can reuse the precision formula with the minor modification that we count the\n(unclipped) occurrence of reference n-grams in the generated text in the numerator\n\nThis was the original proposal for ROUGE. Subsequently, researchers have found that\nfully removing precision can have strong negative effects. Going back to the BLEU\nformula without the clipped counting, we can measure precision as well, and we can\nthen combine both precision and recall ROUGE scores in the harmonic mean to get\nan F1-score. This score is the metric that is nowadays commonly reported for\nROUGE.\n\n**TODO: read about the differences between the different variations of ROUGE (see results below)**","metadata":{}},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-05-12T17:06:23.201331Z","iopub.execute_input":"2024-05-12T17:06:23.201731Z","iopub.status.idle":"2024-05-12T17:06:38.302576Z","shell.execute_reply.started":"2024-05-12T17:06:23.201702Z","shell.execute_reply":"2024-05-12T17:06:38.301380Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=abbdd31a571d549c90b0c9b297db2656b127418f4cd8389cebab8ed78d119cb2\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"rouge_metric = evaluate.load(\"rouge\")","metadata":{"execution":{"iopub.status.busy":"2024-05-12T17:06:52.046688Z","iopub.execute_input":"2024-05-12T17:06:52.047546Z","iopub.status.idle":"2024-05-12T17:06:52.658294Z","shell.execute_reply.started":"2024-05-12T17:06:52.047498Z","shell.execute_reply":"2024-05-12T17:06:52.657315Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### Apply ROUGE score to the summaries created by the various models earlier\n\n**NOTE: therefore that here the `reference` is the gold standard summary from the dataset**","metadata":{}},{"cell_type":"code","source":"reference = dataset[\"train\"][1][\"highlights\"]\n\nrecords = []\n\n# TODO: understand the differences between these methods\nrouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n\nfor model_name in summaries:\n    rouge_metric.add(prediction=summaries[model_name], reference=reference)\n    score = rouge_metric.compute()\n    \n    # NOTE DEPRECATED CODE IN BOOK\n    # https://stackoverflow.com/questions/73826120/numpy-float64-object-has-no-attribute-mid\n    # GET THE ERROR:\n    # AttributeError: 'numpy.float64' object has no attribute 'mid'\n    # ITS FROM OLD HF IMPLEMENTATION - JUST DROP THE .mid.fmeasure STUFF AS IN THE STACKOVERFLOW POST O_o\n    #\n    #rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n    rouge_dict = dict((rn, score[rn]) for rn in rouge_names)\n    \n    records.append(rouge_dict)\n\npd.DataFrame.from_records(records, index=summaries.keys())","metadata":{"execution":{"iopub.status.busy":"2024-05-12T17:09:21.255010Z","iopub.execute_input":"2024-05-12T17:09:21.255385Z","iopub.status.idle":"2024-05-12T17:09:22.302476Z","shell.execute_reply.started":"2024-05-12T17:09:21.255356Z","shell.execute_reply":"2024-05-12T17:09:22.301534Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"            rouge1    rouge2    rougeL  rougeLsum\nbaseline  0.303571  0.090909  0.214286   0.232143\ngpt2      0.212121  0.000000  0.121212   0.212121\nt5        0.486486  0.222222  0.378378   0.486486\nbart      0.582278  0.207792  0.455696   0.506329\npegasus   0.866667  0.655172  0.800000   0.833333","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>baseline</th>\n      <td>0.303571</td>\n      <td>0.090909</td>\n      <td>0.214286</td>\n      <td>0.232143</td>\n    </tr>\n    <tr>\n      <th>gpt2</th>\n      <td>0.212121</td>\n      <td>0.000000</td>\n      <td>0.121212</td>\n      <td>0.212121</td>\n    </tr>\n    <tr>\n      <th>t5</th>\n      <td>0.486486</td>\n      <td>0.222222</td>\n      <td>0.378378</td>\n      <td>0.486486</td>\n    </tr>\n    <tr>\n      <th>bart</th>\n      <td>0.582278</td>\n      <td>0.207792</td>\n      <td>0.455696</td>\n      <td>0.506329</td>\n    </tr>\n    <tr>\n      <th>pegasus</th>\n      <td>0.866667</td>\n      <td>0.655172</td>\n      <td>0.800000</td>\n      <td>0.833333</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Evaluating PEGASUS on the CNN DailyMail dataset\n\n- code for the 3 sentence dummy summary baseline:","metadata":{}},{"cell_type":"code","source":"def evaluate_summaries_baseline(dataset, metric, column_text=\"article\",column_summary=\"highlights\"):\n    summaries = [three_sentence_summary(text) for text in dataset[column_text]]\n    metric.add_batch(predictions=summaries, references=dataset[column_summary])\n    score = metric.compute()\n    return score","metadata":{"execution":{"iopub.status.busy":"2024-05-12T17:12:58.460942Z","iopub.execute_input":"2024-05-12T17:12:58.461386Z","iopub.status.idle":"2024-05-12T17:12:58.467219Z","shell.execute_reply.started":"2024-05-12T17:12:58.461350Z","shell.execute_reply":"2024-05-12T17:12:58.466255Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"**Test dataset split is 10k setences - too large: use 50 instead**\n\nHF BOOK USES 1k SAMPLES FOR 1 HOUR OF GPU O_o i noticed, so just used 50 - don't care about the results just wanted to see behaviour","metadata":{}},{"cell_type":"code","source":"test_sampled = dataset[\"test\"].shuffle(seed=42).select(range(50))\n\nscore = evaluate_summaries_baseline(test_sampled, rouge_metric)\n\n#rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\nrouge_dict = dict((rn, score[rn]) for rn in rouge_names)\n\npd.DataFrame.from_dict(rouge_dict, orient=\"index\", columns=[\"baseline\"]).T","metadata":{"execution":{"iopub.status.busy":"2024-05-12T17:15:59.228159Z","iopub.execute_input":"2024-05-12T17:15:59.229137Z","iopub.status.idle":"2024-05-12T17:16:00.080361Z","shell.execute_reply.started":"2024-05-12T17:15:59.229102Z","shell.execute_reply":"2024-05-12T17:16:00.079342Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"            rouge1    rouge2    rougeL  rougeLsum\nbaseline  0.408768  0.198378  0.271183   0.380515","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>baseline</th>\n      <td>0.408768</td>\n      <td>0.198378</td>\n      <td>0.271183</td>\n      <td>0.380515</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Now implement evaluation func for PEGASUS model:**","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ndef chunks(list_of_elements, batch_size):\n    \"\"\"Yield successive batch-sized chunks from list_of_elements.\"\"\"\n    for i in range(0, len(list_of_elements), batch_size):\n        yield list_of_elements[i : i + batch_size]\n        \n        \ndef evaluate_summaries_pegasus(dataset, metric, model, tokenizer, batch_size=16, device=device,column_text=\"article\",column_summary=\"highlights\"):\n    article_batches = list(chunks(dataset[column_text], batch_size))\n    target_batches = list(chunks(dataset[column_summary], batch_size))\n    for article_batch, target_batch in tqdm(zip(article_batches, target_batches), total=len(article_batches)):\n        inputs = tokenizer(article_batch, max_length=1024, truncation=True,padding=\"max_length\", return_tensors=\"pt\")\n        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),attention_mask=inputs[\"attention_mask\"].to(device),length_penalty=0.8, num_beams=8, max_length=128)\n        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,clean_up_tokenization_spaces=True) for s in summaries]\n        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n    metric.add_batch(predictions=decoded_summaries, references=target_batch)\n    score = metric.compute()\n    return score","metadata":{"execution":{"iopub.status.busy":"2024-05-12T17:19:18.474364Z","iopub.execute_input":"2024-05-12T17:19:18.475110Z","iopub.status.idle":"2024-05-12T17:19:18.541486Z","shell.execute_reply.started":"2024-05-12T17:19:18.475077Z","shell.execute_reply":"2024-05-12T17:19:18.540106Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"Let’s unpack this evaluation code a bit. First we split the dataset into smaller batches\nthat we can process simultaneously. Then for each batch we tokenize the input articles and feed them to the generate() function to produce the summaries using beam\nsearch. We use the same generation parameters as proposed in the paper. The new\nparameter for length penalty ensures that the model does not generate sequences that\nare too long. Finally, we decode the generated texts, replace the <n> token, and add\nthe decoded texts with the references to the metric. At the end, we compute and\nreturn the ROUGE scores. Let’s now load the model again with the AutoModelFor\nSeq2SeqLM class, used for seq2seq generation tasks, and evaluate it:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel_ckpt = \"google/pegasus-cnn_dailymail\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\nscore = evaluate_summaries_pegasus(test_sampled, rouge_metric, model, tokenizer, batch_size=8)\n\n#rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\nrouge_dict = dict((rn, score[rn]) for rn in rouge_names)\n\npd.DataFrame(rouge_dict, index=[\"pegasus\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-12T17:21:59.251018Z","iopub.execute_input":"2024-05-12T17:21:59.252096Z","iopub.status.idle":"2024-05-12T17:22:59.613219Z","shell.execute_reply.started":"2024-05-12T17:21:59.252055Z","shell.execute_reply":"2024-05-12T17:22:59.612362Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n100%|██████████| 7/7 [00:49<00:00,  7.08s/it]\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"           rouge1    rouge2   rougeL  rougeLsum\npegasus  0.518118  0.284161  0.31865   0.439328","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>pegasus</th>\n      <td>0.518118</td>\n      <td>0.284161</td>\n      <td>0.31865</td>\n      <td>0.439328</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**NOTE/reminder - i only did 50 samples instead of 1k or even 10k from the test dataset so scores above aren't to be taken too seriously**\n\nbook has 0.43/.21/.30/.37 so not too far from the above scores\n\n---\n\n# Training a Summarization Model\n\nWe now use the `SAMSum` dataset (from Samsung - collection of dialogues + summaries of them)","metadata":{}},{"cell_type":"code","source":"dataset_samsum = load_dataset(\"samsum\")\nsplit_lengths = [len(dataset_samsum[split])for split in dataset_samsum]\n\n\nprint(f\"Split lengths: {split_lengths}\")\nprint(f\"Features: {dataset_samsum['train'].column_names}\")\n\nprint(\"\\nDialogue:\")\nprint(dataset_samsum[\"test\"][0][\"dialogue\"])\nprint(\"\\nSummary:\")\nprint(dataset_samsum[\"test\"][0][\"summary\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-12T17:31:08.693983Z","iopub.execute_input":"2024-05-12T17:31:08.695084Z","iopub.status.idle":"2024-05-12T17:31:12.835814Z","shell.execute_reply.started":"2024-05-12T17:31:08.695048Z","shell.execute_reply":"2024-05-12T17:31:12.834789Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"Downloading data: 100%|██████████| 6.06M/6.06M [00:00<00:00, 24.6MB/s]\nDownloading data: 100%|██████████| 347k/347k [00:00<00:00, 3.02MB/s]\nDownloading data: 100%|██████████| 335k/335k [00:00<00:00, 2.12MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30f4e908e45a485695e0b8696f5b7ef2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d419f7de4cb47ce85bcf2f672116f4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75b49a79bfe74d24b521073a97f28f11"}},"metadata":{}},{"name":"stdout","text":"Split lengths: [14732, 819, 818]\nFeatures: ['id', 'dialogue', 'summary']\n\nDialogue:\nHannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nHannah: <file_gif>\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nHannah: <file_gif>\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him 🙂\nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye\n\nSummary:\nHannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**note: the (placeholder) gifs and emojis- it's meant to be a whatsapp conversation**\n\n---\n\n## First - evaluate PEGASUS baseline on SAMSum\n\n- remember that PEGASUS has been trained on CNN/DailyMail for summarization; here we have a chat-based format so might not work correctly:\n\n","metadata":{}},{"cell_type":"code","source":"pipe_out = pipe(dataset_samsum[\"test\"][0][\"dialogue\"])\nprint(\"Summary:\")\nprint(pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\"))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T17:48:29.123588Z","iopub.execute_input":"2024-05-12T17:48:29.124404Z","iopub.status.idle":"2024-05-12T17:48:43.115743Z","shell.execute_reply.started":"2024-05-12T17:48:29.124369Z","shell.execute_reply":"2024-05-12T17:48:43.114700Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stderr","text":"Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n","output_type":"stream"},{"name":"stdout","text":"Summary:\nAmanda: Ask Larry Amanda: He called her last time we were at the park together.\nHannah: I'd rather you texted him.\nAmanda: Just text him .\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can see that the model mostly tries to summarize by extracting the key sentences\nfrom the dialogue. This probably worked relatively well on the CNN/DailyMail data‐\nset, but the summaries in SAMSum are more abstract. \n\n**Let’s confirm this by running the full ROUGE evaluation on the test set:**","metadata":{}},{"cell_type":"code","source":"score = evaluate_summaries_pegasus(dataset_samsum[\"test\"], rouge_metric, model, tokenizer, column_text=\"dialogue\", column_summary=\"summary\", batch_size=8)\n\n#rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\nrouge_dict = dict((rn, score[rn]) for rn in rouge_names)\n\npd.DataFrame(rouge_dict, index=[\"pegasus\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-12T17:51:54.816021Z","iopub.execute_input":"2024-05-12T17:51:54.816488Z","iopub.status.idle":"2024-05-12T18:04:20.166375Z","shell.execute_reply.started":"2024-05-12T17:51:54.816458Z","shell.execute_reply":"2024-05-12T18:04:20.164814Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"100%|██████████| 103/103 [12:25<00:00,  7.23s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[44], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m score \u001b[38;5;241m=\u001b[39m evaluate_summaries_pegasus(dataset_samsum[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m], rouge_metric, model, tokenizer, column_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdialogue\u001b[39m\u001b[38;5;124m\"\u001b[39m, column_summary\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m rouge_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrn\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmeasure\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrouge_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(rouge_dict, index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpegasus\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n","Cell \u001b[0;32mIn[44], line 3\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m score \u001b[38;5;241m=\u001b[39m evaluate_summaries_pegasus(dataset_samsum[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m], rouge_metric, model, tokenizer, column_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdialogue\u001b[39m\u001b[38;5;124m\"\u001b[39m, column_summary\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m rouge_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m((rn, \u001b[43mscore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrn\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmid\u001b[49m\u001b[38;5;241m.\u001b[39mfmeasure) \u001b[38;5;28;01mfor\u001b[39;00m rn \u001b[38;5;129;01min\u001b[39;00m rouge_names)\n\u001b[1;32m      5\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(rouge_dict, index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpegasus\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n","\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'mid'"],"ename":"AttributeError","evalue":"'numpy.float64' object has no attribute 'mid'","output_type":"error"}]},{"cell_type":"markdown","source":"**NOTE: i ran it for 15mins only to get the AttributeError: 'numpy.float64' object has no attribute 'mid'**\n\n**NEED TO REPLACE: rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)**\n\nand then rerun - not important though so skipped for now.\n\nWell, the results aren’t great, but this is not unexpected since we’ve moved quite a bit away from the CNN/DailyMail data distribution\n\n---\n\n## Finetuning PEGASUS\n\n- look at stats/distribution of dataset","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-05-12T18:09:03.357370Z","iopub.execute_input":"2024-05-12T18:09:03.357767Z","iopub.status.idle":"2024-05-12T18:09:03.362337Z","shell.execute_reply.started":"2024-05-12T18:09:03.357736Z","shell.execute_reply":"2024-05-12T18:09:03.361376Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"d_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"dialogue\"]]\ns_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"summary\"]]\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\n\naxes[0].hist(d_len, bins=20, color=\"C0\", edgecolor=\"C0\")\naxes[0].set_title(\"Dialogue Token Length\")\naxes[0].set_xlabel(\"Length\")\naxes[0].set_ylabel(\"Count\")\n\naxes[1].hist(s_len, bins=20, color=\"C0\", edgecolor=\"C0\")\naxes[1].set_title(\"Summary Token Length\")\naxes[1].set_xlabel(\"Length\")\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T18:09:05.749826Z","iopub.execute_input":"2024-05-12T18:09:05.750216Z","iopub.status.idle":"2024-05-12T18:09:17.968596Z","shell.execute_reply.started":"2024-05-12T18:09:05.750185Z","shell.execute_reply":"2024-05-12T18:09:17.967553Z"},"trusted":true},"execution_count":47,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x350 with 2 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"We see that most dialogues are much shorter than the CNN/DailyMail articles, with\n100–200 tokens per dialogue. Similarly, the summaries are much shorter, with around\n20–40 tokens (the average length of a tweet).\n\nLet’s keep those observations in mind as we build the data collator for the Trainer.\nFirst we need to tokenize the dataset. For now, we’ll set the maximum lengths to 1024\nand 128 for the dialogues and summaries, respectively","metadata":{}},{"cell_type":"code","source":"def convert_examples_to_features(example_batch):\n    input_encodings = tokenizer(example_batch[\"dialogue\"], max_length=1024, truncation=True)\n    with tokenizer.as_target_tokenizer():\n        target_encodings = tokenizer(example_batch[\"summary\"], max_length=128,truncation=True)\n    return {\"input_ids\": input_encodings[\"input_ids\"], \"attention_mask\": input_encodings[\"attention_mask\"], \"labels\": target_encodings[\"input_ids\"]}","metadata":{"execution":{"iopub.status.busy":"2024-05-12T18:12:03.546433Z","iopub.execute_input":"2024-05-12T18:12:03.547454Z","iopub.status.idle":"2024-05-12T18:12:03.554408Z","shell.execute_reply.started":"2024-05-12T18:12:03.547417Z","shell.execute_reply":"2024-05-12T18:12:03.553290Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"### tokenizer for decoder inputs and special tokens\n\n**IMPORTANT: new when working with encoder - decoder stuff:**\n\nA new thing in the use of the tokenization step is the `tokenizer.as_target_tokenizer()` context. Some models require special tokens in the decoder inputs, so it’s\nimportant to differentiate between the tokenization of encoder and decoder inputs. In\nthe with statement (called a context manager), the tokenizer knows that it is tokenizing for the decoder and can process sequences accordingly.","metadata":{}},{"cell_type":"code","source":"dataset_samsum[\"train\"][0]","metadata":{"execution":{"iopub.status.busy":"2024-05-12T18:12:36.157923Z","iopub.execute_input":"2024-05-12T18:12:36.158530Z","iopub.status.idle":"2024-05-12T18:12:36.165600Z","shell.execute_reply.started":"2024-05-12T18:12:36.158503Z","shell.execute_reply":"2024-05-12T18:12:36.164689Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"{'id': '13818513',\n 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}"},"metadata":{}}]},{"cell_type":"code","source":"dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True)\ncolumns = [\"input_ids\", \"labels\", \"attention_mask\"]\ndataset_samsum_pt.set_format(type=\"torch\", columns=columns)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T18:13:34.723270Z","iopub.execute_input":"2024-05-12T18:13:34.724224Z","iopub.status.idle":"2024-05-12T18:13:40.730724Z","shell.execute_reply.started":"2024-05-12T18:13:34.724191Z","shell.execute_reply":"2024-05-12T18:13:40.729776Z"},"trusted":true},"execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14732 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8191564237704e98b9546b2b35ded5e2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3935: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b3e4b1f04fe4fc48d614b2236aee439"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e8c6130d60d477db900c07d1c20d409"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Data Collator for Seq2Seq \n\n**TODO: reread this, quite unclear for now**\n\nNow, we need to create the data collator. This function is called in the Trainer just\nbefore the batch is fed through the model. In most cases we can use the default collator, which collects all the tensors from the batch and simply stacks them. For the\nsummarization task we need to not only stack the inputs but also prepare the targets\non the decoder side. PEGASUS is an encoder-decoder transformer and thus has the\nclassic seq2seq architecture. In a seq2seq setup, a common approach is to apply\n“teacher forcing” in the decoder. With this strategy, the decoder receives input tokens\n(like in decoder-only models such as GPT-2) that consists of the labels shifted by one\nin addition to the encoder output; so, when making the prediction for the next token\nthe decoder gets the ground truth shifted by one as an input, as illustrated in the following table:\n\ndecoder_input label\n\nstep\n\n1 [PAD] Transformers\n\n2 [PAD, Transformers] are\n\n3 [PAD, Transformers, are] awesome\n\n4 [PAD, Transformers, are, awesome] for\n\n5 [PAD, Transformers, are, awesome, for] text\n\n6 [PAD, Transformers, are, awesome, for, text] summarization\n\n\nWe shift it by one so that the decoder only sees the previous ground truth labels and\nnot the current or future ones. Shifting alone suffices since the decoder has masked\nself-attention that masks all inputs at present and in the future.\n\nSo, when we prepare our batch, we set up the decoder inputs by shifting the labels to\nthe right by one. After that, we make sure the padding tokens in the labels are ignored\nby the loss function by setting them to –100. We actually don’t have to do this man‐\nually, though, since the DataCollatorForSeq2Seq comes to the rescue and takes care\nof all these steps for us:","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\nseq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T18:18:51.100082Z","iopub.execute_input":"2024-05-12T18:18:51.100880Z","iopub.status.idle":"2024-05-12T18:18:51.105480Z","shell.execute_reply.started":"2024-05-12T18:18:51.100847Z","shell.execute_reply":"2024-05-12T18:18:51.104572Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\ntraining_args = TrainingArguments(\n    output_dir='pegasus-samsum-nlp-with-transformers-ch06',\n    num_train_epochs=1,\n    warmup_steps=500,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    weight_decay=0.01,\n    logging_steps=10,\n    push_to_hub=True,\n    evaluation_strategy='steps',\n    eval_steps=500,\n    save_steps=1e6,\n    gradient_accumulation_steps=16,\n    report_to=\"none\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T18:20:21.847705Z","iopub.execute_input":"2024-05-12T18:20:21.848421Z","iopub.status.idle":"2024-05-12T18:20:21.895084Z","shell.execute_reply.started":"2024-05-12T18:20:21.848384Z","shell.execute_reply":"2024-05-12T18:20:21.894203Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"**Note: first time using gradient accumulation**\n\nSince model is big we set batchsize = 1.\n\nBut small batch sizes can hurt convergence of model.\n\nSo we use Gradient Accumulation (note: the explanation in book is really unclear: \n\nAs the name suggests, instead of calculating *the gradients of the full batch all at once* (**I think what they mean here is \"instead of calculating the gradients of a VERY LARGE BATCH\"**), we make smaller\nbatches and aggregate the gradients. When we have aggregated enough gradients, we\nrun the optimization step. Naturally this is a bit slower than doing it in one pass, but\nit saves us a lot of GPU memory.","metadata":{}},{"cell_type":"code","source":"# will save to HF O_o might reuse model later\n\nfrom huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T18:24:06.953093Z","iopub.execute_input":"2024-05-12T18:24:06.953521Z","iopub.status.idle":"2024-05-12T18:24:06.982350Z","shell.execute_reply.started":"2024-05-12T18:24:06.953490Z","shell.execute_reply":"2024-05-12T18:24:06.981474Z"},"trusted":true},"execution_count":53,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a62e946fccb1415aa0320c585b362ea8"}},"metadata":{}}]},{"cell_type":"code","source":"trainer = Trainer(model=model,\n                  args=training_args,\n                  tokenizer=tokenizer,\n                  data_collator=seq2seq_data_collator,\n                  train_dataset=dataset_samsum_pt[\"train\"],\n                  eval_dataset=dataset_samsum_pt[\"validation\"],\n                 )","metadata":{"execution":{"iopub.status.busy":"2024-05-12T18:24:59.910766Z","iopub.execute_input":"2024-05-12T18:24:59.911786Z","iopub.status.idle":"2024-05-12T18:25:00.419744Z","shell.execute_reply.started":"2024-05-12T18:24:59.911744Z","shell.execute_reply":"2024-05-12T18:25:00.418941Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# -- TRAINING --\ntrainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T18:25:44.851428Z","iopub.execute_input":"2024-05-12T18:25:44.853423Z","iopub.status.idle":"2024-05-12T19:00:23.277896Z","shell.execute_reply.started":"2024-05-12T18:25:44.853376Z","shell.execute_reply":"2024-05-12T19:00:23.276971Z"},"trusted":true},"execution_count":55,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [920/920 34:34, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.659200</td>\n      <td>1.483876</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=920, training_loss=1.8168589343195376, metrics={'train_runtime': 2077.4559, 'train_samples_per_second': 7.091, 'train_steps_per_second': 0.443, 'total_flos': 5528248038285312.0, 'train_loss': 1.8168589343195376, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## NOTE - trainer.model below\n\nI had not seen that before - I was wondering about this when figuring out the `trainer.predict()` stuff : maybe this is better way of \"getting the model\" if you have the trainer already loaded??\n\nLots of this stuff is so underdocumented O_o","metadata":{}},{"cell_type":"code","source":"# -- EVALUATION ON TEST SET --\nscore = evaluate_summaries_pegasus(dataset_samsum[\"test\"],\n                                   rouge_metric,\n                                   trainer.model,\n                                   tokenizer,\n                                   batch_size=2,\n                                   column_text=\"dialogue\",\n                                   column_summary=\"summary\")\n\n#rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\nrouge_dict = dict((rn, score[rn]) for rn in rouge_names)\n\npd.DataFrame(rouge_dict, index=[f\"pegasus\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-12T19:03:14.000831Z","iopub.execute_input":"2024-05-12T19:03:14.001239Z","iopub.status.idle":"2024-05-12T19:12:40.368641Z","shell.execute_reply.started":"2024-05-12T19:03:14.001209Z","shell.execute_reply":"2024-05-12T19:12:40.367663Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stderr","text":"100%|██████████| 410/410 [09:26<00:00,  1.38s/it]\n","output_type":"stream"},{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"           rouge1    rouge2    rougeL  rougeLsum\npegasus  0.555556  0.230769  0.518519   0.518519","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>pegasus</th>\n      <td>0.555556</td>\n      <td>0.230769</td>\n      <td>0.518519</td>\n      <td>0.518519</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"For some reason the scores here are much better than ones in book O_o maybe optimizations since 2020 of the seq2seq automodels ??\n\n---\n\n## Note on generative models p162 book\n\nYou can also evaluate the generations as part of the training loop:\nuse the extension of TrainingArguments called Seq2SeqTraining\nArguments and specify predict_with_generate=True. Pass it to\nthe dedicated Trainer called Seq2SeqTrainer, which then uses the\ngenerate() function instead of the model’s forward pass to create\npredictions for evaluation. ","metadata":{}},{"cell_type":"code","source":"trainer.push_to_hub(\"Finetune PEGASUS on SAMSum dataset - ch06 Summarization of NLP With Transformers book\")","metadata":{"execution":{"iopub.status.busy":"2024-05-12T19:14:15.885075Z","iopub.execute_input":"2024-05-12T19:14:15.886037Z","iopub.status.idle":"2024-05-12T19:15:22.325673Z","shell.execute_reply.started":"2024-05-12T19:14:15.886004Z","shell.execute_reply":"2024-05-12T19:15:22.324773Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"164612af82b848c188699f1b3f617453"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff217922c709487d9b4cf85295cc1d9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56c41fc6b48541a99ff3af5d111f363c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/4.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27b9d12fbe38466aab4fbecb93fae92f"}},"metadata":{}},{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/benjaminzwhite/pegasus-samsum-nlp-with-transformers-ch06/commit/82da0b593be7601d616b78b7de9de18d69bb4d06', commit_message='Finetune PEGASUS on SAMSum dataset - ch06 Summarization of NLP With Transformers book', commit_description='', oid='82da0b593be7601d616b78b7de9de18d69bb4d06', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Using finetuned model and generating summaries\n\nGenerate a summary from the test set to see quality:","metadata":{}},{"cell_type":"code","source":"gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\nsample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\nreference = dataset_samsum[\"test\"][0][\"summary\"]\npipe = pipeline(\"summarization\", model=\"transformersbook/pegasus-samsum\")\n\nprint(\"Dialogue:\")\nprint(sample_text)\n\nprint(\"\\nReference Summary:\")\nprint(reference)\n\nprint(\"\\nModel Summary:\")\nprint(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-12T19:20:05.395823Z","iopub.execute_input":"2024-05-12T19:20:05.396206Z","iopub.status.idle":"2024-05-12T19:20:34.557240Z","shell.execute_reply.started":"2024-05-12T19:20:05.396180Z","shell.execute_reply":"2024-05-12T19:20:34.556258Z"},"trusted":true},"execution_count":58,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"200d0e29500a42f18d656ab97a016b3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"990bbc8a4b844e308fac3f2f65411a08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31658ea006fa41b2a16e5536646b556a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0631bb21ec5e40f48d231ae65e52c372"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6195ffb4a5bb46a2af1ac5b129cd4fad"}},"metadata":{}},{"name":"stderr","text":"Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n","output_type":"stream"},{"name":"stdout","text":"Dialogue:\nHannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nHannah: <file_gif>\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nHannah: <file_gif>\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him 🙂\nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye\n\nReference Summary:\nHannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n\nModel Summary:\nAmanda can't find Betty's number. Larry called Betty last time they were at the park together. Hannah wants Amanda to text Larry instead of calling Betty.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Above is with the checkpoint from the book - same below but with my personal saved checkpoint:","metadata":{}},{"cell_type":"code","source":"# -- note : using my checkpoint --\n\npipe = pipeline(\"summarization\", model=\"benjaminzwhite/pegasus-samsum-nlp-with-transformers-ch06\")\n\nprint(\"Dialogue:\")\nprint(sample_text)\n\nprint(\"\\nReference Summary:\")\nprint(reference)\n\nprint(\"\\nModel Summary:\")\nprint(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-12T19:22:58.601118Z","iopub.execute_input":"2024-05-12T19:22:58.601747Z","iopub.status.idle":"2024-05-12T19:23:49.164493Z","shell.execute_reply.started":"2024-05-12T19:22:58.601704Z","shell.execute_reply":"2024-05-12T19:23:49.163506Z"},"trusted":true},"execution_count":59,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1042dc25e5d4405f81a0df0f2d1ac27c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a76658caf264e2a811c9762bd895ad6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/275 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ddb54a842ec40abb1243b027df32543"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/20.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35683ad6b70f49ccb73acdb4a47a613e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f968bc1284446cc9ef5e84405ebcd63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/6.60M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5b4ecc2a739412cbeadac1e7b9bcd2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ec9125a983e465986b3be47ed39a4d1"}},"metadata":{}},{"name":"stderr","text":"Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n","output_type":"stream"},{"name":"stdout","text":"Dialogue:\nHannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nHannah: <file_gif>\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nHannah: <file_gif>\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him 🙂\nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye\n\nReference Summary:\nHannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n\nModel Summary:\nAmanda can't find Betty's number. Larry called Betty last time they were at the park together. Hannah wants Amanda to text Larry. Amanda will text Larry.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Just for reference the side by side:\n\n- (HF book checkpoint) Amanda can't find Betty's number. Larry called Betty last time they were at the park together. Hannah wants Amanda to text Larry instead of calling Betty.\n- (my checkpoint) Amanda can't find Betty's number. Larry called Betty last time they were at the park together. Hannah wants Amanda to text Larry. Amanda will text Larry.\n\n2nd seems worse : it is NOT Amanda who will text Larry, but Hannah\n\n---\n\n### test on custom sample from book","metadata":{}},{"cell_type":"code","source":"custom_dialogue = \"\"\"\\\nThom: Hi guys, have you heard of transformers?\nLewis: Yes, I used them recently!\nLeandro: Indeed, there is a great library by Hugging Face.\nThom: I know, I helped build it ;)\nLewis: Cool, maybe we should write a book about it. What do you think?\nLeandro: Great idea, how hard can it be?!\nThom: I am in!\nLewis: Awesome, let's do it together!\n\"\"\"\nprint(pipe(custom_dialogue, **gen_kwargs)[0][\"summary_text\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-12T19:27:35.054672Z","iopub.execute_input":"2024-05-12T19:27:35.055622Z","iopub.status.idle":"2024-05-12T19:27:43.877155Z","shell.execute_reply.started":"2024-05-12T19:27:35.055589Z","shell.execute_reply":"2024-05-12T19:27:43.876183Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stderr","text":"Your max_length is set to 128, but your input_length is only 91. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n","output_type":"stream"},{"name":"stdout","text":"Thom, Lewis, Leandro and Leandro are going to write a book about transformers and Hugging Face. Thom is in for it, Lewis and Leandro are in for it.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**compare the above (my checkpoint) to HF book one:**\n\nThom, Lewis and Leandro are going to write a book about transformers. Thom\nhelped build a library by Hugging Face. They are going to do it together.\n\nIMHO theirs seems better again on this\n\n---\n\n# End of chapter and conclusion\n\nText summarization poses some unique challenges compared to other tasks that can\nbe framed as classification tasks, like sentiment analysis, named entity recognition, or\nquestion answering. Conventional metrics such as accuracy do not reflect the quality\nof the generated text. As we saw, the BLEU and ROUGE metrics can better evaluate\ngenerated texts; however, human judgment remains the best measure.\n\nA common question when working with summarization models is how we can summarize documents where the texts are longer than the model’s context length.\n\nUnfortunately, there is no single strategy to solve this problem, and to date this is still\nan open and active research question. For example, recent work (NOTE BOOK IS 2020) by OpenAI showed\nhow to scale summarization by applying it recursively to long documents and using\nhuman feedback in the loop","metadata":{}}]}