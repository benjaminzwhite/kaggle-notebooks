{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuning Llama 2.0 with Single GPU Magic\n\nFollowing article:\n\n[https://ai.plainenglish.io/fine-tuning-llama2-0-with-qloras-single-gpu-magic-1b6a6679d436](https://ai.plainenglish.io/fine-tuning-llama2-0-with-qloras-single-gpu-magic-1b6a6679d436)","metadata":{}},{"cell_type":"code","source":"!pip install transformers datasets peft accelerate bitsandbytes safetensors","metadata":{"execution":{"iopub.status.busy":"2024-04-16T20:00:59.788382Z","iopub.execute_input":"2024-04-16T20:00:59.788807Z","iopub.status.idle":"2024-04-16T20:01:17.485710Z","shell.execute_reply.started":"2024-04-16T20:00:59.788772Z","shell.execute_reply":"2024-04-16T20:01:17.484810Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.39.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nCollecting peft\n  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.28.0)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (0.4.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.22.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, peft\nSuccessfully installed bitsandbytes-0.43.1 peft-0.10.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import os, sys\nimport torch\nimport datasets\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    DataCollatorForLanguageModeling,\n    DataCollatorForSeq2Seq,\n    Trainer,\n    TrainingArguments,\n    GenerationConfig\n)\nfrom peft import PeftModel, LoraConfig, prepare_model_for_kbit_training, get_peft_model","metadata":{"execution":{"iopub.status.busy":"2024-04-16T20:01:17.487639Z","iopub.execute_input":"2024-04-16T20:01:17.487937Z","iopub.status.idle":"2024-04-16T20:01:36.068171Z","shell.execute_reply.started":"2024-04-16T20:01:17.487911Z","shell.execute_reply":"2024-04-16T20:01:36.067235Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-04-16 20:01:27.206459: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-16 20:01:27.206558: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-16 20:01:27.338739: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model Loading\n\n[https://huggingface.co/NousResearch/Llama-2-7b-hf](https://huggingface.co/NousResearch/Llama-2-7b-hf)\n\n**NOTE: below cells were commented out - kept getting bugs due to accelerate/bitsandbytes library : only fix is that you NEED TO HAVE A GPU!!! I was running on CPU**","metadata":{}},{"cell_type":"code","source":"#!pip install -i https://pypi.org/simple/ bitsandbytes\n\n#!pip install accelerate\n\n#!pip install transformers==4.30\n\n#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118","metadata":{"execution":{"iopub.status.busy":"2024-04-16T20:01:36.069452Z","iopub.execute_input":"2024-04-16T20:01:36.070449Z","iopub.status.idle":"2024-04-16T20:01:36.074546Z","shell.execute_reply.started":"2024-04-16T20:01:36.070413Z","shell.execute_reply":"2024-04-16T20:01:36.073540Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"### config ###\nmodel_id = \"NousResearch/Llama-2-7b-hf\"\nmax_length = 512\ndevice_map = \"auto\"\nbatch_size = 128\nmicro_batch_size = 32\ngradient_accumulation_steps = batch_size // micro_batch_size\n\n# nf4\" use a symmetric quantization scheme with 4 bits precision\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n# load model from huggingface\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    use_cache=False,\n    device_map=device_map\n)\n\n# load tokenizer from huggingface\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-04-16T20:01:36.076755Z","iopub.execute_input":"2024-04-16T20:01:36.077102Z","iopub.status.idle":"2024-04-16T20:02:42.803238Z","shell.execute_reply.started":"2024-04-16T20:01:36.077071Z","shell.execute_reply":"2024-04-16T20:02:42.802120Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bd7c604b921474c84f7cc3e6cd9f390"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4033adf07d04fea91b4ae507781466c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d8a407e42074fd4bdf257702c8fa305"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0fef536c889c433cb7e72010a9e41f5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2415e7d8a8a54ceab1e4d42ade2d65d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c362eb8c14b493288aa826593016d83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c5138a9ea9c493f9a027443f5596974"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edafc9f4a025485ea93b6f0726511001"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec52efd528794330b35185dc126a1260"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88a91f13507a4c65b5e1821a15095f21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4862dcc138d447b9f2bd1cfe2ee813b"}},"metadata":{}}]},{"cell_type":"markdown","source":"Helper function called “print_number_of_trainable_model_parameters” to inspect the trainable parameters of the original model. Upon running this function, it provides us the output trainable model parameter: 262,410,240","metadata":{}},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    print(f\"trainable model parameters: {trainable_model_params}. All model parameters: {all_model_params} \")\n    return trainable_model_params\n\nori_p = print_number_of_trainable_model_parameters(model)\n\nori_p","metadata":{"execution":{"iopub.status.busy":"2024-04-16T20:02:42.804411Z","iopub.execute_input":"2024-04-16T20:02:42.804740Z","iopub.status.idle":"2024-04-16T20:02:42.815962Z","shell.execute_reply.started":"2024-04-16T20:02:42.804714Z","shell.execute_reply":"2024-04-16T20:02:42.815100Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"trainable model parameters: 262410240. All model parameters: 3500412928 \n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"262410240"},"metadata":{}}]},{"cell_type":"markdown","source":"Next, we can start packing the model into the LoRA format while keeping the original parameters frozen and introducing additional weights as discussed earlier. The LoRA model has several configurable parameters:\n\n* rdetermines the rank of the update matrices, also known as Lora attention dimension. Lower rank results in smaller update matrices with fewer trainable parameters. Increasing r (not more than 32) will lead to more robust model but higher memory consumption at the same time.\n* lora_alpha controls the LoRA scaling factor\n* target_modules is a list of module names, such as “q_proj” and “v_proj,” which serves as the targets for the LoRA model. The specific module names may vary depending on the underlying model.\n* bias: Specifies if the bias parameters should be trained. Can be 'none', 'all' or 'lora_only'.\n\nAfter attaching model with the LoRA adapter, let’s print the trainable parameters again and compare them to the original model. Remarkably, the trainable model parameter: 4,194,304is now represent only less than 2% of the original model’s size.","metadata":{}},{"cell_type":"code","source":"# LoRA config\nmodel = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\nmodel = get_peft_model(model, peft_config)\n\n### compare trainable parameters #\npeft_p = print_number_of_trainable_model_parameters(model)\nprint(f\"# Trainable Parameter \\nBefore: {ori_p} \\nAfter: {peft_p} \\nPercentage: {round(peft_p / ori_p * 100, 2)}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-16T20:22:44.841231Z","iopub.execute_input":"2024-04-16T20:22:44.841606Z","iopub.status.idle":"2024-04-16T20:22:45.000722Z","shell.execute_reply.started":"2024-04-16T20:22:44.841564Z","shell.execute_reply":"2024-04-16T20:22:44.999844Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"trainable model parameters: 4194304. All model parameters: 3504607232 \n# Trainable Parameter \nBefore: 262410240 \nAfter: 4194304 \nPercentage: 1.6\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Test before finetuning\n\nJust before the thrilling fine-tuning process, let’ not skip the process of generating an output from a pre-trained language model and observe its response. In this case, when asking the model to write a poem about Singapore, the generated output appears to be quite vague and repetitive, indicating that the model struggles to provide a coherent and meaningful response.","metadata":{}},{"cell_type":"code","source":"### generate ###\nprompt = \"Write me a poem about Singapore.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\ngenerate_ids = model.generate(inputs.input_ids, max_length=64)\nprint('\\nAnswer: ', tokenizer.decode(generate_ids[0]))\nres = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\nprint(res)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T20:26:09.203351Z","iopub.execute_input":"2024-04-16T20:26:09.204132Z","iopub.status.idle":"2024-04-16T20:26:14.169788Z","shell.execute_reply.started":"2024-04-16T20:26:09.204101Z","shell.execute_reply":"2024-04-16T20:26:14.168810Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\nAnswer:  <s> Write me a poem about Singapore. nobody can write a poem about Singapore.\nI'm not sure if I'm allowed to write a poem about Singapore.\nI'm not sure if I'm allowed to write a poem about Singapore. I'm not sure if I'm allowed to write\nWrite me a poem about Singapore. nobody can write a poem about Singapore.\nI'm not sure if I'm allowed to write a poem about Singapore.\nI'm not sure if I'm allowed to write a poem about Singapore. I'm not sure if I'm allowed to write\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Note: when I run the above I get Warning saying input_ids are not on same device as model**","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-04-16T20:27:28.506478Z","iopub.execute_input":"2024-04-16T20:27:28.507298Z","iopub.status.idle":"2024-04-16T20:27:28.512986Z","shell.execute_reply.started":"2024-04-16T20:27:28.507268Z","shell.execute_reply":"2024-04-16T20:27:28.512182Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"### generate ###\nprompt = \"Write me a poem about Singapore.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\")\n\n# my code:\ninputs = inputs.to(device)\n\ngenerate_ids = model.generate(inputs.input_ids, max_length=64)\nprint('\\nAnswer: ', tokenizer.decode(generate_ids[0]))\nres = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\nprint(res)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T20:28:24.586888Z","iopub.execute_input":"2024-04-16T20:28:24.587625Z","iopub.status.idle":"2024-04-16T20:28:30.802465Z","shell.execute_reply.started":"2024-04-16T20:28:24.587572Z","shell.execute_reply":"2024-04-16T20:28:30.801562Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\nAnswer:  <s> Write me a poem about Singapore. nobody can write a poem about Singapore.\nI'm not sure if I'm allowed to write a poem about Singapore.\nI'm not sure if I'm allowed to write a poem about Singapore. I'm not sure if I'm allowed to write\nWrite me a poem about Singapore. nobody can write a poem about Singapore.\nI'm not sure if I'm allowed to write a poem about Singapore.\nI'm not sure if I'm allowed to write a poem about Singapore. I'm not sure if I'm allowed to write\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Results aren't great - same as him on article they are incoherent also OK","metadata":{}},{"cell_type":"markdown","source":"# Data Loading\n\nTo demonstate the process of fine-tuning an Instruction-LLM, we are going to use a public dataset sourced from databricks/databricks-dolly-15k which presents an array of instruction-response pairs. Notably, certain samples in this dataset also incorporate contextual information, adding an extra layer of complexity and richness to the model’s comprehension process. Allow me to present a captivating sample record extracted from this intriguing raw data:\n\n```\n{\n    'instruction': 'Why can camels survive for long without water?',\n    'context': '',\n    'response': 'Camels use the fat in their humps to keep them filled with energy and hydration for long periods of time.',\n    'category': 'open_qa',\n}\n```\n\nA prompt_template is created to enhance the learning capabilities of the model. This ingenious template consists of two distinct types: prompt_input and prompt_no_input. The former is employed for samples that encompass an input context, while the latter caters to instances lacking such contextual information. By precisely pairing each task’s instruction with the appropriate context (if available), we foster a deeper understanding and context-awareness within the model.","metadata":{}},{"cell_type":"code","source":"max_length = 256\ndataset = datasets.load_dataset(\n    \"databricks/databricks-dolly-15k\", split='train'\n)\n\n### generate prompt based on template ###\nprompt_template = {\n    \"prompt_input\": \\\n    \"Below is an instruction that describes a task, paired with an input that provides further context.\\\n    Write a response that appropriately completes the request.\\\n    \\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\",\n\n    \"prompt_no_input\": \\\n    \"Below is an instruction that describes a task.\\\n    Write a response that appropriately completes the request.\\\n    \\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\",\n\n    \"response_split\": \"### Response:\"\n}\n\ndef generate_prompt(instruction, input=None, label=None, prompt_template=prompt_template):\n    if input:\n        res = prompt_template[\"prompt_input\"].format(\n            instruction=instruction, input=input)\n    else:\n        res = prompt_template[\"prompt_no_input\"].format(\n            instruction=instruction)\n    if label:\n        res = f\"{res}{label}\"\n    return res","metadata":{"execution":{"iopub.status.busy":"2024-04-16T20:54:02.221229Z","iopub.execute_input":"2024-04-16T20:54:02.221627Z","iopub.status.idle":"2024-04-16T20:54:05.015693Z","shell.execute_reply.started":"2024-04-16T20:54:02.221585Z","shell.execute_reply":"2024-04-16T20:54:05.014787Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/8.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"839ec8051ebe425da253db1250b22c99"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 13.1M/13.1M [00:00<00:00, 50.4MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9d023bc77e74151ad4e8c4aa90f859d"}},"metadata":{}}]},{"cell_type":"markdown","source":"We first generate the full prompt by combining the instruction, context, and response using the generate_prompt function. Once the full prompt is crafted, we tokenize it using the provided tokenizer, which transforms the text into input_ids and attention_mask. Notably, to train the model to predict the next word, we designate the labelsimilar to the input_idsand facilitate a shift-right operation by the trainer. However, to avoid the model focusing on the next word in the instruction and context, we shall mask all the original tokens in these segments, replacing them with -100, while retaining only the response input. The data is further organized into training and validation sets, and unnecessary columns are removed, thus culminating in a refined and highly effective dataset poised for training.","metadata":{}},{"cell_type":"code","source":"def tokenize(tokenizer, prompt, max_length=max_length, add_eos_token=False):\n    result = tokenizer(\n        prompt,\n        truncation=True,\n        max_length=max_length,\n        padding=False,\n        return_tensors=None)\n\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result\n\ndef generate_and_tokenize_prompt(data_point):\n    full_prompt = generate_prompt(\n        data_point[\"instruction\"],\n        data_point[\"context\"],\n        data_point[\"response\"],\n    )\n    tokenized_full_prompt = tokenize(tokenizer, full_prompt)\n    user_prompt = generate_prompt(data_point[\"instruction\"], data_point[\"context\"])\n    tokenized_user_prompt = tokenize(tokenizer, user_prompt)\n    user_prompt_len = len(tokenized_user_prompt[\"input_ids\"])\n    mask_token = [-100] * user_prompt_len\n    tokenized_full_prompt[\"labels\"] = mask_token + tokenized_full_prompt[\"labels\"][user_prompt_len:]\n    return tokenized_full_prompt\n\ndataset = dataset.train_test_split(test_size=1000, shuffle=True, seed=42)\ncols = [\"instruction\", \"context\", \"response\", \"category\"]\ntrain_data = dataset[\"train\"].shuffle().map(generate_and_tokenize_prompt, remove_columns=cols)\nval_data = dataset[\"test\"].shuffle().map(generate_and_tokenize_prompt, remove_columns=cols,)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T20:59:26.916359Z","iopub.execute_input":"2024-04-16T20:59:26.917202Z","iopub.status.idle":"2024-04-16T20:59:48.867491Z","shell.execute_reply.started":"2024-04-16T20:59:26.917170Z","shell.execute_reply":"2024-04-16T20:59:48.866587Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14011 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e1d764cef6543b3b69b3b71a86455c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"889442473cf740559d9b6d65851b3229"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Model Training\n\nWith extensive preparation of the data and model, the moment has come to initiate the training process. With the flexibility to fine-tune the trainer’s settings as needed, I run 200 steps on a crucial five-hour training session, with Google Colab.","metadata":{}},{"cell_type":"code","source":"# changed params compared to his : num_train_epochs 20 -> 1\n# save_total_limit 3 -> 1\n\nargs = TrainingArguments(\n    output_dir=\"./llama-7b-int4-dolly\",\n    num_train_epochs=1,\n    max_steps=200,\n    fp16=True,\n    optim=\"paged_adamw_32bit\",\n    learning_rate=2e-4,\n    lr_scheduler_type=\"constant\",\n    per_device_train_batch_size=micro_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    gradient_checkpointing=True,\n    group_by_length=False,\n    logging_steps=10,\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    disable_tqdm=False,\n    report_to=[\"tensorboard\"],\n)\n\ntrainer = Trainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=val_data,\n    args=args,\n    data_collator=DataCollatorForSeq2Seq(\n      tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True),\n)\n\n# silence the warnings. re-enable for inference!\nmodel.config.use_cache = False\ntrainer.train()\nmodel.save_pretrained(\"llama-7b-int4-dolly\")","metadata":{"execution":{"iopub.status.busy":"2024-04-16T22:09:29.783874Z","iopub.status.idle":"2024-04-16T22:09:29.784331Z","shell.execute_reply.started":"2024-04-16T22:09:29.784096Z","shell.execute_reply":"2024-04-16T22:09:29.784124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generation\n\nAfter several hours of training with a single GPU, it’s time to test the model’s performance using the input prompt “Write me a poem about Singapore” which we previously used. The code snippet starts by loading the pre-trained Llama-2–7b-hf model and Peft weights. The model’s generation configuration is set to control factors such as\n\n* temperature controls the randomness of the generation process. When the temperature is high, the generator is more random and generates diverse but less coherent outputs. When the temperature is low, the generator is less random and generates more coherent but less diverse outputs.\n* top-p select the most promising candidates from a set of generated options. The “p” in top-p stands for “probability,” and it refers to the probability of a given candidate being the best option.\n* top-k is similar to top-p, but instead of selecting a percentage of candidates, it selects a fixed number of candidates with the highest probability scores.\n* num_beam in Beam search algorithm that allows the model to consider multiple possible outputs simultaneously. It works by maintaining a set of possible outputs, called the “beam,” and iteratively expanding the beam by adding new outputs that are likely to be correct.","metadata":{}},{"cell_type":"code","source":"# model path and weight\nmodel_id = \"NousResearch/Llama-2-7b-hf\"\npeft_path = \"./llama-7b-int4-dolly\"\n\n# loading model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,\n    use_cache=False,\n    device_map=\"auto\"\n)\n\n# loading peft weight\nmodel = PeftModel.from_pretrained(\n    model,\n    peft_path,\n    torch_dtype=torch.float16,\n)\nmodel.eval()\n\n# generation config\ngeneration_config = GenerationConfig(\n    temperature=0.1,\n    top_p=0.75,\n    top_k=40,\n    num_beams=4, # beam search\n)\n\n# generating reply\nwith torch.no_grad():\n    prompt = \"Write me a poem about Singapore.\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    generation_output = model.generate(\n        input_ids=inputs.input_ids,\n        generation_config=generation_config,\n        return_dict_in_generate=True,\n        output_scores=True,\n        max_new_tokens=64,\n    )\n    print('\\nAnswer: ', tokenizer.decode(generation_output.sequences[0]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model’s output is now showing promising improvements compared to the pre-trained model. Although the result might not meet our high poetic expectations, it’s essential to consider that we employed the smallest available 7b-Llama2.0 model and trained only on a limited weight using Lora for a short period. Nevertheless, this outcome is already impressive, considering the constraints.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}