{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install unsloth\n\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2024-10-13T16:32:39.783000Z","iopub.execute_input":"2024-10-13T16:32:39.783400Z","iopub.status.idle":"2024-10-13T16:35:49.908403Z","shell.execute_reply.started":"2024-10-13T16:32:39.783361Z","shell.execute_reply":"2024-10-13T16:35:49.906986Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n    \"unsloth/Phi-3-mini-4k-instruct\",          # Phi-3 2x faster!d\n    \"unsloth/Phi-3-medium-4k-instruct\",\n    \"unsloth/gemma-2-9b-bnb-4bit\",\n    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T16:36:55.847498Z","iopub.execute_input":"2024-10-13T16:36:55.847970Z","iopub.status.idle":"2024-10-13T16:37:51.296984Z","shell.execute_reply.started":"2024-10-13T16:36:55.847929Z","shell.execute_reply":"2024-10-13T16:37:51.295835Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"name":"stdout","text":"==((====))==  Unsloth 2024.9.post4: Fast Llama patching. Transformers = 4.44.2.\n   \\\\   /|    GPU: Tesla P100-PCIE-16GB. Max memory: 15.888 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.4.1+cu121. CUDA = 6.0. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post1. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31a96b40ac5944a8aad0ed47d103fdda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba976e0b0bf14519a494b66e20444961"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39158abd54a2498ba07a29756217b1ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a072e601f6bf403a8d21fa902e7b4278"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/345 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33f53755fed248e9bc695fcd5d15c972"}},"metadata":{}}]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:34:37.421869Z","iopub.execute_input":"2024-10-13T17:34:37.422349Z","iopub.status.idle":"2024-10-13T17:34:37.431732Z","shell.execute_reply.started":"2024-10-13T17:34:37.422309Z","shell.execute_reply":"2024-10-13T17:34:37.430773Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"Unsloth: Already have LoRA adapters! We shall skip this step.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Data prep","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nabsa_quad = load_dataset(\"NEUDM/absa-quad\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T16:39:23.366894Z","iopub.execute_input":"2024-10-13T16:39:23.367751Z","iopub.status.idle":"2024-10-13T16:39:26.251067Z","shell.execute_reply.started":"2024-10-13T16:39:23.367710Z","shell.execute_reply":"2024-10-13T16:39:26.250092Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/2.83k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"221e284af45c4ec4b54a3cc1af8a0536"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generation/train.jsonl:   0%|          | 0.00/2.00M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19ddd2f7a907488f80899a91e4315c69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generation/dev.jsonl:   0%|          | 0.00/503k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eca497bb580548e291cb4525c6f6d1c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generation/test.jsonl:   0%|          | 0.00/1.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fc2adfc6af64b7db55a39864d444254"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2098 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7dac19b08924ef6b488273436888ca8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/525 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f367695f4f74ee7ad1437d347170bc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1081 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec834bf7b6ce4ba39f99dbb0f0cd6ce6"}},"metadata":{}}]},{"cell_type":"code","source":"import ast\nimport json","metadata":{"execution":{"iopub.status.busy":"2024-10-13T16:58:54.333575Z","iopub.execute_input":"2024-10-13T16:58:54.334342Z","iopub.status.idle":"2024-10-13T16:58:54.338296Z","shell.execute_reply.started":"2024-10-13T16:58:54.334302Z","shell.execute_reply":"2024-10-13T16:58:54.337316Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:05:24.695635Z","iopub.execute_input":"2024-10-13T17:05:24.696341Z","iopub.status.idle":"2024-10-13T17:05:24.700256Z","shell.execute_reply.started":"2024-10-13T17:05:24.696301Z","shell.execute_reply":"2024-10-13T17:05:24.699376Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"llama3_train_dataset = []\n\n\nfor example in absa_quad[\"train\"].iter(batch_size=1):\n    # get the raw text\n    example_text = example[\"input\"][0]\n    #print(example_text)\n\n    input_text = ast.literal_eval(example_text)[0]\n    \n    gold_labels = example[\"output\"][0]\n    \n    #print(gold_labels)\n    golds = ast.literal_eval(gold_labels)\n    tmp_list_of_quads = []\n    for quad in golds:\n        #print(quad)\n        tmp_d = {}\n        tmp_d[\"opinion term\"] = quad[0]\n        tmp_d[\"aspect category\"] = quad[1]\n        tmp_d[\"sentiment\"] = quad[2]\n        tmp_d[\"justification\"] = quad[3]\n        \n        # convert to json representation for making the prompt\n        \n        tmp_list_of_quads.append(json.dumps(tmp_d))\n    \n    # create the string representation of the gold answer\n    # UNSLOTH SAYS ADD EOS_TOKEN MANUALLY\n    gold_answer = '{\\\"aspect_based_sentiment_analysis\\\": ' + '[' + ', '.join(tmp_list_of_quads) + ']' + ' }' + EOS_TOKEN\n    \n    llama3template = f\"\"\"Below is a DOCUMENT in which human beings may be expressing themselves about products or services.\n\nPerform a full aspect-based sentiment analysis of the DOCUMENT.\n    \nOnly use sentiment labels that appear in the below list of ALLOWED SENTIMENTS.\n    \nOnly use aspect category labels that appear in the below list of ALLOWED ASPECT CATEGORIES.\n    \nUse the exact words and spelling found in the DOCUMENT without modification.\n\nReturn your answer as a structured JSON object without deviation.\n\n### DOCUMENT:\n\n{input_text}\n\n### ALLOWED SENTIMENTS:\n\n- positive\n- negative\n- neutral\n\n### ALLOWED ASPECT CATEGORIES:\n\n- food quality\n- service general\n- restaurant general\n- ambience general\n- food style_options\n- restaurant miscellaneous\n- food prices\n- restaurant prices\n- drinks quality\n- drinks style_options\n- location general\n- drinks prices\n- food general\n\n### RESPONSE:\n\n{gold_answer}\"\"\"\n    \n    llama3_train_dataset.append(llama3template)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:34:44.487140Z","iopub.execute_input":"2024-10-13T17:34:44.487544Z","iopub.status.idle":"2024-10-13T17:34:44.807788Z","shell.execute_reply.started":"2024-10-13T17:34:44.487506Z","shell.execute_reply":"2024-10-13T17:34:44.806961Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"print(llama3_train_dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:34:50.351972Z","iopub.execute_input":"2024-10-13T17:34:50.352389Z","iopub.status.idle":"2024-10-13T17:34:50.357506Z","shell.execute_reply.started":"2024-10-13T17:34:50.352351Z","shell.execute_reply":"2024-10-13T17:34:50.356551Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Below is a DOCUMENT in which human beings may be expressing themselves about products or services.\n\nPerform a full aspect-based sentiment analysis of the DOCUMENT.\n    \nOnly use sentiment labels that appear in the below list of ALLOWED SENTIMENTS.\n    \nOnly use aspect category labels that appear in the below list of ALLOWED ASPECT CATEGORIES.\n    \nUse the exact words and spelling found in the DOCUMENT without modification.\n\nReturn your answer as a structured JSON object without deviation.\n\n### DOCUMENT:\n\nThe wait here is long for dim sum , but if you do n't like sharing tables or if the typical raucous dim sum atmosphere is not your gig , this is a sleek ( for Chinatown ) alternative .\n\n### ALLOWED SENTIMENTS:\n\n- positive\n- negative\n- neutral\n\n### ALLOWED ASPECT CATEGORIES:\n\n- food quality\n- service general\n- restaurant general\n- ambience general\n- food style_options\n- restaurant miscellaneous\n- food prices\n- restaurant prices\n- drinks quality\n- drinks style_options\n- location general\n- drinks prices\n- food general\n\n### RESPONSE:\n\n{\"aspect_based_sentiment_analysis\": [{\"opinion term\": \"wait\", \"aspect category\": \"service general\", \"sentiment\": \"negative\", \"justification\": \"long\"}, {\"opinion term\": \"atmosphere\", \"aspect category\": \"ambience general\", \"sentiment\": \"negative\", \"justification\": \"raucous\"}, {\"opinion term\": \"NULL\", \"aspect category\": \"restaurant miscellaneous\", \"sentiment\": \"negative\", \"justification\": \"sleek\"}] }<|end_of_text|>\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\n# convert to HF dataset\ndf = pd.DataFrame(llama3_train_dataset)\ntrain_data = Dataset.from_pandas(df.rename(columns={0: \"text\"}), split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:34:55.640126Z","iopub.execute_input":"2024-10-13T17:34:55.640526Z","iopub.status.idle":"2024-10-13T17:34:55.667348Z","shell.execute_reply.started":"2024-10-13T17:34:55.640488Z","shell.execute_reply":"2024-10-13T17:34:55.666427Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"df.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:10:46.759844Z","iopub.execute_input":"2024-10-13T17:10:46.760255Z","iopub.status.idle":"2024-10-13T17:10:46.772589Z","shell.execute_reply.started":"2024-10-13T17:10:46.760219Z","shell.execute_reply":"2024-10-13T17:10:46.771594Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"                                                   0\n0  Below is a DOCUMENT in which human beings may ...\n1  Below is a DOCUMENT in which human beings may ...\n2  Below is a DOCUMENT in which human beings may ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Below is a DOCUMENT in which human beings may ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Below is a DOCUMENT in which human beings may ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Below is a DOCUMENT in which human beings may ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_data[\"text\"][0]","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:11:10.680454Z","iopub.execute_input":"2024-10-13T17:11:10.680840Z","iopub.status.idle":"2024-10-13T17:11:10.692152Z","shell.execute_reply.started":"2024-10-13T17:11:10.680803Z","shell.execute_reply":"2024-10-13T17:11:10.691258Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"'Below is a DOCUMENT in which human beings may be expressing themselves about products or services.\\n\\nPerform a full aspect-based sentiment analysis of the DOCUMENT.\\n    \\nOnly use sentiment labels that appear in the below list of ALLOWED SENTIMENTS.\\n    \\nOnly use aspect category labels that appear in the below list of ALLOWED ASPECT CATEGORIES.\\n    \\nUse the exact words and spelling found in the DOCUMENT without modification.\\n\\nReturn your answer as a structured JSON object without deviation.\\n\\n### DOCUMENT:\\n\\nThe wait here is long for dim sum , but if you do n\\'t like sharing tables or if the typical raucous dim sum atmosphere is not your gig , this is a sleek ( for Chinatown ) alternative .\\n\\n### ALLOWED SENTIMENTS:\\n\\n- positive\\n- negative\\n- neutral\\n\\n### ALLOWED ASPECT CATEGORIES:\\n\\n- food quality\\n- service general\\n- restaurant general\\n- ambience general\\n- food style_options\\n- restaurant miscellaneous\\n- food prices\\n- restaurant prices\\n- drinks quality\\n- drinks style_options\\n- location general\\n- drinks prices\\n- food general\\n\\n### RESPONSE:\\n\\n{\"aspect_based_sentiment_analysis\": [{\"opinion term\": \"wait\", \"aspect category\": \"service general\", \"sentiment\": \"negative\", \"justification\": \"long\"}, {\"opinion term\": \"atmosphere\", \"aspect category\": \"ambience general\", \"sentiment\": \"negative\", \"justification\": \"raucous\"}, {\"opinion term\": \"NULL\", \"aspect category\": \"restaurant miscellaneous\", \"sentiment\": \"negative\", \"justification\": \"sleek\"}] }<|end_of_text|>'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Trying to do the train on completions only\n\nnot very clear how this works as of 2024, examples seem old","metadata":{}},{"cell_type":"code","source":"from trl import DataCollatorForCompletionOnlyLM","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:09:12.640864Z","iopub.execute_input":"2024-10-13T17:09:12.641294Z","iopub.status.idle":"2024-10-13T17:09:12.646180Z","shell.execute_reply.started":"2024-10-13T17:09:12.641258Z","shell.execute_reply":"2024-10-13T17:09:12.645071Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# doesnt seem to work\n# - was getting 0 loss during training even after 1 step\n\n#response_template = \"### RESPONSE:\"\n#completion_only_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:09:47.649840Z","iopub.execute_input":"2024-10-13T17:09:47.650240Z","iopub.status.idle":"2024-10-13T17:09:47.655092Z","shell.execute_reply.started":"2024-10-13T17:09:47.650206Z","shell.execute_reply":"2024-10-13T17:09:47.654142Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"# back to Unsloth main part\n\n- note: trying myself with the `completion_only_collator` stuff added to the `SFTTrainer`","metadata":{}},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_data,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    #data_collator=completion_only_collator, # NOTE -- I ADDED THIS FOR THE COMPLETIONS ONLY STUFF\n    args = TrainingArguments(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 4,\n        \n        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n        warmup_steps = 5,\n        max_steps = 60,\n\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:35:01.081695Z","iopub.execute_input":"2024-10-13T17:35:01.082512Z","iopub.status.idle":"2024-10-13T17:35:03.371345Z","shell.execute_reply.started":"2024-10-13T17:35:01.082470Z","shell.execute_reply":"2024-10-13T17:35:03.370303Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/2098 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abd43c45b78b41e7a9c126ff94de0ca2"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"code","source":"gpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:35:10.104429Z","iopub.execute_input":"2024-10-13T17:35:10.105276Z","iopub.status.idle":"2024-10-13T17:35:10.112088Z","shell.execute_reply.started":"2024-10-13T17:35:10.105228Z","shell.execute_reply":"2024-10-13T17:35:10.111092Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"GPU = Tesla P100-PCIE-16GB. Max memory = 15.888 GB.\n6.877 GB of memory reserved.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# go\n\nlets-a-goooooooo","metadata":{}},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:35:13.951866Z","iopub.execute_input":"2024-10-13T17:35:13.952776Z","iopub.status.idle":"2024-10-13T17:52:24.046163Z","shell.execute_reply.started":"2024-10-13T17:35:13.952735Z","shell.execute_reply":"2024-10-13T17:52:24.045155Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 2,098 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 60\n \"-____-\"     Number of trainable parameters = 41,943,040\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 16:50, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.210100</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2.118000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>2.067600</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.906900</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.773400</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.582800</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.222700</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.922000</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.707100</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.552600</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.457900</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.413200</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.360000</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.289400</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.287000</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.268200</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.185400</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.221600</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.288200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.215500</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.230100</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.256100</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.275500</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.266700</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.227600</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.198700</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.341100</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.226900</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.201500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.208100</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.240000</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.263300</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.208700</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.196700</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.210200</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.217100</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.228400</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.243100</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.213000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.230200</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.152200</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.161300</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.310300</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.242400</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.252700</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.180400</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.170900</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.229000</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.238000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.186000</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.213200</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.192800</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.137200</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.163400</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.179100</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.203100</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.175500</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.259100</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.175000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.252500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory         /max_memory*100, 3)\nlora_percentage = round(used_memory_for_lora/max_memory*100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T17:59:58.687630Z","iopub.execute_input":"2024-10-13T17:59:58.688476Z","iopub.status.idle":"2024-10-13T17:59:58.702152Z","shell.execute_reply.started":"2024-10-13T17:59:58.688432Z","shell.execute_reply":"2024-10-13T17:59:58.700963Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"1026.745 seconds used for training.\n17.11 minutes used for training.\nPeak reserved memory = 6.975 GB.\nPeak reserved memory for training = 0.098 GB.\nPeak reserved memory % of max memory = 43.901 %.\nPeak reserved memory for training % of max memory = 0.617 %.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"inference_llama3template = f\"\"\"Below is a DOCUMENT in which human beings may be expressing themselves about products or services.\n\nPerform a full aspect-based sentiment analysis of the DOCUMENT.\n    \nOnly use sentiment labels that appear in the below list of ALLOWED SENTIMENTS.\n    \nOnly use aspect category labels that appear in the below list of ALLOWED ASPECT CATEGORIES.\n    \nUse the exact words and spelling found in the DOCUMENT without modification.\n\nReturn your answer as a structured JSON object without deviation.\n\n### DOCUMENT:\n\n{query}\n\n### ALLOWED SENTIMENTS:\n\n- positive\n- negative\n- neutral\n\n### ALLOWED ASPECT CATEGORIES:\n\n- food quality\n- service general\n- restaurant general\n- ambience general\n- food style_options\n- restaurant miscellaneous\n- food prices\n- restaurant prices\n- drinks quality\n- drinks style_options\n- location general\n- drinks prices\n- food general\n\n### RESPONSE:\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-10-13T18:12:54.657976Z","iopub.execute_input":"2024-10-13T18:12:54.658456Z","iopub.status.idle":"2024-10-13T18:12:54.664033Z","shell.execute_reply.started":"2024-10-13T18:12:54.658417Z","shell.execute_reply":"2024-10-13T18:12:54.663034Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"query = \"I hated this place's burgers they are the worst i have ever tasted!\"","metadata":{"execution":{"iopub.status.busy":"2024-10-13T18:12:57.034660Z","iopub.execute_input":"2024-10-13T18:12:57.035576Z","iopub.status.idle":"2024-10-13T18:12:57.039585Z","shell.execute_reply.started":"2024-10-13T18:12:57.035529Z","shell.execute_reply":"2024-10-13T18:12:57.038555Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"# DEBUG TODO\n\n\n- i don't understand : if i use tokenizer(prompt_temoplate.format(document=current_query) or whatever, it seems to never update and use old values/prompts ??!?! that's why i'm manually \"writing\" the prompt below\n\n**UPDATE: i think it's because i was using a f string for the prompt so that variable name within the { } was actually looking earlier in notebook - need to skip the f string if use \"\"\" \"\"\" and then format later**","metadata":{}},{"cell_type":"code","source":"# NOTE -- after debugging: skip the f string here, so variables don't need to be defined\n# (i think before i must have had a variable named query that was going in to my prompt as a F-string, BEFORE the call to .format)\nformatted_prompt = \"\"\"Below is a DOCUMENT in which human beings may be expressing themselves about products or services.\n\nPerform a full aspect-based sentiment analysis of the DOCUMENT.\n    \nOnly use sentiment labels that appear in the below list of ALLOWED SENTIMENTS.\n    \nOnly use aspect category labels that appear in the below list of ALLOWED ASPECT CATEGORIES.\n    \nUse the exact words and spelling found in the DOCUMENT without modification.\n\nReturn your answer as a structured JSON object without deviation.\n\n### DOCUMENT:\n\n{manual_add_query}\n\n### ALLOWED SENTIMENTS:\n\n- positive\n- negative\n- neutral\n\n### ALLOWED ASPECT CATEGORIES:\n\n- food quality\n- service general\n- restaurant general\n- ambience general\n- food style_options\n- restaurant miscellaneous\n- food prices\n- restaurant prices\n- drinks quality\n- drinks style_options\n- location general\n- drinks prices\n- food general\n\n### RESPONSE:\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-10-13T18:14:44.804024Z","iopub.execute_input":"2024-10-13T18:14:44.804431Z","iopub.status.idle":"2024-10-13T18:14:44.809670Z","shell.execute_reply.started":"2024-10-13T18:14:44.804395Z","shell.execute_reply":"2024-10-13T18:14:44.808698Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"current_query=\"I really hated this place's burgers they are the worst i have ever tasted in New York!\"","metadata":{"execution":{"iopub.status.busy":"2024-10-13T18:15:22.926681Z","iopub.execute_input":"2024-10-13T18:15:22.927591Z","iopub.status.idle":"2024-10-13T18:15:22.931604Z","shell.execute_reply.started":"2024-10-13T18:15:22.927549Z","shell.execute_reply":"2024-10-13T18:15:22.930618Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"FastLanguageModel.for_inference(model) # Unsloth has 2x faster inference!\n\ninputs = tokenizer(formatted_prompt.format(manual_add_query=current_query), return_tensors = \"pt\").to(\"cuda\")\n\noutputs = model.generate(input_ids = inputs.input_ids,\n                         attention_mask = inputs.attention_mask,\n                         max_new_tokens = 500)\n\ntokenizer.batch_decode(outputs)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T18:19:10.903718Z","iopub.execute_input":"2024-10-13T18:19:10.904440Z","iopub.status.idle":"2024-10-13T18:19:14.221363Z","shell.execute_reply.started":"2024-10-13T18:19:10.904397Z","shell.execute_reply":"2024-10-13T18:19:14.220390Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"['<|begin_of_text|>Below is a DOCUMENT in which human beings may be expressing themselves about products or services.\\n\\nPerform a full aspect-based sentiment analysis of the DOCUMENT.\\n    \\nOnly use sentiment labels that appear in the below list of ALLOWED SENTIMENTS.\\n    \\nOnly use aspect category labels that appear in the below list of ALLOWED ASPECT CATEGORIES.\\n    \\nUse the exact words and spelling found in the DOCUMENT without modification.\\n\\nReturn your answer as a structured JSON object without deviation.\\n\\n### DOCUMENT:\\n\\nI really hated this place\\'s burgers they are the worst i have ever tasted in New York!\\n\\n### ALLOWED SENTIMENTS:\\n\\n- positive\\n- negative\\n- neutral\\n\\n### ALLOWED ASPECT CATEGORIES:\\n\\n- food quality\\n- service general\\n- restaurant general\\n- ambience general\\n- food style_options\\n- restaurant miscellaneous\\n- food prices\\n- restaurant prices\\n- drinks quality\\n- drinks style_options\\n- location general\\n- drinks prices\\n- food general\\n\\n### RESPONSE:\\n{\"aspect_based_sentiment_analysis\": [{\"opinion term\": \"burgers\", \"aspect category\": \"food quality\", \"sentiment\": \"negative\", \"justification\": \"worst\"}] }<|end_of_text|>']"},"metadata":{}}]},{"cell_type":"code","source":"res = tokenizer.batch_decode(outputs)\n\n# add 0 here - might not work when have several in batch TODO: test with dataset not just 1 sample\nstart_idx = res[0].index(\"### RESPONSE:\\n\")\n\nprint(start_idx)\n\n# TODO: fix this better with error handling or if train with chat template understand how it works\ngenerated_absa_text = res[0][start_idx + len(\"### RESPONSE:\\n\"):]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T18:21:34.709439Z","iopub.execute_input":"2024-10-13T18:21:34.709850Z","iopub.status.idle":"2024-10-13T18:21:34.716671Z","shell.execute_reply.started":"2024-10-13T18:21:34.709810Z","shell.execute_reply":"2024-10-13T18:21:34.715688Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"951\n","output_type":"stream"}]},{"cell_type":"code","source":"generated_absa_text = generated_absa_text.rstrip(EOS_TOKEN)\n\ngenerated_absa_text","metadata":{"execution":{"iopub.status.busy":"2024-10-13T18:22:04.403321Z","iopub.execute_input":"2024-10-13T18:22:04.404055Z","iopub.status.idle":"2024-10-13T18:22:04.410037Z","shell.execute_reply.started":"2024-10-13T18:22:04.404010Z","shell.execute_reply":"2024-10-13T18:22:04.409046Z"},"trusted":true},"execution_count":89,"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"'{\"aspect_based_sentiment_analysis\": [{\"opinion term\": \"burgers\", \"aspect category\": \"food quality\", \"sentiment\": \"negative\", \"justification\": \"worst\"}] }'"},"metadata":{}}]},{"cell_type":"markdown","source":"# try JSON loading","metadata":{}},{"cell_type":"code","source":"absa_data = json.loads(generated_absa_text)\n\nabsa_data[\"aspect_based_sentiment_analysis\"]","metadata":{"execution":{"iopub.status.busy":"2024-10-13T18:23:02.206616Z","iopub.execute_input":"2024-10-13T18:23:02.207589Z","iopub.status.idle":"2024-10-13T18:23:02.214022Z","shell.execute_reply.started":"2024-10-13T18:23:02.207543Z","shell.execute_reply":"2024-10-13T18:23:02.213076Z"},"trusted":true},"execution_count":90,"outputs":[{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"[{'opinion term': 'burgers',\n  'aspect category': 'food quality',\n  'sentiment': 'negative',\n  'justification': 'worst'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"# Saving and GGUF stuff\n\n**NOTICED THAT THIS IS ---NOT--- THE INSTRUCT VERSION O_o**","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T18:27:12.828941Z","iopub.execute_input":"2024-10-13T18:27:12.829882Z","iopub.status.idle":"2024-10-13T18:27:12.855438Z","shell.execute_reply.started":"2024-10-13T18:27:12.829838Z","shell.execute_reply":"2024-10-13T18:27:12.854601Z"},"trusted":true},"execution_count":91,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87ad0a9f6c5b499da9f43942b8795339"}},"metadata":{}}]},{"cell_type":"code","source":"SAVE_NAME = \"Llama-3.1-8B\"","metadata":{"execution":{"iopub.status.busy":"2024-10-13T18:30:38.534657Z","iopub.execute_input":"2024-10-13T18:30:38.535407Z","iopub.status.idle":"2024-10-13T18:30:38.539471Z","shell.execute_reply.started":"2024-10-13T18:30:38.535364Z","shell.execute_reply":"2024-10-13T18:30:38.538497Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"# save LoRA stuff\n\n\nmodel.push_to_hub(f\"benjaminzwhite/{SAVE_NAME}_ABSQ-AQSP_LoRA\") \ntokenizer.push_to_hub(f\"benjaminzwhite/{SAVE_NAME}_ABSQ-AQSP_LoRA\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T18:30:41.018726Z","iopub.execute_input":"2024-10-13T18:30:41.019668Z","iopub.status.idle":"2024-10-13T18:30:48.285181Z","shell.execute_reply.started":"2024-10-13T18:30:41.019625Z","shell.execute_reply":"2024-10-13T18:30:48.283955Z"},"trusted":true},"execution_count":93,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/595 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b117120eb64841e99d0645f744379bde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c480b444d812432ca871a50c32ffc2a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21fbe5db896a492bab84de45d970dc32"}},"metadata":{}},{"name":"stdout","text":"Saved model to https://huggingface.co/benjaminzwhite/Llama-3.1-8B_ABSQ-AQSP_LoRA\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Try doing the merging\n\nhttps://huggingface.co/blog/mlabonne/sft-llama3\n\n**UPDATE : run out of space, will try redownloading the LoRA stuff and do GGUF only**","metadata":{}},{"cell_type":"code","source":"model.push_to_hub_merged(f\"benjaminzwhite/{SAVE_NAME}_ABSQ-AQSP_LoRA_merged_16bit\", tokenizer, save_method=\"merged_16bit\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T18:33:34.579851Z","iopub.execute_input":"2024-10-13T18:33:34.580719Z","iopub.status.idle":"2024-10-13T18:35:05.537226Z","shell.execute_reply.started":"2024-10-13T18:33:34.580676Z","shell.execute_reply":"2024-10-13T18:35:05.535535Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stderr","text":"Unsloth: You are pushing to hub, but you passed your HF username = benjaminzwhite.\nWe shall truncate benjaminzwhite/Llama-3.1-8B_ABSQ-AQSP_LoRA_merged_16bit to Llama-3.1-8B_ABSQ-AQSP_LoRA_merged_16bit\nUnsloth: You have 2 CPUs. Using `safe_serialization` is 10x slower.\nWe shall switch to Pytorch saving, which will take 3 minutes and not 30 minutes.\nTo force `safe_serialization`, set it to `None` instead.\nUnsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\nmodel which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\nUnsloth: Will remove a cached repo with size 5.7G\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 19.97 out of 31.36 RAM for saving.\n","output_type":"stream"},{"name":"stderr","text":" 38%|â–ˆâ–ˆâ–ˆâ–Š      | 12/32 [00:00<00:00, 25.19it/s]We will save to Disk and not RAM now.\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:24<00:00,  1.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Saving tokenizer... Done.\nUnsloth: Saving model... This might take 5 minutes for Llama-7b...\nUnsloth: Saving Llama-3.1-8B_ABSQ-AQSP_LoRA_merged_16bit/pytorch_model-00001-of-00004.bin...\nUnsloth: Saving Llama-3.1-8B_ABSQ-AQSP_LoRA_merged_16bit/pytorch_model-00002-of-00004.bin...\nUnsloth: Saving Llama-3.1-8B_ABSQ-AQSP_LoRA_merged_16bit/pytorch_model-00003-of-00004.bin...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:652\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 652\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:886\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    885\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 886\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:778] . PytorchStreamWriter failed writing file data/63: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[94], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub_merged\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbenjaminzwhite/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mSAVE_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_ABSQ-AQSP_LoRA_merged_16bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmerged_16bit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/save.py:1262\u001b[0m, in \u001b[0;36munsloth_push_to_hub_merged\u001b[0;34m(self, repo_id, tokenizer, save_method, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m arguments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m arguments[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m-> 1262\u001b[0m \u001b[43munsloth_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m   1264\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/save.py:690\u001b[0m, in \u001b[0;36munsloth_save_model\u001b[0;34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m    681\u001b[0m     hf_api\u001b[38;5;241m.\u001b[39mupload_folder(\n\u001b[1;32m    682\u001b[0m         folder_path \u001b[38;5;241m=\u001b[39m new_save_directory,\n\u001b[1;32m    683\u001b[0m         path_in_repo \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m         ignore_patterns \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.md\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    688\u001b[0m     )\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 690\u001b[0m     \u001b[43minternal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msave_pretrained_settings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;66;03m# Revert config back\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2795\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m         safe_save_file(shard, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, shard_file), metadata\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m   2794\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2795\u001b[0m         \u001b[43msave_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2798\u001b[0m     path_to_weights \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, weights_name)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/save.py:171\u001b[0m, in \u001b[0;36mfast_save_pickle\u001b[0;34m(shard, name)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfast_save_pickle\u001b[39m(shard, name):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# Use this if # CPUs is <= 2\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Saving \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 171\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HIGHEST_PROTOCOL seems to not work with Pytorch!\u001b[39;49;00m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# pickle_module   = pickle,\u001b[39;49;00m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# pickle_protocol = pickle.HIGHEST_PROTOCOL,\u001b[39;49;00m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:651\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    648\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 651\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    652\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:499\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:603] . unexpected pos 2785153024 vs 2785152912"],"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:603] . unexpected pos 2785153024 vs 2785152912","output_type":"error"}]},{"cell_type":"code","source":"quant_methods = [\"q2_k\", \"q3_k_m\", \"q4_k_m\", \"q5_k_m\", \"q6_k\", \"q8_0\"]\nfor quant in quant_methods:\n    model.push_to_hub_gguf(f\"benjaminzwhite/{SAVE_NAME}_ABSQ-AQSP_GGUF\", tokenizer, quant)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T18:36:07.279713Z","iopub.execute_input":"2024-10-13T18:36:07.280402Z","iopub.status.idle":"2024-10-13T18:36:31.268941Z","shell.execute_reply.started":"2024-10-13T18:36:07.280353Z","shell.execute_reply":"2024-10-13T18:36:31.267555Z"},"trusted":true},"execution_count":95,"outputs":[{"name":"stdout","text":"fatal: could not create work tree dir 'llama.cpp': No space left on device\nmake: *** llama.cpp: No such file or directory.  Stop.\nmake: *** llama.cpp: No such file or directory.  Stop.\nRequirement already satisfied: gguf in /opt/conda/lib/python3.10/site-packages (0.10.0)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (3.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from gguf) (1.26.4)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from gguf) (6.0.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from gguf) (4.66.4)\nUnsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 22.65 out of 31.36 RAM for saving.\n","output_type":"stream"},{"name":"stderr","text":"  9%|â–‰         | 3/32 [00:00<00:02, 10.71it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:652\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 652\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:886\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    885\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 886\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:778] . PytorchStreamWriter failed writing file data/0: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[95], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m quant_methods \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq2_k\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq3_k_m\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq4_k_m\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq5_k_m\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq6_k\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq8_0\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m quant \u001b[38;5;129;01min\u001b[39;00m quant_methods:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpush_to_hub_gguf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbenjaminzwhite/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mSAVE_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_ABSQ-AQSP_GGUF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/save.py:1756\u001b[0m, in \u001b[0;36munsloth_push_to_hub_gguf\u001b[0;34m(self, repo_id, tokenizer, quantization_method, first_conversion, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1754\u001b[0m     python_install\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m   1755\u001b[0m     install_llama_cpp_blocking(use_cuda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1756\u001b[0m     new_save_directory, old_username \u001b[38;5;241m=\u001b[39m \u001b[43munsloth_save_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1757\u001b[0m     makefile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/unsloth/save.py:556\u001b[0m, in \u001b[0;36munsloth_save_model\u001b[0;34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m    554\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning_once(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe will save to Disk and not RAM now.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    555\u001b[0m filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(temporary_location, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 556\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpickle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHIGHEST_PROTOCOL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# weights_only = True weirdly fails?\u001b[39;00m\n\u001b[1;32m    558\u001b[0m state_dict[name] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(filename, map_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, mmap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, weights_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:651\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    648\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 651\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    652\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:499\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream\u001b[38;5;241m.\u001b[39mclose()\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:603] . unexpected pos 576 vs 470"],"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:603] . unexpected pos 576 vs 470","output_type":"error"}]}]}